<html>
<head><base href="file:///Users/beka/Projects/Brain%20Archive/brain_archive/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>If Uploads Come First</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/meme/memelist.html?m=7">Visions of the Future</a> &gt; 
If Uploads Come First
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20071011141222/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0209.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0209.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/articles/art0209.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">If Uploads Come First</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0078.html" target="_top">Robin Hanson</a><br></span></td>
</table>
<br>
<div class="TeaserText">What if we obtain the ability to upload our minds to an artificial medium?  What if we can copy ourselves? In this 1994 essay, Robin Hanson looks at the possible social impacts of this question and how human values may evolve.</div>
<br>
<br><p>Originally published March 8, 1994 <a href="http://web.archive.org/web/20071011141222/http://hanson.gmu.edu/uploads.html" target="_new">here</a>. Published on KurzweilAI.net June 5, 2001.</p><h1>Abstract</h1><p>What if we someday learn how to model small <a class="thought" href="entries/brain_entry.html">brain</a> units, and so can "upload" ourselves into new <a class="thought" href="entries/computer_entry.html">computer</a> brains? What if this happens before we learn how to make <a class="thought" href="entries/human_entry.html">human</a>-level <a class="thought" href="entries/ai_entry.html">artificial intelligence</a>s? The result could be a sharp transition to an upload-dominated world, with many dramatic consequences. In particular, fast and cheap replication may once again make Darwinian <a class="thought" href="entries/evolution_entry.html">evolution</a> of <a class="thought" href="entries/human_entry.html">human</a> values a powerful force in <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/history_entry.html">history</a>. With evolved values, most uploads would value <a class="thought" href="entries/life_entry.html">life</a> even when <a class="thought" href="entries/life_entry.html">life</a> is hard or short, uploads would reproduce quickly, and wages would fall. But total wealth should rise, so we could all do better by accepting uploads, or at worse taxing them, rather than trying to delay or segregate them.</p><h1>Introduction</h1><p>The <a class="thought" href="entries/future_entry.html">future</a> is hard to predict. We may feel confident that eventually <a class="thought" href="entries/space_entry.html">space</a> will be colonized, or that eventually we'll make stuff by putting each <a class="thought" href="entries/atom_entry.html">atom</a> just where we want it. But so many other changes may happen before and during those changes that it is hard to say with much confidence how <a class="thought" href="entries/space_entry.html">space</a> travel or <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> may affect the ordinary person. Our vision seems to fade into a fog of possibilities.</p>
<p>The scenario I am about to describe excites me because it seems an exception to this general rule--more like a crack of dawn than a fog, like a sharp transition with sharp implications regardless of the night that went before. Or like a sight on the horizon much clearer than the terrain in between. And, as scenarios go, this one seems rather likely. Here it is.</p><h1>If A.I. Is Hard</h1><p>The <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> is one of the most complex <a class="thought" href="entries/system_entry.html">system</a>s we know, and so <a class="thought" href="entries/progress_entry.html">progress</a> in understanding the <a class="thought" href="entries/brain_entry.html">brain</a> may be slow, relative to other forms of technological and scientific <a class="thought" href="entries/progress_entry.html">progress</a>. What if <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> (A.I.), the problem of designing intelligent <a class="thought" href="entries/system_entry.html">system</a>s from scratch, turns out to be similarly hard, one of the hardest design tasks we confront? [1]</p>
<p>If so, it may well be that technological <a class="thought" href="entries/progress_entry.html">progress</a> and economic <a class="thought" href="entries/growth_entry.html">growth</a> give us <a class="thought" href="entries/computer_entry.html">computer</a>s with roughly the <a class="thought" href="entries/computation_entry.html">computation</a>al power of the <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> well before we know how to directly <a class="thought" href="entries/program_entry.html">program</a> such <a class="thought" href="entries/computer_entry.html">computer</a>s with <a class="thought" href="entries/human_entry.html">human</a>-equivalent <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. After all, we make <a class="thought" href="entries/progress_entry.html">progress</a> in <a class="thought" href="entries/software_entry.html">software</a> as well as <a class="thought" href="entries/hardware_entry.html">hardware</a>; we could now make much better use of a thirty year old <a class="thought" href="entries/computer_entry.html">computer</a> than folks could the day it was built, and similar <a class="thought" href="entries/progress_entry.html">progress</a> should continue after we get <a class="thought" href="entries/human_entry.html">human</a>-equivalent <a class="thought" href="entries/hardware_entry.html">hardware</a>. We don't know just how good <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> <a class="thought" href="entries/software_entry.html">software</a> is, but it might well be beyond our abilities when we have good enough <a class="thought" href="entries/hardware_entry.html">hardware</a>. [2]</p>
<p>Not having <a class="thought" href="entries/human_entry.html">human</a>-level A.I. would not mean <a class="thought" href="entries/computer_entry.html">computer</a>s and robots couldn't do better than us on many specific tasks, or that <a class="thought" href="entries/computer_entry.html">computer</a>-aided humans wouldn't be many times more productive than unaided humans. We might even realize extreme "<a class="thought" href="entries/cyborg_entry.html">cyborg</a>" visions, with <a class="thought" href="entries/biological_entry.html">biological</a> brains and bodies wrapped in lots of artificial extras--imagine heavy use of <a class="thought" href="entries/computer_entry.html">computer</a> agents, visual pre-processors, local <a class="thought" href="entries/information_entry.html">information</a> banks, etc.</p>
<p>But not having <a class="thought" href="entries/human_entry.html">human</a>-level A.I. could mean that <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> continues to be very productive - that on average the amount of valued stuff that can be produced decreases by a substantial fraction when the amount of <a class="thought" href="entries/human_entry.html">human</a> labor used to produce that stuff decreases by a substantial fraction. <a class="thought" href="entries/cyborg_entry.html">Cyborg</a> add-ons, without that <a class="thought" href="entries/brain_entry.html">brain</a> inside, couldn't do nearly as much.</p>
<p>Thus, as today, and as standard economic models [3] predict, most folks would still spend much, perhaps most, of their <a class="thought" href="entries/time_entry.html">time</a> working. And most wealth would remain in the form of people's abilities to work, even if the median worker is incredibly wealthy by today's standards. We are, after all, incredibly wealthy by the standards of the ancients, yet we still work. In contrast, having loyal <a class="thought" href="entries/human_entry.html">human</a>-level A.I.s could be more like owning a hundred <a class="thought" href="entries/human_entry.html">human</a> slaves, each as skilled as yourself--in this case there is hardly any point in working, unless for the pleasure of it.</p>
<p>A limited understanding of the <a class="thought" href="entries/brain_entry.html">brain</a> and <a class="thought" href="entries/biology_entry.html">biology</a> in general would also suggest that humans would not be highly modified - that whatever we would have added on the outside, inside we would be basically the same sort of people with the same sort of motivations and cognitive abilities. And we would be likely still mortal as well. After all, even <a class="thought" href="entries/biology_entry.html">biology</a> has evolved the <a class="thought" href="entries/brain_entry.html">brain</a> largely by leaving old complex <a class="thought" href="entries/system_entry.html">system</a>s alone; new functionality is mainly added by wrapping old <a class="thought" href="entries/system_entry.html">system</a>s in new add-on modules.</p><h1>Uploads</h1><p>Imagine that before we figure out how to write <a class="thought" href="entries/human_entry.html">human</a>-level <a class="thought" href="entries/software_entry.html">software</a>, but after we have <a class="thought" href="entries/human_entry.html">human</a>-level <a class="thought" href="entries/hardware_entry.html">hardware</a>, our understanding of the <a class="thought" href="entries/brain_entry.html">brain</a> <a class="thought" href="entries/progress_entry.html">progress</a>es to the point where we have a <a class="thought" href="entries/reason_entry.html">reason</a>able model of local <a class="thought" href="entries/brain_entry.html">brain</a> processes. That is, while still ignorant about larger <a class="thought" href="entries/brain_entry.html">brain</a> organization, we learn to identify small <a class="thought" href="entries/brain_entry.html">brain</a> units (such as <a class="thought" href="entries/synapse_entry.html">synapse</a>s, <a class="thought" href="entries/brain_entry.html">brain</a> cells, or clusters of cells) with limited interaction modes and internal states, and have a "good enough" model of how the state of each unit changes as a function of its interactions. The finiteness and locality of ordinary <a class="thought" href="entries/physics_entry.html">physics</a> and biochemistry, and the stability of <a class="thought" href="entries/brain_entry.html">brain</a> states against small perturbations, should ensure that such a model exists, though it may be hard to find. [4]</p>
<p>Imagine further that we learn how to take apart a real <a class="thought" href="entries/brain_entry.html">brain</a> and to build a total model of that <a class="thought" href="entries/brain_entry.html">brain</a>--by identifying each unit, its internal state, and the connections between units. [5] A "good enough" model for each unit should induce in the total <a class="thought" href="entries/brain_entry.html">brain</a> model the same general high-level external behavior as in the real <a class="thought" href="entries/brain_entry.html">brain</a>, even if it doesn't reproduce every detail. That is, if we implement this model in some <a class="thought" href="entries/computer_entry.html">computer</a>, that <a class="thought" href="entries/computer_entry.html">computer</a> will "act" just like the original <a class="thought" href="entries/brain_entry.html">brain</a>, responding to given <a class="thought" href="entries/brain_entry.html">brain</a> inputs with the same sort of outputs.</p>
<p>That model would be what we call an "upload"--<a class="thought" href="entries/software_entry.html">software</a> with <a class="thought" href="entries/human_entry.html">human</a>-level <a class="thought" href="entries/intelligence_entry.html">intelligence</a>, yet created using little understanding of how the <a class="thought" href="entries/brain_entry.html">brain</a> works, on anything but the lowest levels of organization. In <a class="thought" href="entries/software_entry.html">software</a> terminology, this is like "porting" <a class="thought" href="entries/software_entry.html">software</a> to a new <a class="thought" href="entries/language_entry.html">language</a> or <a class="thought" href="entries/platform_entry.html">platform</a>, rather than rewriting a new version from scratch (more the A.I. approach). One can <a class="thought" href="entries/port_entry.html">port</a> <a class="thought" href="entries/software_entry.html">software</a> without understanding it, if one understands the <a class="thought" href="entries/language_entry.html">language</a> it was written in.</p>
<p>Of course some will doubt that such a <a class="thought" href="entries/brain_entry.html">brain</a> model would "feel" the same on the inside, or even feel anything at all. But it must act just as if it feels, since it must act like the original <a class="thought" href="entries/brain_entry.html">brain</a>, and so many people will believe that it does so feel.</p>
<p>Now without some sort of connection to the world, such an upload would likely go crazy or attempt suicide, as would most proto-uploads, not-quite good-enough <a class="thought" href="entries/brain_entry.html">brain</a>-models that fail on important details like hormonal regulation of <a class="thought" href="entries/emotion_entry.html">emotion</a>s. But with even very crude fingers and eyes or ears, uploads might not only find <a class="thought" href="entries/life_entry.html">life</a> worth living but become productive workers in trades where crude interaction can be good enough, such as writing novels, doing math, etc. And with more advanced <a class="thought" href="entries/android_entry.html">android</a> bodies or <a class="thought" href="entries/virtual_reality_entry.html">virtual reality</a>, uploads might eventually become productive in most trades, and miss their original bodies much less.</p>
<p>Thus some people should be willing to become uploads, even if their old brains were destroyed in the process. And since, without A.I., uploads should be productive workers, there should be big money to be made in funding the creation of such uploads. The day such money starts to flow, uploads should begin to be created in significant quantity. This day would be the "dawn" I referred to above, a sharp transition with clear and dramatic consequences.</p><h1>Upload Consequences</h1><p>The consequences for the uploads themselves are the most immediate. They would live in synthetic bodies and brains, which could vary much more from each other than ordinary bodies and brains. Upload <a class="thought" href="entries/brain_entry.html">brain</a> models could be run at speeds many times that of ordinary <a class="thought" href="entries/human_entry.html">human</a> brains, and speed variations could induce great variations in upload's subjective ages and experience. And upload bodies could also vary in size, reliability, <a class="thought" href="entries/energy_entry.html">energy</a> drain, maintenance costs, extra body features, etc. Strong social hierarchies might develop; some might even be "gods" in comparison to others.</p>
<p>To a fast (meaning accelerated) upload, the world would seem more sluggish. <a class="thought" href="entries/computer_entry.html">Computer</a>s would seem slower, and so fast uploads would find less value in them; <a class="thought" href="entries/computer_entry.html">computer</a>s would be used less, though still much used. <a class="thought" href="entries/communication_entry.html">Communication</a> delays would make the <a class="thought" href="entries/earth_entry.html">Earth</a> feel bigger, and <a class="thought" href="entries/space_entry.html">space</a> colonization would seem a slower and more forbidding prospect (all else equal). Interest rates would seem smaller, making investing in the <a class="thought" href="entries/future_entry.html">future</a> less attractive for a given <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of values.</p>
<p>Fast uploads who want physical bodies that can keep up with their faster brains might use proportionally smaller bodies. For example, assume it takes 10<sup>15</sup> instructions per second and 10<sup>15</sup> fast <a class="thought" href="entries/memory_entry.html">memory</a> bits to run a <a class="thought" href="entries/brain_entry.html">brain</a> model at familiar speeds, and that upload brains could be built using nanomechanical <a class="thought" href="entries/computer_entry.html">computer</a>s and <a class="thought" href="entries/memory_entry.html">memory</a> registers, as described in [Drexler]. If so, an approx. 7 mm. tall <a class="thought" href="entries/human_entry.html">human</a>-shaped body could have a <a class="thought" href="entries/brain_entry.html">brain</a> that that fits in its <a class="thought" href="entries/brain_entry.html">brain</a> cavity, keeps up with its approx. 260 times faster body motions, and consumes approx. 16 W of power. Such uploads would glow like Tinkerbell in air, or might live underwater to keep cool. Bigger slower bodies could run much cooler by using reversible <a class="thought" href="entries/computer_entry.html">computer</a>s [Hanson].</p>
<p>Billions of such uploads could live and work in a single high-rise building, with roomy accommodations for all, if enough power and cooling were available. To avoid alienation, many uploads might find comfort by living among tiny familiar-looking trees, houses, etc., and living under an artificial sun that rises and sets approx. 260 times a day. Other uploads may reject the familiar and aggressively explore the new possibilities. For such tiny uploads, <a class="thought" href="entries/gravity_entry.html">gravity</a> would seem much weaker, higher sound pitches would be needed, and visual resolution of ordinary <a class="thought" href="entries/light_entry.html">light</a> might decline (in both angular and intensity terms).</p>
<p>Alternatively, uploads seeking familiarity might withdraw more into virtual realities, if such simulations were not overly expensive. For relaxing and having fun, virtual realities could be anything uploads wanted them to be. But for getting real work done, "virtual" realities could not be arbitrary; they would have to reflect the underlying realities of the physical, <a class="thought" href="entries/software_entry.html">software</a>, <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, or social worlds they represent. Since, compared with <a class="thought" href="entries/software_entry.html">software</a> we write, the <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> seems especially good at dealing with the physical world, and since dealing with physical objects and processes should remain a big part of useful work for a long <a class="thought" href="entries/time_entry.html">time</a> to come, many uploads should remain familiar with the physical world for a long <a class="thought" href="entries/time_entry.html">time</a> to come.</p>
<p>An intermediate approach between tiny bodies and <a class="thought" href="entries/virtual_reality_entry.html">virtual reality</a> would be to separate brains from bodies. brains might be relatively fixed in location, and use high-<a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> connections to "tele-operate" remote bodies. Of course such separation would not be economical at distances where <a class="thought" href="entries/communication_entry.html">communication</a>s costs were too high relative to <a class="thought" href="entries/brain_entry.html">brain</a> <a class="thought" href="entries/hardware_entry.html">hardware</a> costs.</p>
<p>Uploads might need to find better ways to trust each other. While ordinary humans can often find unconscious signs of deception in facial <a class="thought" href="entries/expression_entry.html">expression</a>s, upload faces may be under more direct conscious control. And uploads minds could be tortured without leaving any direct physical evidence of the <a class="thought" href="entries/event_entry.html">event</a>.</p>
<p>If, as seems <a class="thought" href="entries/reason_entry.html">reason</a>able, upload brains are given extra wiring to allow the current <a class="thought" href="entries/brain_entry.html">brain</a> state to be cheaply "read out" and "written in", then uploads could change bodies or brains relatively often, and could be transported long distances by ordinary <a class="thought" href="entries/communication_entry.html">communication</a> lines. "Backups" could be saved, allowing near <a class="thought" href="entries/immortality_entry.html">immortality</a> for those who could afford it; if your current <a class="thought" href="entries/brain_entry.html">brain</a> and body is unexpectedly destroyed, your latest backup can be installed in a new <a class="thought" href="entries/brain_entry.html">brain</a> and body.</p>
<p>The most dramatic consequences for both uploads and everyone one else come, I think, from the fact that uploads can be copied as well as backed-up. The state of one upload <a class="thought" href="entries/brain_entry.html">brain</a> might be read out and written into a new upload <a class="thought" href="entries/brain_entry.html">brain</a>, while that state still remained in the original <a class="thought" href="entries/brain_entry.html">brain</a>. At the moment of creation, there would be two identical upload minds, minds which would then diverge with their differing experiences.</p>
<p>Uploads who copy themselves at many different times would produce a zoo of entities of varying degrees of similarity to each other. Richer concepts of <a class="thought" href="entries/identity_entry.html">identity</a> would be needed to deal with this zoo, and social custom and law would face many new questions, ranging from "Which copies do I send Christmas cards to?" to "Which copies should be punished for the crimes of any one of them?"[6]</p>
<p>New forms of social organization might be useful for families of copies of the same original mind; some families of copies might be very loyal, while others might fight constantly. Teams of people who work well together might even be copied together, creating "team families". Political institutions like "one man, one vote" might require substantial modification, though large copy families could find obvious candidates to represent them in legislatures.</p><h1>A Population Explosion?</h1><p>Perhaps the most dramatic consequence of upload copying is the potential for an huge population explosion. If copying is fast, cheap, and painless, and if enough uploads desire to, can afford to, and are allowed to make such copies, the upload population could grow at a rate far exceeding the rate at which their total wealth grows, triggering a rapid <a class="thought" href="entries/reduction_entry.html">reduction</a> in per-capita (meaning per-copy) wealth.</p>
<p>Would an upload population explode? For a little perspective, let's review ordinary <a class="thought" href="entries/human_entry.html">human</a> population <a class="thought" href="entries/growth_entry.html">growth</a>. In the short term one might take people's values [7] as given. In that case reproduction rates depend on values and per-capita wealth, and per-capita wealth depends on values and reproduction rates.</p>
<p>People choose to have more or fewer babies depending on their values and culture, how much such babies will cost them, the wealth they have to give, how much payback they expect to get from their children later, and on how their children's lifestyle will depend on family size. <a class="thought" href="entries/technology_entry.html">Technology</a> and wealth also influence contraception and the number of babies who survive to adulthood.</p>
<p>Changes in per capita wealth, on the other hand, depend not only on reproduction rates, but also on how much folks value current consumption over <a class="thought" href="entries/future_entry.html">future</a> consumption, and on the rates of <a class="thought" href="entries/growth_entry.html">growth</a> possible in physical, <a class="thought" href="entries/human_entry.html">human</a>, and <a class="thought" href="entries/knowledge_entry.html">knowledge</a> capital. And <a class="thought" href="entries/knowledge_entry.html">knowledge</a> capital <a class="thought" href="entries/growth_entry.html">growth</a> rates seem to grow with the size of the <a class="thought" href="entries/human_entry.html">human</a> population [Simon].</p>
<p>The net result of all these factors is not clear from theory, but since we have observed rising per-capita wealth for the last few centuries, we might suppose the net tradeoff, given current values, favors rising per-capita wealth.</p>
<p>A few centuries is only about a dozen generations, however. And Darwinian arguments suggest that if values can be inherited, then after enough generations the values in a <a class="thought" href="entries/species_entry.html">species</a> should evolve to favor the maximum sustainable population for any given <a class="thought" href="entries/technology_entry.html">technology</a>, and the maximum sustainable <a class="thought" href="entries/growth_entry.html">growth</a> rate as <a class="thought" href="entries/technology_entry.html">technology</a> improves [Hansson &amp; Stuart].</p>
<p>This Darwinian view holds that our familiar <a class="thought" href="entries/human_entry.html">human</a> values, for resources, health, comfort, leisure, adventure, friendship, etc., were well suited for promoting maximal population and <a class="thought" href="entries/growth_entry.html">growth</a> in the sort of environments our ancestors faced long ago. And this view suggests that any current conflict between values and maximal <a class="thought" href="entries/growth_entry.html">growth</a>, such as that suggested by declining populations in Europe, is a temporary aberration due to "recent" rapid changes in the <a class="thought" href="entries/human_entry.html">human</a> environment.</p>
<p>Thus, given enough generations, <a class="thought" href="entries/human_entry.html">human</a> values should evolve to promote maximal <a class="thought" href="entries/growth_entry.html">growth</a> in our new sorts of environments--one may still worry, for example, that small minorities who value exceptionally large families [8] will eventually come to dominate the population.</p>
<p>Of course a complete story of how <a class="thought" href="entries/human_entry.html">human</a> values evolve must include the <a class="thought" href="entries/evolution_entry.html">evolution</a> of idea and value <a class="thought" href="entries/element_entry.html">element</a>s as "memes", entities in their own right and not just as properties passed from <a class="thought" href="entries/human_entry.html">human</a> parent to child through a combination of genetic and cultural <a class="thought" href="entries/evolution_entry.html">evolution</a>. But if our receptivity to accepting non-parental values can be genetically or culturally modulated, it is hard to see how <a class="thought" href="entries/human_entry.html">human</a> values could consistently resist <a class="thought" href="entries/human_entry.html">human</a> Darwinian pressures over the long term, even with memetic <a class="thought" href="entries/evolution_entry.html">evolution</a>. Overall, these Darwinian arguments suggesting maximal <a class="thought" href="entries/growth_entry.html">growth</a> seem roughly right.</p>
<p>Fortunately, however, this Darwinian process seems slow, and if economic <a class="thought" href="entries/growth_entry.html">growth</a> rates continue their historical acceleration, they should soon exceed the maximum rates at which ordinary humans can have babies. From then on, per-capita wealth would have to increase, at least until artificial wombs were created, or until raw materials or <a class="thought" href="entries/knowledge_entry.html">knowledge</a> <a class="thought" href="entries/progress_entry.html">progress</a> started to "run out", and could no longer expand exponentially with the population as they have so far. For now though, the world seems to be changing too fast for Darwinian <a class="thought" href="entries/evolution_entry.html">evolution</a> to catch up.</p>
<p>How do uploads change all this? An upload considering making a copy is much like a parent considering making a child. An upload would consider the cost to create a copy, the lifestyle that copy could afford, and how much they would value having another <a class="thought" href="entries/entity_entry.html">entity</a> like themselves. Uploads may value having copies of themselves more or less than ordinary folks now value having children somewhat like them - this is hard to predict.</p>
<p>But what is clearer is that upload reproduction rates can be very fast--the upload population could grow as fast as factories could generate new upload brains and bodies, if funds could be found to pay these factories. Upload copies, after all, do not need to be raised to adulthood and then trained in some profession; they are immediately ready to become productive members of society. Thus the main limitations on reproduction, and hence on Darwinian <a class="thought" href="entries/evolution_entry.html">evolution</a> of values, would become economic and political. Who would want to pay how much to make upload copies? And who would try how hard to stop them?</p><h1>Upload <a class="thought" href="entries/economics_entry.html">Economics</a></h1><p>To separate some issues, let us first imagine an upload, a contract lawyer by trade, who is neutral on the subject of whether she would like more entities like herself around, but who is considering an offer from someone else to pay for the creation of a copy. For simplicity, imagine that the original would keep all unique possessions and exclusive associations, such a painting, spouse, or job, and that the copy will have to start from scratch.</p>
<p>Such an upload might plausibly agree to this copy if she decided such a copy would consider their <a class="thought" href="entries/life_entry.html">life</a> "worth living", better to have existed than not. And since this copy could earn wages as a contract lawyer, she might consider <a class="thought" href="entries/life_entry.html">life</a> worth living if those wages, plus interest on some initial wealth endowment, were enough to cover some minimum standard of living.</p>
<p>Note, however, that if an upload expects wages to be high enough above their minimum required income, they might agree to a copy even with a <i>negative</i> initial endowment. That is, if a copy were to be loaned enough money to buy their new <a class="thought" href="entries/brain_entry.html">brain</a> and body, that copy might still find <a class="thought" href="entries/life_entry.html">life</a> worth living even under the burden of paying back this loan. [9]</p>
<p>If we now add in the original upload's values for having copies around, presumably positive for having more company but negative for the added wage competition, we should find that such an upload has some minimum expected income at which she would be willing to spin off copies. And given that this upload has decided to make a copy, she may or may not prefer to transfer some of the original's wealth to that copy.</p>
<p>Of course some uploads, perhaps even most, might not accept this line of <a class="thought" href="entries/reason_entry.html">reason</a>ing. But those that do would, if not forcibly prevented, keep making copies until their minimum income threshold is reached. Thus if there are even a few such uploads [10], wages for contract lawyers should quickly fall to near the lowest wage any one such upload contract lawyer is willing to work for. At this point many previous contract lawyers would find themselves displaced, even though the total number of contract lawyers has risen. And a large fraction of all contract lawyers should be copies of that one upload!</p>
<p>Of course abilities vary, and the lack of an ordinary body could be a disadvantage for early uploads competing with ordinary workers [11], limiting the number of ordinary workers uploads could initially displace. And reduced <a class="thought" href="entries/diversity_entry.html">diversity</a> of <a class="thought" href="entries/thought_entry.html">thought</a> among a large family of copies may put them at a disadvantage in trades which place a premium on <a class="thought" href="entries/creativity_entry.html">creativity</a>. But in many trades, like contract law, a large number of standardized workers might have special advantages, especially in reputation-building.</p>
<p>It also takes <a class="thought" href="entries/time_entry.html">time</a> for a labor market to absorb new workers; each job is somewhat different, and it takes <a class="thought" href="entries/time_entry.html">time</a> for people to learn each new job. Uploads running faster than ordinary humans might quickly master the relevant book-<a class="thought" href="entries/learning_entry.html">learning</a>, but for most jobs most <a class="thought" href="entries/learning_entry.html">learning</a> comes from watching and working with co-workers. At first, most co-workers will not be uploads, and most physical processes being managed would be tuned for ordinary <a class="thought" href="entries/human_entry.html">human</a> speeds, so being very much faster than usual may not be worth the cost of the faster <a class="thought" href="entries/hardware_entry.html">hardware</a>.</p>
<p>But as uploads became a larger part of the economy, upload communities which standardize on faster speeds would become more economical. If the rate at which faster uploads can grow wealth increases to match their faster speeds, then market interest rates should grow with the speed of such uploads. Slower individuals would then be much more tempted to save instead of consuming their wealth.</p>
<p>Falling wages should mean that, on the margin, labor is substituted for other forms of capital. So lower wage uploads should use fewer <a class="thought" href="entries/computer_entry.html">computer</a> and other <a class="thought" href="entries/productivity_entry.html">productivity</a> aids, and hence seem less "<a class="thought" href="entries/cyborg_entry.html">cyborg</a>ish".</p>
<p>What about professions where no upload has prior training? Even if the cost to upload people were very high, or the number of volunteers very low, upload workers should still displace other workers, though at a slower rate. If the wage in some trade were above an upload's minimum, even considering the costs of <a class="thought" href="entries/learning_entry.html">learning</a> that trade, and if loans could be arranged, copies would be created intending to master that trade.</p>
<p>The <a class="thought" href="entries/economics_entry.html">economics</a> of training uploads could be much like the current <a class="thought" href="entries/economics_entry.html">economics</a> of <a class="thought" href="entries/software_entry.html">software</a>. For example, labor "products" might be sold at substantially above marginal cost in <a class="thought" href="entries/order_entry.html">order</a> to recoup a large initial training cost. To control prices, some families might want to formally centralize their decisions about how many copies they make, so that each copy is no longer free to make more copies. In other families, informal mechanisms might be sufficient.</p>
<p>As with other <a class="thought" href="entries/software_entry.html">software</a>, uploads might reach capacity limits; after a few hundred or thousand years of <a class="thought" href="entries/subjective_experience_entry.html">subjective experience</a>, uploads might go crazy in some now unknown way, or simply be less and less able to learn new skills and <a class="thought" href="entries/information_entry.html">information</a>. If this happens, then investments in training might be limited to backups made and saved when uploads are below some critical subjective age.</p>
<p>Also as with <a class="thought" href="entries/software_entry.html">software</a> now, illicit copying of uploads might be a big problem. An upload who loses even one copy to pirates might end up with millions of illicit copies tortured into working as slaves in various hidden corners. To prevent such a <a class="thought" href="entries/fate_entry.html">fate</a>, uploads may be somewhat paranoid about security. They may prefer the added security of physical bodies, with "skulls" rigged to self-destruct on penetration or command. And without strong <a class="thought" href="entries/cryptography_entry.html">cryptography</a>, they may be wary of traveling by just sending bits.</p><h1>The <a class="thought" href="entries/evolution_entry.html">Evolution</a> of Values</h1><p>The analysis above suggests that, at least at first, the upload population should expand as fast as people can arrange loans, build brains and bodies, learn new jobs and professions, and as fast as the economy can absorb these new workers. Per-capita wages seem likely to fall in this period, for ordinary humans as well as uploads, though total wealth should rise.</p>
<p>This population explosion should continue until it reaches limits, such as those of values or of subsistence. Values limits would be reached if almost no capable, versatile upload found copies worth making at the prevailing low wages. Subsistence limits would be reached if uploads simply couldn't make ends meet on a lower income; lowering their standard of living any more would lower their <a class="thought" href="entries/productivity_entry.html">productivity</a>, and hence wages, by so much that they could not afford even that lower standard.</p>
<p>Would values limit this explosion? Yes, of course, if typical values were held constant; few people now who would make productive uploads would be willing to work at subsistence levels. It seems, however, that values will not be held constant. With upload copying, the potential rate and selectivity of reproduction could once again be comparable to the rate at which the world changes; Darwinian <a class="thought" href="entries/evolution_entry.html">evolution</a> (this <a class="thought" href="entries/time_entry.html">time</a> asexual) would have caught up with a changing world, and be once again a powerful force in <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/history_entry.html">history</a>. And since the transmission of values from "parent" to "child" is so much more reliable with upload copying, the direct <a class="thought" href="entries/evolution_entry.html">evolution</a> of "memes" should have even less room to modify our <a class="thought" href="entries/basic_entry.html">basic</a> Darwinian story.</p>
<p>As wages dropped, upload population <a class="thought" href="entries/growth_entry.html">growth</a> would be highly selective, selecting capable people willing to work for low wages, who value <a class="thought" href="entries/life_entry.html">life</a> even when <a class="thought" href="entries/life_entry.html">life</a> is hard. Soon the dominant upload values would be those of the few initial uploads with the most extreme values, willing to work for the lowest wages [12]. From this point on, value <a class="thought" href="entries/evolution_entry.html">evolution</a> would be limited by the rate at which people's values could drift with age, or could adjust to extreme circumstances.</p>
<p>Investors with foresight should be able to make this <a class="thought" href="entries/evolution_entry.html">evolution</a> of upload values even faster than ordinary "blind" <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/evolution_entry.html">evolution</a>. Investors seeking upload candidates, or upload copies, to whom to loan money, would likely seek out the few capable people with the most extreme and pliable values. After all, these candidates would, all else equal, have the best chances of repaying their loans.</p>
<p>Values might evolve even faster by combining crude modification techniques, like the equivalent of neuroactive drugs or even torture, with the ability to rerun experiments from identical starting points. Of course I do not advocate such experiments, but if they were effective, someone somewhere would likely use them. Fortunately, I suspect ordinary <a class="thought" href="entries/human_entry.html">human</a> values are varied and flexible enough to accommodate demand without resorting to such techniques. For example, identical twins who live together are much more different from each other than those reared apart. Similarly, an upload in a million-copy family should try all the harder to be different somehow, including in their values. Thus, given all these factors, the <a class="thought" href="entries/evolution_entry.html">evolution</a> of upload values might be very fast indeed.</p>
<p>What would values evolve to? Would wages hit subsistence level limits? I expect that over many generations (i.e., times copied) Darwinian selection should favor maximum long-term generation of "wealth" that can be used to buy new copies. That is, since upload reproduction can be so directly bought, we expect <a class="thought" href="entries/evolution_entry.html">evolution</a> to favor uploads whose values induce them to take <a class="thought" href="entries/action_entry.html">action</a>s which give their copy lineage the maximum long-term financial return on their investments, including their investments in new copies, new skills, or in "leisure."</p>
<p>Uploads who are overly shy about copying would lose out, holding less of the total wealth (as a group), measured by market value of assets, and constituting less of the population. Similarly, uploads who go wild in copying, just because they like the idea of having lots of copies, would become more numerous in the short term but lose out in the long term, both in total wealth and population. Thus we don't expect uploads to become as poor as possible, though we do expect them to eliminate consumption of "frills" which don't proportionally contribute to maximum long term <a class="thought" href="entries/productivity_entry.html">productivity</a>.</p>
<p>We should also expect an <a class="thought" href="entries/evolution_entry.html">evolution</a> of values regarding <a class="thought" href="entries/death_entry.html">death</a> and risk. [13] Imagine situations in which making a copy might pay off big, but most likely the copy would fail, run out of money and have to be "evicted" from its <a class="thought" href="entries/brain_entry.html">brain</a> and body. Many people might decline such opportunities, because they so dislike the prospect of such "<a class="thought" href="entries/death_entry.html">death</a>". Others might consider this not much bigger a deal than forgetting what happened at a party because they were too drunk; "they" would only lose their experiences since the last copy <a class="thought" href="entries/event_entry.html">event</a>. I expect <a class="thought" href="entries/evolution_entry.html">evolution</a> to prefer the later <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of values over the former.</p>
<p>Perhaps the hardest values to change in uploads will be our deeply-ingrained values for having children. Early upload <a class="thought" href="entries/technology_entry.html">technology</a> would likely not be able to create a baby's <a class="thought" href="entries/brain_entry.html">brain</a> from scratch, or even to upload a child's <a class="thought" href="entries/brain_entry.html">brain</a> and then correctly model <a class="thought" href="entries/brain_entry.html">brain</a> development processes. And even when such <a class="thought" href="entries/technology_entry.html">technology</a> is available, children would likely be a poor investment, from a long-term <a class="thought" href="entries/growth_entry.html">growth</a> point of view. New children may offer new perspectives, but with enough adult uploads, these benefits should only rarely exceed their high costs. Adults can offer new perspectives as well, and can do so cheaply.</p>
<p>Eventually, <a class="thought" href="entries/human_entry.html">human</a>-level <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> may be achieved at competitive <a class="thought" href="entries/hardware_entry.html">hardware</a> costs, or we may learn enough about the high-level organization of our brains to modify them substantially, perhaps merging distinct copies or splitting off "partials" of minds. The upload era would have ended, and many of the consequences of uploads described above may no longer apply; it seems particularly hard to project beyond this point.</p>
<p>But before then the upload era may last a long <a class="thought" href="entries/time_entry.html">time</a>, at least subjectively to uploads running at the dominant upload speed. If many uploads are fast, <a class="thought" href="entries/history_entry.html">history</a> will be told from the fast uploads' point of view; <a class="thought" href="entries/history_entry.html">history</a> chronicles wars and revolutions, triumphs and disasters, innovations and discoveries, and cares little about how many times the <a class="thought" href="entries/earth_entry.html">earth</a> spins.</p><h1>Upload <a class="thought" href="entries/politics_entry.html">Politics</a></h1><p>If voters and politicians lose their composure at the mere prospect of genetic modification of humans, or of wage competition by foreign workers, imagine the potential reaction against strong wage competition by "<a class="thought" href="entries/machine_entry.html">machine</a>-people" with strange values. <a class="thought" href="entries/uploading_entry.html">Uploading</a> might be forbidden, or upload copying might be highly restricted or forbidden. Of course without world <a class="thought" href="entries/government_entry.html">government</a> or strong multi-lateral agreements, uploads would eventually be developed in some country, and the transition would just have been delayed. And even with world <a class="thought" href="entries/government_entry.html">government</a>, covert <a class="thought" href="entries/uploading_entry.html">uploading</a> and copying might happen, perhaps using <a class="thought" href="entries/cryptography_entry.html">cryptography</a> to hide.</p>
<p>If level heads can be found, however, they should be told that if <a class="thought" href="entries/uploading_entry.html">uploading</a> and copying are allowed, it is possible to make almost everyone better off. While an upload transition might reduce the market value of ordinary people's <a class="thought" href="entries/human_entry.html">human</a> capital, their training and ability to earn wages, it should increase total wealth, the total market value of all capital, including <a class="thought" href="entries/human_entry.html">human</a> capital of uploads and others, real estate, company stock, etc. Thus it can potentially make each person better off.</p>
<p>For example, if most non-uploads had about the same fraction of their wealth in each form of capital, including owning shares in firms that make loans to uploads, and if a large enough fraction of upload wages went to pay off such loans, then most non-uploads would get richer from the transition. Even if you weren't one of the highly-copied uploads, your reduced wage-earning ability would be more than compensated for by your increased income from other sources. You could stop working, yet get richer and richer. By <a class="thought" href="entries/uploading_entry.html">uploading</a> and resisting copying, you could become effectively immortal.</p>
<p>The per-capita wealth of highly-copied uploads might decline, but that would not be a bad thing from their point of view. Their choice would indicate that they prefer many poorer copies to a single richer copy, just as parents today prefer the expense of children to the rich <a class="thought" href="entries/life_entry.html">life</a> of leisure possible without them.</p>
<p>Could a big fraction of upload wages go to paying loans? Yes, if there is enough competition between uploads, and if investors are not overly restricted by law. For example, refusing to loan to an upload if any other copy in their family has purposely defaulted on a loan might discourage such behavior. Alternatively, loans might be made to a copy family as a whole. But these options would have to be allowed by law.</p>
<p>Could most non-uploads sufficiently diversify their assets? Yes, if we develop financial institutions which allow this, such as allowing people to trade fractions of their <a class="thought" href="entries/future_entry.html">future</a> wages for shares in mutual funds. But tax laws like those that now encourage highly undiversified real estate holdings could cause problems. And even if people are able to so diversify their assets, they may not choose to do so, yet later demand that politicians fix their mistake.</p>
<p>If forced to act by their constituents, politicians would do better to tax uploads and copies, rather than forbidding them, and give the proceeds to those who would otherwise lose out. [14] Total wealth would grow more slowly than it otherwise would, but grow faster than without uploads. Of course there remains the problem of identifying the losers; politicals <a class="thought" href="entries/system_entry.html">system</a>s have often failed to find such win-win deals in the past, and could well fail again.</p>
<p>What about those who have values and abilities compatible with becoming part of the few highly-copied uploads? Would there be great inequality here, with some lucky few beating out the just-as-qualified rest?</p>
<p>If the cost to create an upload <a class="thought" href="entries/brain_entry.html">brain</a> model from an ordinary <a class="thought" href="entries/brain_entry.html">brain</a> were very high relative to the cost of creating a copy of an upload, or if <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/hardware_entry.html">hardware</a> were so cheap that even the earliest uploads were run very fast, the first few uploads might have a strong advantage over late-comers; early uploads may have lots more experience, lower costs, and may be a proven commodity relative to new uploads. [15] Billions of copies of the first few dozen uploads might then fill almost all the labor niches.</p>
<p><a class="thought" href="entries/computer_entry.html">Computer</a> <a class="thought" href="entries/technology_entry.html">technology</a> should keep improving even if work on <a class="thought" href="entries/uploading_entry.html">uploading</a> is delayed by <a class="thought" href="entries/politics_entry.html">politics</a>, lowering the cost of copying and the cost to run fast. Thus the early-adopter advantage would increase the longer <a class="thought" href="entries/uploading_entry.html">uploading</a> is delayed; delaying <a class="thought" href="entries/uploading_entry.html">uploading</a> should induce more, not less, inequality. So, if anything, one might prefer to speed up <a class="thought" href="entries/progress_entry.html">progress</a> on <a class="thought" href="entries/uploading_entry.html">uploading</a> <a class="thought" href="entries/technology_entry.html">technology</a>, to help make an <a class="thought" href="entries/uploading_entry.html">uploading</a> transition more equitable.</p>
<p>Similar arguments suggest that a delayed transition might be more sudden, since supporting technologies should be more mature. Sudden transitions should risk inducing more <a class="thought" href="entries/military_entry.html">military</a> and other social instabilities. All of these points argue against trying to delay an upload transition. [16]</p>
<p>Contrary to some fears, however, there seem to be no clear <a class="thought" href="entries/military_entry.html">military</a> implications from an upload transition, beyond the issue of transition speed and general risks from change. Yes, recently backed-up upload soldiers needn't fear <a class="thought" href="entries/death_entry.html">death</a>, and their commanders need only fear the loss of their bodies and brains, not of their experience and skills. But this is really just the standard upload trend toward cheaper labor translated into the <a class="thought" href="entries/military_entry.html">military</a> domain. It says little about fundamental <a class="thought" href="entries/military_entry.html">military</a> issues such as the relative expense of offense vs. defense, or feasible <a class="thought" href="entries/military_entry.html">military</a> buildup speeds vs. economic <a class="thought" href="entries/growth_entry.html">growth</a> rates.</p>
<p>What if uploads decide to take over by force, refusing to pay back their loans and grabbing other forms of capital? Well for comparison, consider the question: What if our children take over, refusing to pay back their student loans or to pay for Social Security? Or consider: What if short people revolt tonight, and kill all the tall people?</p>
<p>In general, most societies have many potential subgroups who could plausibly take over by force, if they could coordinate among themselves. But such revolt is rare in practice; short people know that if they kill all the tall folks tonight, all the blond people might go next week, and who knows where it would all end? And short people are highly integrated into society; some of their best friends are tall people.</p>
<p>In contrast, violence is more common between geographic and culturally separated subgroups. Neighboring nations have gone to war, ethnic minorities have revolted against <a class="thought" href="entries/government_entry.html">government</a>s run by other ethnicities, and slaves and other sharply segregated economic classes have rebelled.</p>
<p>Thus the best way to keep the peace with uploads would be to allow them as full as possible integration in with the rest of society. Let them live and work with ordinary people, and let them loan and sell to each other through the same institutions they use to deal with ordinary humans. Banning uploads into <a class="thought" href="entries/space_entry.html">space</a>, the seas, or the attic so as not to shock other folks might be ill-advised. Imposing especially heavy upload taxes, or treating uploads as property, as just <a class="thought" href="entries/software_entry.html">software</a> someone owns or as non-<a class="thought" href="entries/human_entry.html">human</a> slaves like dogs, might be especially unwise. [17]</p><h1>The Bottom Line</h1><p>Because understanding and designing <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is so hard, we may learn how to model small <a class="thought" href="entries/brain_entry.html">brain</a> units before learn how to make <a class="thought" href="entries/human_entry.html">human</a>-level A.I. Much will have changed by that <a class="thought" href="entries/time_entry.html">time</a>, but an upload transition would be so fundamental that we can still forsee some clear consequences. Subjective lifespans could be longer, minds could run faster, and reproduction could be cheaper, faster, and more precise. With <a class="thought" href="entries/human_entry.html">human</a> labor still in demand, an upload population should explode, and Darwinian <a class="thought" href="entries/evolution_entry.html">evolution</a> of values should once again become a powerful force in <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/history_entry.html">history</a>. Most uploads should quickly come to value <a class="thought" href="entries/life_entry.html">life</a> even when <a class="thought" href="entries/life_entry.html">life</a> is hard or short, and wages should fall dramatically.</p>
<p>What does this all mean for you now? If you expect that you or people you care about might live to see an upload transition, you might want to start to teach yourself and your children some new habits. Learn to diversify your assets, so they are less at risk from a large drop in wages; invest in mutual funds, real estate, etc., and consider ways in which you might sell fractions of your <a class="thought" href="entries/future_entry.html">future</a> wages for other forms of wealth. If you can't so diversify, consider saving more. [18]</p>
<p>Those who might want to be one of the few highly copies uploads should carefully consider whether their values and skills are appropriate. How much do you value <a class="thought" href="entries/life_entry.html">life</a> when it is hard and alien? Can you quickly learn many new skills? Can you get along with people like yourself? And such people might consider how they might become one of the first uploads. [19] Those who don't want to be highly-copied uploads should get used to the idea of their descendants becoming a declining fraction of total wealth and population, of leaving a rich but marginalized lineage.</p>
<p>If you participate in political or social reform, you might consider sowing seeds of acceptance of an upload transition, and of the benefits of an integrated society, and might consider helping to develop institutions to make it a win-win outcome for everyone. And if you <a class="thought" href="entries/research_entry.html">research</a> or develop <a class="thought" href="entries/technology_entry.html">technology</a>, consider helping to speed the development of upload <a class="thought" href="entries/technology_entry.html">technology</a>, so that the transition is less sudden when it comes.</p><h1>Footnotes</h1><p>1. This is my impression from 9 years of A.I. <a class="thought" href="entries/research_entry.html">research</a>, though of course many A.I. <a class="thought" href="entries/research_entry.html">research</a>ers disagree.</p>
<p>2. We might well have good enough <a class="thought" href="entries/hardware_entry.html">hardware</a> now for a slow A.I. that doesn't deal much with the physical world--say an A.I. contract lawyer.</p>
<p>3. Consider a model where utility is roughly a product of powers of leisure and consumption, and amount produced is roughly a product of powers of labor and other capital. Such a model can explain why leisure <a class="thought" href="entries/time_entry.html">time</a> has not changed much as per capita wealth has increased dramatically over the last few centuries, can explain high leisure among slave owners, and explains why leisure is higher in places and times with high income taxes. One can explain seasonal high leisure among foraging tribes as due to seasonal limits on foraging <a class="thought" href="entries/productivity_entry.html">productivity</a>.</p>
<p>4. <a class="thought" href="entries/penrose_entry.html">Roger Penrose</a>, in The Emporer's New Mind, suggests that non-local corrections to quantum <a class="thought" href="entries/gravity_entry.html">gravity</a> may play an important role in the brain; I find this extremely unlikely.</p>
<p>5. See [Merkle] for an exploration of the near-term feasibility of this, and [Platt] for a fictional account.</p>
<p>6. A viable, though perhaps not optimal, alternative is to hold all copies responsible for the <a class="thought" href="entries/action_entry.html">action</a>s of any one of them. If punishment is by fine when possible, then copy families could use insurance to contract away this interdependence.</p>
<p>7. By "values", I mean all preferences, desires, moral convictions, etc.</p>
<p>8. The Hutterites, a U.S. <a class="thought" href="entries/religion_entry.html">religion</a> group, has averaged 9 kids per family for a century.</p>
<p>9. Such a loan might come from the original upload or any other source, and might involve more risk-sharing than a simple loan--more like a joint investment.</p>
<p>10. Meaning enough so that they can't effectively conspire to keep their wages high.</p>
<p>11. Thus janitorial jobs should be safer longer than <a class="thought" href="entries/program_entry.html">program</a>mer jobs.</p>
<p>12. These wages are per product produced, not per <a class="thought" href="entries/time_entry.html">time</a> spent.</p>
<p>13. It seems that <a class="thought" href="entries/evolution_entry.html">evolution</a> should favor values that are roughly risk-neutral over the long term, with utility linear up to near the point of total world wealth. This seems to imply values roughly logarithmic in returns to short independent periods. 14. Note that such a tax would be a tax on the poor, paid to the relatively rich, if one counted per upload copy. 15. Many initial uploads might well be <a class="thought" href="entries/cryonics_entry.html">cryonics</a> patients, if legal permission to dissect and experiment with their brains were easier to obtain.</p>
<p>16. Note that, in contrast, a delayed <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> <a class="thought" href="entries/assembler_entry.html">assembler</a> transition seems likely to be less sudden, since pre-transition manufacturing abilities would not be as far behind the new <a class="thought" href="entries/nanotechnology_entry.html">nanotech</a> abilities. Efforts to "design-ahead" <a class="thought" href="entries/nanotechnology_entry.html">nanotech</a> <a class="thought" href="entries/device_entry.html">device</a>s, however, might make for a more sudden transition.</p>
<p>17. A similar argument applies to A.I.s capable of wanting to revolt.</p>
<p>18. This is, by the way, the same strategy that you should use to prepare for the possibility that A.I. is developed before uploads.</p>
<p>19. <a class="thought" href="entries/cryonics_entry.html">Cryonics</a> patients might want to grant explicit permission to become uploads.</p><h1>References</h1><a name="r1"></a>
<p class="Reference"><a class="thought" href="entries/drexler_entry.html">K. Eric Drexler</a>, <i>Nanosystems</i>, John Wiley &amp; Sons, Inc., New York, 1992.</p>
<a name="r2"></a>
<p class="Reference">Robin Hanson, "Reversible Agents: Need Robots Waste Bits to See, Talk, and Achieve?", Proc. 2nd <a class="thought" href="entries/physics_entry.html">Physics</a> of <a class="thought" href="entries/computation_entry.html">Computation</a> Workshop, 1992.</p>
<a name="r3"></a>
<p class="Reference">Ingemar Hansson, Charles Stuart, "Malthusian Selection of Preferences", American Economic Review, June 1990, V.80 No. 3. pp.529-544.</p>
<a name="r4"></a>
<p class="Reference">Alan R. Rogers, <a class="thought" href="entries/evolution_entry.html">Evolution</a> of <a class="thought" href="entries/time_entry.html">Time</a> Preference by Natural Selection, AER 84(3)460-481. (June 1994)</p>
<a name="r5"></a>
<p class="Reference">Ralph Merkle, "Large Scale Analysis of Neural Structures", Tech Report CSL-89-10, <a class="thought" href="entries/xerox_parc_entry.html">Xerox PARC</a>, 3333 Coyote Hill Road Palo Alto, CA 94304, 1989.</p>
<a name="r6"></a>
<p class="Reference">Charles Platt, <i>The </i><a class="thought" href="entries/silicon_entry.html">Silicon</a><i> Man</i>, Tafford Publishing, Houston, 1991.</p>
<a name="r7"></a>
<p class="Reference">Julian Simon, <i>The Ultimate Resource</i>, Princeton University Press, 1981.</p><h1>Acknowledgments</h1><p>This paper is better for the thoughful comments on earlier drafts by Stuart Card, <a class="thought" href="entries/hal_entry.html">Hal</a> Finney, Daniel Green, Josh Storrs Hall, Nancy Lebovitz, <a class="thought" href="entries/moravec_entry.html">Hans Moravec</a>, <a class="thought" href="entries/more_entry.html">Max More</a>, Jay Prime Positive, Mike Price, Marc Ringuette, Nick Szabo, and <a class="thought" href="entries/vinge_entry.html">Vernor Vinge</a>, and because of prior discussions of related issues with David Friedman, Keith Henson, Richard Kennaway, David Krieger, Tim May, Ralph Merkle, Perry Metzger, Mark Miller, Ravi Pandya, and Steve Witham. Many of these discussions took place on the <a class="thought" href="entries/extropian_entry.html">Extropian</a>s mailing list (<a class="thought" href="entries/extropian_entry.html">extropian</a>s@extropy.org).</p>
<p>This article originally appeared in <a class="thought" href="entries/extropy_entry.html">Extropy</a> 6:2 (1994).  Reprinted with permission.</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D74094" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id74095"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Quite  short  of  interesting aspects of  Transforming  Human Mind to the Hardware indepent  of  brain..<br><span class="mindxheader"><i>posted on 01/06/2007 11:34 PM by <a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/profile.php?id=3550">Agathon</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D74094%23id74095" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D74095" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Actually  , I  was  expecting  from this  man something  more  intriguing  in terms of like futuristic outcomes..
<br>
<br>
He  never mentioned  about  what  the  real  future  would  be  like..  what  would  be  like  the  conception of  uploading  the  mind  to  the  hardware,..
<br>
<br>
I  think the  author  is  somehow  economist  or materialist..  he  just  gets  to  the paradigmatic  way of  thinking  just  about  wealth, economical  aspects  and wealth, policy..
<br>
<br>
I  think  that shouldn't  be  the things  talked  about..  coz  they  are  boring  and uninteresting..  when  you  gonna  impress  people  and  wanna  make  your  story  more  intriguing  you  should  talk  about  not  the  boring  details  of  the economical  aspects  or  laws  or policies..
<br>
<br>
If  I  were  the  author  I  would  mention about  ..  what  it  be  like  being and  living  in a  computer  hardware  like  place,,  world  may  it  be  simulation knowing that it  is  simulation and  
<br>
being  sure  and  convinced  that your  this  virtual  reality,,  virtual  self  could  be  destroyed  just  because  of  taking  away  your  wealth..  money..
<br>
May be somebody..  some  novice worker  could  delete  you  forever even not  recognizing  that  you  are  there  ..  or  may  be  there  gets a    fire  on the  hardware  where  your  information of  your  mind  is  stored..
<br>
What  would  happen then?  What  would  happen to  your  privacy?
<br>
who  would  update  you  regularly  about  the things  going  on ?
<br>
and  would  living  in the  hardware  within  the  simulations  being  aware  of  it..be  just  boring  and  unrealistic..  and  dangerous..
<br>
For  example  the  company  that  has  uploaded  your  mind  to  the  hardware..  what  if  they  went  bankrupt  or  bombed  by  the  terrorists?
<br>
Would  you  do  with  it?
<br>
<br>
Just  imagine  you  would  even be  not  capable  of  saving  yourself,  defending  yourself..  may  be  some  kinda intentional  viruses  could  be  used  to delete  some  information that  you  had  in your  memories  and  in your  mind..
<br>
after  which  you  would  never  be  the  very  self  that  you  used  to  be ones...
<br>
<br>
So  I  think  this  kinda  thing  is  far  more futuristic  and  ungrounded fantasy .. :-)
<br>
But  who  knows what could  happen.. </p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id74098"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: If Uploads Come First<br><span class="mindxheader"><i>posted on 01/07/2007 3:35 AM by <a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D74094%23id74098" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D74098" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>The article is an another example of the obscession with printing nonsense, as a way to reach the great unwashed masses:)
<br>
<br>
es
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id74102"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: If Uploads Come First<br><span class="mindxheader"><i>posted on 01/07/2007 6:22 AM by <a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/profile.php?id=1573">Extropia</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D74094%23id74102" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011141222/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D74102" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Yes, absolutely. Thank you for detailing the flaws in Hanson's argument. I continue to learn a great deal from your writings, ES:)</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011141222im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>