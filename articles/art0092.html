<html>
<head><base href="file:///Users/beka/Projects/Brain%20Archive/brain_archive/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>The Technological Singularity</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/meme/memelist.html?m=1">The Singularity</a> &gt; 
The Technological Singularity
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20100621162543/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0092.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0092.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/articles/art0092.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">The Technological Singularity</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0007.html" target="_top">Vernor Vinge</a><br></span></td>
</table>
<br>
<div class="TeaserText">This is the article that introduced the idea of The Singularity. The original version of this article was presented at the VISION-21 Symposium sponsored by NASA Lewis Research Center and the Ohio Aerospace Institute, March 30-31, 1993. A slightly changed version appeared in the Winter 1993 issue of Whole Earth Review.</div>
<br>
<br><span class="AuthorAffiliation">Department of Mathematical <a class="thought" href="entries/science_entry.html">Science</a>s, San Diego State University</span>
<br>
<br><h1>What Is The <a class="thought" href="entries/singularity_entry.html">Singularity</a>?</h1><p>Originally published 1993 as an academic paper, San Diego State University. Published on KurzweilAI.net February 22, 2001. Version that appears on <a class="thought" href="entries/vinge_entry.html">Vernor Vinge</a>'s website can be read <a href="http://web.archive.org/web/20100621162543/http://www.ugcs.caltech.edu/~phoenix/vinge/vinge-sing.html" target="_new">here</a>.</p>
<p>The acceleration of technological <a class="thought" href="entries/progress_entry.html">progress</a> has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/life_entry.html">life</a> on <a class="thought" href="entries/earth_entry.html">Earth</a>. The precise cause of this change is the imminent creation by <a class="thought" href="entries/technology_entry.html">technology</a> of entities with greater than <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. There are several means by which <a class="thought" href="entries/science_entry.html">science</a> may achieve this breakthrough (and this is another <a class="thought" href="entries/reason_entry.html">reason</a> for having confidence that the <a class="thought" href="entries/event_entry.html">event</a> will occur):</p>
<ul>
<li>There may be developed <a class="thought" href="entries/computer_entry.html">computer</a>s that are "awake" and superhumanly intelligent. (To date, there has been much controversy as to whether we can create <a class="thought" href="entries/human_entry.html">human</a> equivalence in a <a class="thought" href="entries/machine_entry.html">machine</a>. But if the answer is "yes, we can", then there is little doubt that beings more intelligent can be constructed shortly thereafter.)</li>
<li>Large <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/network_entry.html">network</a>s (and their associated users) may "wake up" as a superhumanly intelligent <a class="thought" href="entries/entity_entry.html">entity</a>.</li>
<li><a class="thought" href="entries/computer_entry.html">Computer</a>/<a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/interface_entry.html">interface</a>s may become so intimate that users may <a class="thought" href="entries/reason_entry.html">reason</a>ably be considered superhumanly intelligent.</li>
<li><a class="thought" href="entries/biological_entry.html">Biological</a> <a class="thought" href="entries/science_entry.html">science</a> may provide means to improve natural <a class="thought" href="entries/human_entry.html">human</a> intellect.</li>
</ul>
<p>The first three possibilities depend in large part on improvements in <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/hardware_entry.html">hardware</a>. <a class="thought" href="entries/progress_entry.html">Progress</a> in <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/hardware_entry.html">hardware</a> has followed an amazingly steady curve in the last few decades [17]. Based largely on this trend, I believe that the creation of greater than <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> will occur during the next thirty years. (Charles Platt [20] has pointed out that <a class="thought" href="entries/ai_entry.html">AI</a> enthusiasts have been making claims like this for the last thirty years. Just so I'm not guilty of a relative-<a class="thought" href="entries/time_entry.html">time</a> ambiguity, let me more specific: I'll be surprised if this <a class="thought" href="entries/event_entry.html">event</a> occurs before 2005 or after 2030.)</p>
<p>What are the consequences of this event? When greater-than-<a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> drives <a class="thought" href="entries/progress_entry.html">progress</a>, that <a class="thought" href="entries/progress_entry.html">progress</a> will be much more rapid. In fact, there seems no <a class="thought" href="entries/reason_entry.html">reason</a> why <a class="thought" href="entries/progress_entry.html">progress</a> itself would not involve the creation of still more intelligent entities--on a still--shorter <a class="thought" href="entries/time_entry.html">time</a> scale. The best <a class="thought" href="entries/analogy_entry.html">analogy</a> that I see is with the <a class="thought" href="entries/evolution_entry.html">evolution</a>ary past: <a class="thought" href="entries/animal_entry.html">Animal</a>s can adapt to problems and make <a class="thought" href="entries/invention_entry.html">invention</a>s, but often no faster than natural selection can do its work--the world acts as its own <a class="thought" href="entries/simulator_entry.html">simulator</a> in the case of natural selection. We humans have the ability to internalize the world and conduct "what if's" in our heads; we can solve many problems thousands of times faster than natural selection. Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our <a class="thought" href="entries/human_entry.html">human</a> past as we humans are from the lower <a class="thought" href="entries/animal_entry.html">animal</a>s.</p>
<p>From the <a class="thought" href="entries/human_entry.html">human</a> point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were <a class="thought" href="entries/thought_entry.html">thought</a> might only happen in "a million years" (if ever) will likely happen in the next century. (In [5], Greg Bear paints a picture of the major changes happening in a <a class="thought" href="entries/matter_entry.html">matter</a> of hours.)</p>
<p>I think it's fair to call this <a class="thought" href="entries/event_entry.html">event</a> a <a class="thought" href="entries/singularity_entry.html">singularity</a> ("the <a class="thought" href="entries/singularity_entry.html">Singularity</a>" for the purposes of this paper). It is a point where our old models must be discarded and a new reality rules. As we move closer to this point, it will loom vaster and vaster over <a class="thought" href="entries/human_entry.html">human</a> affairs till the notion becomes a commonplace. Yet when it finally happens it may still be a great surprise and a greater unknown. In the 1950s there were very few who saw it: Stan Ulam [28] paraphrased John von Neumann as saying:</p>
<p>One conversation centered on the ever accelerating <a class="thought" href="entries/progress_entry.html">progress</a> of <a class="thought" href="entries/technology_entry.html">technology</a> and changes in the mode of <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/life_entry.html">life</a>, which gives the appearance of approaching some essential <a class="thought" href="entries/singularity_entry.html">singularity</a> in the <a class="thought" href="entries/history_entry.html">history</a> of the race beyond which <a class="thought" href="entries/human_entry.html">human</a> affairs, as we know them, could not continue.</p>
<p>Von Neumann even uses the term <a class="thought" href="entries/singularity_entry.html">singularity</a>, though it appears he is <a class="thought" href="entries/thinking_entry.html">thinking</a> of normal <a class="thought" href="entries/progress_entry.html">progress</a>, not the creation of superhuman intellect. (For me, the superhumanity is the essence of the <a class="thought" href="entries/singularity_entry.html">Singularity</a>. Without that we would get a glut of technical riches, never properly absorbed (see [25]).)</p>
<p>In the 1960s there was recognition of some of the implications of superhuman <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. I. J. Good wrote [11]:</p>
<p>Let an ultraintelligent <a class="thought" href="entries/machine_entry.html">machine</a> be defined as a <a class="thought" href="entries/machine_entry.html">machine</a> that can far surpass all the intellectual activities of any any man however clever. Since the design of <a class="thought" href="entries/machine_entry.html">machine</a>s is one of these intellectual activities, an ultraintelligent <a class="thought" href="entries/machine_entry.html">machine</a> could design even better <a class="thought" href="entries/machine_entry.html">machine</a>s; there would then unquestionably be an "<a class="thought" href="entries/intelligence_entry.html">intelligence</a> explosion," and the <a class="thought" href="entries/intelligence_entry.html">intelligence</a> of man would be left far behind. Thus the first ultraintelligent <a class="thought" href="entries/machine_entry.html">machine</a> is the <i>last</i> <a class="thought" href="entries/invention_entry.html">invention</a> that man need ever make, provided that the <a class="thought" href="entries/machine_entry.html">machine</a> is docile enough to tell us how to keep it under control. ... It is more probable than not that, within the twentieth century, an ultraintelligent <a class="thought" href="entries/machine_entry.html">machine</a> will be built and that it will be the last <a class="thought" href="entries/invention_entry.html">invention</a> that man need make.</p>
<p>Good has captured the essence of the runaway, but does not pursue its most disturbing consequences. Any intelligent <a class="thought" href="entries/machine_entry.html">machine</a> of the sort he describes would not be humankind's "tool"--any more than humans are the tools of rabbits or robins or chimpanzees.</p>
<p>Through the '60s and '70s and '80s, recognition of the cataclysm spread [29] [1] [31] [5]. Perhaps it was the <a class="thought" href="entries/science_entry.html">science</a>-fiction writers who felt the first concrete impact. After all, the "hard" <a class="thought" href="entries/science_entry.html">science</a>-fiction writers are the ones who try to write specific stories about all that <a class="thought" href="entries/technology_entry.html">technology</a> may do for us. More and more, these writers felt an opaque wall across the <a class="thought" href="entries/future_entry.html">future</a>. Once, they could put such fantasies millions of years in the <a class="thought" href="entries/future_entry.html">future</a> [24]. Now they saw that their most diligent extrapolations resulted in the unknowable ... soon. Once, galactic empires might have seemed a Post-<a class="thought" href="entries/human_entry.html">Human</a> domain. Now, sadly, even interplanetary ones are.</p>
<p>What about the '90s and the '00s and the '10s, as we slide toward the edge? How will the approach of the <a class="thought" href="entries/singularity_entry.html">Singularity</a> spread across the <a class="thought" href="entries/human_entry.html">human</a> world view? For a while yet, the general critics of <a class="thought" href="entries/machine_entry.html">machine</a> sapience will have good press. After all, till we have <a class="thought" href="entries/hardware_entry.html">hardware</a> as powerful as a <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> it is probably foolish to think we'll be able to create <a class="thought" href="entries/human_entry.html">human</a> equivalent (or greater) <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. (There is the far-fetched possibility that we could make a <a class="thought" href="entries/human_entry.html">human</a> equivalent out of less powerful <a class="thought" href="entries/hardware_entry.html">hardware</a>, if we were willing to give up speed, if we were willing to settle for an artificial being who was literally slow [30]. But it's much more likely that devising the <a class="thought" href="entries/software_entry.html">software</a> will be a tricky process, involving lots of false starts and experimentation. If so, then the arrival of self-aware <a class="thought" href="entries/machine_entry.html">machine</a>s will not happen till after the development of <a class="thought" href="entries/hardware_entry.html">hardware</a> that is substantially more powerful than humans' natural equipment.)</p>
<p>But as <a class="thought" href="entries/time_entry.html">time</a> passes, we should see more symptoms. The dilemma felt by <a class="thought" href="entries/science_fiction_entry.html">science fiction</a> writers will be perceived in other creative endeavors. (I have heard <a class="thought" href="entries/thought_entry.html">thought</a>ful comic book writers worry about how to have spectacular effects when everything visible can be produced by the technologically commonplace.) We will see automation replacing higher and higher level jobs. We have tools right now (symbolic math <a class="thought" href="entries/program_entry.html">program</a>s, <a class="thought" href="entries/computer_aided_design_entry.html">cad</a>/cam) that release us from most low-level drudgery. Or put another way: The work that is truly productive is the domain of a steadily smaller and more elite fraction of humanity. In the coming of the <a class="thought" href="entries/singularity_entry.html">Singularity</a>, we are seeing the predictions of <i>true</i> technological unemployment finally come true.</p>
<p>Another symptom of <a class="thought" href="entries/progress_entry.html">progress</a> toward the <a class="thought" href="entries/singularity_entry.html">Singularity</a>: ideas themselves should spread ever faster, and even the most radical will quickly become commonplace. When I began writing <a class="thought" href="entries/science_fiction_entry.html">science fiction</a> in the middle '60s, it seemed very easy to find ideas that took decades to percolate into the cultural <a class="thought" href="entries/consciousness_entry.html">consciousness</a>; now the lead <a class="thought" href="entries/time_entry.html">time</a> seems more like eighteen months. (Of course, this could just be me losing my imagination as I get old, but I see the effect in others too.) Like the shock in a compressible flow, the <a class="thought" href="entries/singularity_entry.html">Singularity</a> moves closer as we accelerate through the critical speed.</p>
<p>And what of the arrival of the <a class="thought" href="entries/singularity_entry.html">Singularity</a> itself? What can be said of its actual appearance? Since it involves an intellectual runaway, it will probably occur faster than any technical revolution seen so far. The precipitating <a class="thought" href="entries/event_entry.html">event</a> will likely be unexpected--perhaps even to the <a class="thought" href="entries/research_entry.html">research</a>ers involved. ("But all our previous models were catatonic! We were just tweaking some parameters....") If <a class="thought" href="entries/network_entry.html">network</a>ing is widespread enough (into ubiquitous <a class="thought" href="entries/embedded_systems_entry.html">embedded systems</a>), it may seem as if our <a class="thought" href="entries/artifact_entry.html">artifact</a>s as a whole had suddenly wakened.</p>
<p>And what happens a month or two (or a day or two) after that? I have only <a class="thought" href="entries/analog_entry.html">analog</a>ies to point to: The rise of humankind. We will be in the Post-<a class="thought" href="entries/human_entry.html">Human</a> era. And for all my rampant technological <a class="thought" href="entries/optimism_entry.html">optimism</a>, sometimes I think I'd be more comfortable if I were regarding these transcendental events from one thousand years remove ... instead of twenty.</p><h1>Can the <a class="thought" href="entries/singularity_entry.html">Singularity</a> be Avoided?</h1><p>Well, maybe it won't happen at all: Sometimes I try to imagine the symptoms that we should expect to see if the <a class="thought" href="entries/singularity_entry.html">Singularity</a> is not to develop. There are the widely respected arguments of Penrose [19] and Searle [22] against the practicality of <a class="thought" href="entries/machine_entry.html">machine</a> sapience. In August of 1992, <a class="thought" href="entries/thinking_machines_entry.html">Thinking Machines Corporation</a> held a workshop to investigate the question "How We Will Build a <a class="thought" href="entries/machine_entry.html">Machine</a> that Thinks" [27]. As you might guess from the workshop's title, the participants were not especially supportive of the arguments against <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. In fact, there was general agreement that minds can exist on <a class="thought" href="entries/nonbiological_entry.html">nonbiological</a> <a class="thought" href="entries/substrate_entry.html">substrate</a>s and that <a class="thought" href="entries/algorithm_entry.html">algorithm</a>s are of central importance to the existence of minds. However, there was much debate about the raw <a class="thought" href="entries/hardware_entry.html">hardware</a> power that is present in organic brains. A minority felt that the largest 1992 <a class="thought" href="entries/computer_entry.html">computer</a>s were within three orders of magnitude of the power of the <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a>. The majority of the participants agreed with Moravec's estimate [17] that we are ten to forty years away from <a class="thought" href="entries/hardware_entry.html">hardware</a> <a class="thought" href="entries/parity_entry.html">parity</a>. And yet there was another minority who pointed to [7] [21], and conjectured that the <a class="thought" href="entries/computation_entry.html">computation</a>al competence of single <a class="thought" href="entries/neuron_entry.html">neuron</a>s may be far higher than generally believed. If so, our present <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/hardware_entry.html">hardware</a> might be as much as <i>ten</i> orders of magnitude short of the equipment we carry around in our heads. If this is true (or for that <a class="thought" href="entries/matter_entry.html">matter</a>, if the Penrose or Searle critique is valid), we might never see a <a class="thought" href="entries/singularity_entry.html">Singularity</a>. Instead, in the early '00s we would find our <a class="thought" href="entries/hardware_entry.html">hardware</a> performance curves beginning to level off--this because of our inability to automate the design work needed to support further <a class="thought" href="entries/hardware_entry.html">hardware</a> improvements. We'd end up with some <i>very</i> powerful <a class="thought" href="entries/hardware_entry.html">hardware</a>, but without the ability to push it further. Commercial <a class="thought" href="entries/digital_entry.html">digital</a> signal processing might be awesome, giving an <a class="thought" href="entries/analog_entry.html">analog</a> appearance even to <a class="thought" href="entries/digital_entry.html">digital</a> operations, but nothing would ever "wake up" and there would never be the intellectual runaway which is the essence of the <a class="thought" href="entries/singularity_entry.html">Singularity</a>. It would likely be seen as a golden age ... and it would also be an end of <a class="thought" href="entries/progress_entry.html">progress</a>. This is very like the <a class="thought" href="entries/future_entry.html">future</a> predicted by Gunther Stent. In fact, on page 137 of [25], Stent explicitly cites the development of <a class="thought" href="entries/transhuman_entry.html">transhuman</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> as a sufficient condition to break his projections.</p>
<p>But if the technological <a class="thought" href="entries/singularity_entry.html">Singularity</a> can happen, it will. Even if all the <a class="thought" href="entries/government_entry.html">government</a>s of the world were to understand the "threat" and be in deadly fear of it, <a class="thought" href="entries/progress_entry.html">progress</a> toward the goal would continue. In fiction, there have been stories of laws passed forbidding the construction of "a <a class="thought" href="entries/machine_entry.html">machine</a> in the likeness of the <a class="thought" href="entries/human_entry.html">human</a> mind" [13]. In fact, the competitive advantage--economic, <a class="thought" href="entries/military_entry.html">military</a>, even artistic--of every advance in automation is so compelling that passing laws, or having customs, that forbid such things merely assures that someone else will get them first.</p>
<p>Eric Drexler [8] has provided spectacular insights about how far technical improvement may go. He agrees that superhuman <a class="thought" href="entries/intelligence_entry.html">intelligence</a>s will be available in the near <a class="thought" href="entries/future_entry.html">future</a>--and that such entities pose a threat to the <a class="thought" href="entries/human_entry.html">human</a> status quo. But Drexler argues that we can confine such <a class="thought" href="entries/transhuman_entry.html">transhuman</a> <a class="thought" href="entries/device_entry.html">device</a>s so that their results can be examined and used safely. This is I. J. Good's ultraintelligent <a class="thought" href="entries/machine_entry.html">machine</a>, with a dose of caution. I argue that confinement is intrinsically impractical. For the case of physical confinement: Imagine yourself locked in your home with only limited <a class="thought" href="entries/data_entry.html">data</a> <a class="thought" href="entries/access_entry.html">access</a> to the outside, to your masters. If those masters <a class="thought" href="entries/thought_entry.html">thought</a> at a rate--say, one million times slower than you, there is little doubt that over a period of years (your <a class="thought" href="entries/time_entry.html">time</a>) you could come up with "helpful advice" that would incidentally <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> you free. (I call this "fast <a class="thought" href="entries/thinking_entry.html">thinking</a>" form of <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a> "weak superhumanity". Such a "weakly superhuman" <a class="thought" href="entries/entity_entry.html">entity</a> would probably burn out in a few weeks of outside <a class="thought" href="entries/time_entry.html">time</a>. "Strong superhumanity" would be more than cranking up the clock speed on a <a class="thought" href="entries/human_entry.html">human</a>-equivalent mind. It's hard to say precisely what "strong superhumanity" would be like, but the difference appears to be profound. Imagine running a dog mind at very high speed. Would a thousand years of doggy living add up to any <a class="thought" href="entries/human_entry.html">human</a> insight? (Now if the dog mind were cleverly rewired and <i>then</i> run at high speed, we might see something different....) Many speculations about <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a> seem to be based on the weakly superhuman model. I believe that our best guesses about the post-<a class="thought" href="entries/singularity_entry.html">Singularity</a> world can be obtained by <a class="thought" href="entries/thinking_entry.html">thinking</a> on the <a class="thought" href="entries/nature_entry.html">nature</a> of strong superhumanity. I will return to this point later in the paper.)</p>
<p>Another approach to confinement is to build <i>rules</i> into the mind of the created superhuman <a class="thought" href="entries/entity_entry.html">entity</a> (for example, Asimov's Laws [3]). I think that any rules strict enough to be effective would also produce a <a class="thought" href="entries/device_entry.html">device</a> whose ability was clearly inferior to the unfettered versions (and so <a class="thought" href="entries/human_entry.html">human</a> competition would favor the development of the those more dangerous models). Still, the Asimov <a class="thought" href="entries/dream_entry.html">dream</a> is a wonderful one: Imagine a willing slave, who has 1000 times your capabilities in every way. Imagine a creature who could satisfy your every safe wish (whatever that means) and still have 99.9% of its <a class="thought" href="entries/time_entry.html">time</a> free for other activities. There would be a new <a class="thought" href="entries/universe_entry.html">universe</a> we never really understood, but filled with benevolent gods (though one of <i>my</i> wishes might be to become one of them).</p>
<p>If the <a class="thought" href="entries/singularity_entry.html">Singularity</a> cannot be prevented or confined, just how bad could the Post-<a class="thought" href="entries/human_entry.html">Human</a> era be? Well ... pretty bad. The physical <a class="thought" href="entries/extinction_entry.html">extinction</a> of the <a class="thought" href="entries/human_entry.html">human</a> race is one possibility. (Or as Eric Drexler put it of <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>: Given all that such <a class="thought" href="entries/technology_entry.html">technology</a> can do, perhaps <a class="thought" href="entries/government_entry.html">government</a>s would simply decide that they no longer need citizens!). Yet physical <a class="thought" href="entries/extinction_entry.html">extinction</a> may not be the scariest possibility. Again, <a class="thought" href="entries/analog_entry.html">analog</a>ies: Think of the different ways we relate to <a class="thought" href="entries/animal_entry.html">animal</a>s. Some of the crude physical abuses are implausible, yet.... In a Post-<a class="thought" href="entries/human_entry.html">Human</a> world there would still be plenty of niches where <a class="thought" href="entries/human_entry.html">human</a> equivalent automation would be desirable: <a class="thought" href="entries/embedded_systems_entry.html">embedded systems</a> in autonomous <a class="thought" href="entries/device_entry.html">device</a>s, self-aware <a class="thought" href="entries/daemon_entry.html">daemon</a>s in the lower functioning of larger <a class="thought" href="entries/sentient_entry.html">sentient</a>s. (A strongly superhuman <a class="thought" href="entries/intelligence_entry.html">intelligence</a> would likely be a <a class="thought" href="entries/society_of_mind_entry.html">Society of Mind</a> [16] with some very competent <a class="thought" href="entries/component_entry.html">component</a>s.) Some of these <a class="thought" href="entries/human_entry.html">human</a> equivalents might be used for nothing more than <a class="thought" href="entries/digital_entry.html">digital</a> signal processing. They would be more like whales than humans. Others might be very <a class="thought" href="entries/human_entry.html">human</a>-like, yet with a one-sidedness, a <i>dedication</i> that would put them in a mental hospital in our era. Though none of these creatures might be flesh-and-blood humans, they might be the closest things in the new enviroment to what we call <a class="thought" href="entries/human_entry.html">human</a> now. (I. J. Good had something to say about this, though at this late date the advice may be moot: Good [12] proposed a "Meta-Golden Rule", which might be paraphrased as "Treat your inferiors as you would be treated by your superiors." It's a wonderful, paradoxical idea (and most of my friends don't believe it) since the game-theoretic payoff is so hard to articulate. Yet if we were able to follow it, in some <a class="thought" href="entries/sense_entry.html">sense</a> that might say something about the plausibility of such kindness in this <a class="thought" href="entries/universe_entry.html">universe</a>.)</p>
<p>I have argued above that we cannot prevent the <a class="thought" href="entries/singularity_entry.html">Singularity</a>, that its coming is an inevitable consequence of the humans' natural competitiveness and the possibilities inherent in <a class="thought" href="entries/technology_entry.html">technology</a>. And yet ... we are the initiators. Even the largest avalanche is triggered by small things. We have the freedom to establish initial conditions, make things happen in ways that are less inimical than others. Of course (as with starting avalanches), it may not be clear what the right guiding nudge really is:</p><h1>Other Paths to the <a class="thought" href="entries/singularity_entry.html">Singularity</a>: <a class="thought" href="entries/intelligence_entry.html">Intelligence</a> Amplification</h1><p>When people speak of creating superhumanly intelligent beings, they are usually imagining an <a class="thought" href="entries/ai_entry.html">AI</a> project. But as I noted at the beginning of this paper, there are other paths to superhumanity. <a class="thought" href="entries/computer_entry.html">Computer</a> <a class="thought" href="entries/network_entry.html">network</a>s and <a class="thought" href="entries/human_entry.html">human</a>-<a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/interface_entry.html">interface</a>s seem more mundane than <a class="thought" href="entries/ai_entry.html">AI</a>, and yet they could lead to the <a class="thought" href="entries/singularity_entry.html">Singularity</a>. I call this contrasting approach <a class="thought" href="entries/intelligence_entry.html">Intelligence</a> Amplification (IA). IA is something that is proceeding very naturally, in most cases not even recognized by its developers for what it is. But every <a class="thought" href="entries/time_entry.html">time</a> our ability to <a class="thought" href="entries/access_entry.html">access</a> <a class="thought" href="entries/information_entry.html">information</a> and to communicate it to others is improved, in some <a class="thought" href="entries/sense_entry.html">sense</a> we have achieved an increase over natural <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. Even now, the team of a PhD <a class="thought" href="entries/human_entry.html">human</a> and good <a class="thought" href="entries/computer_entry.html">computer</a> workstation (even an off-net workstation!) could probably max any written <a class="thought" href="entries/intelligence_entry.html">intelligence</a> test in existence.</p>
<p>And it's very likely that IA is a much easier road to the achievement of superhumanity than pure <a class="thought" href="entries/ai_entry.html">AI</a>. In humans, the hardest development problems have already been solved. Building up from within ourselves ought to be easier than figuring out first what we really are and then building <a class="thought" href="entries/machine_entry.html">machine</a>s that are all of that. And there is at least conjectural precedent for this approach. Cairns-Smith [6] has speculated that <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/life_entry.html">life</a> may have begun as an adjunct to still more primitive <a class="thought" href="entries/life_entry.html">life</a> based on crystalline <a class="thought" href="entries/growth_entry.html">growth</a>. Lynn Margulis (in [15] and elsewhere) has made strong arguments that mutualism is a great driving force in <a class="thought" href="entries/evolution_entry.html">evolution</a>.</p>
<p>Note that I am not proposing that <a class="thought" href="entries/ai_entry.html">AI</a> <a class="thought" href="entries/research_entry.html">research</a> be ignored or less funded. What goes on with <a class="thought" href="entries/ai_entry.html">AI</a> will often have applications in IA, and vice versa. I am suggesting that we recognize that in <a class="thought" href="entries/network_entry.html">network</a> and <a class="thought" href="entries/interface_entry.html">interface</a> <a class="thought" href="entries/research_entry.html">research</a> there is something as profound (and potential wild) as <a class="thought" href="entries/ai_entry.html">Artificial Intelligence</a>. With that insight, we may see projects that are not as directly applicable as conventional <a class="thought" href="entries/interface_entry.html">interface</a> and <a class="thought" href="entries/network_entry.html">network</a> design work, but which serve to advance us toward the <a class="thought" href="entries/singularity_entry.html">Singularity</a> along the IA path.</p>
<p>Here are some possible projects that take on special significance, given the IA point of view:</p>
<ul>
<li><a class="thought" href="entries/human_entry.html">Human</a>/<a class="thought" href="entries/computer_entry.html">computer</a> team automation: Take problems that are normally considered for purely <a class="thought" href="entries/machine_entry.html">machine</a> solution (like hill-climbing problems), and design <a class="thought" href="entries/program_entry.html">program</a>s and <a class="thought" href="entries/interface_entry.html">interface</a>s that take a advantage of humans' <a class="thought" href="entries/intuition_entry.html">intuition</a> and available <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/hardware_entry.html">hardware</a>. Considering all the bizarreness of higher dimensional hill-climbing problems (and the neat <a class="thought" href="entries/algorithm_entry.html">algorithm</a>s that have been devised for their solution), there could be some very interesting displays and control tools provided to the <a class="thought" href="entries/human_entry.html">human</a> team member.</li>
<li>Develop <a class="thought" href="entries/human_entry.html">human</a>/<a class="thought" href="entries/computer_entry.html">computer</a> symbiosis in art: Combine the graphic generation capability of modern <a class="thought" href="entries/machine_entry.html">machine</a>s and the esthetic sensibility of humans. Of course, there has been an enormous amount of <a class="thought" href="entries/research_entry.html">research</a> in designing <a class="thought" href="entries/computer_entry.html">computer</a> aids for artists, as labor saving tools. I'm suggesting that we explicitly aim for a greater merging of competence, that we explicitly recognize the cooperative approach that is possible. Karl Sims [23] has done wonderful work in this direction.</li>
<li>Allow <a class="thought" href="entries/human_entry.html">human</a>/<a class="thought" href="entries/computer_entry.html">computer</a> teams at <a class="thought" href="entries/chess_entry.html">chess</a> tournaments. We already have <a class="thought" href="entries/program_entry.html">program</a>s that can play better than almost all humans. But how much work has been done on how this power could be used by a <a class="thought" href="entries/human_entry.html">human</a>, to get something even better? If such teams were allowed in at least some <a class="thought" href="entries/chess_entry.html">chess</a> tournaments, it could have the positive effect on IA <a class="thought" href="entries/research_entry.html">research</a> that allowing <a class="thought" href="entries/computer_entry.html">computer</a>s in tournaments had for the corresponding niche in <a class="thought" href="entries/ai_entry.html">AI</a>.</li>
<li>Develop <a class="thought" href="entries/interface_entry.html">interface</a>s that allow <a class="thought" href="entries/computer_entry.html">computer</a> and <a class="thought" href="entries/network_entry.html">network</a> <a class="thought" href="entries/access_entry.html">access</a> without requiring the <a class="thought" href="entries/human_entry.html">human</a> to be tied to one spot, sitting in front of a <a class="thought" href="entries/computer_entry.html">computer</a>. (This is an aspect of IA that fits so well with known economic advantages that lots of effort is already being spent on it.)</li>
<li>Develop more symmetrical decision support <a class="thought" href="entries/system_entry.html">system</a>s. A popular <a class="thought" href="entries/research_entry.html">research</a>/product area in recent years has been decision support <a class="thought" href="entries/system_entry.html">system</a>s. This is a form of IA, but may be too focused on <a class="thought" href="entries/system_entry.html">system</a>s that are oracular. As much as the <a class="thought" href="entries/program_entry.html">program</a> giving the user <a class="thought" href="entries/information_entry.html">information</a>, there must be the idea of the user giving the <a class="thought" href="entries/program_entry.html">program</a> guidance.</li>
<li>Use local area nets to make <a class="thought" href="entries/human_entry.html">human</a> teams that really work (ie, are more effective than their <a class="thought" href="entries/component_entry.html">component</a> members). This is generally the area of "groupware", already a very popular commercial pursuit. The change in viewpoint here would be to regard the group activity as a combination organism. In one <a class="thought" href="entries/sense_entry.html">sense</a>, this suggestion might be regarded as the goal of inventing a "Rules of <a class="thought" href="entries/order_entry.html">Order</a>" for such combination operations. For instance, group focus might be more easily maintained than in classical meetings. Expertise of individual <a class="thought" href="entries/human_entry.html">human</a> members could be isolated from ego issues such that the contribution of different members is focused on the team project. And of course shared <a class="thought" href="entries/data_entry.html">data</a> bases could be used much more conveniently than in conventional committee operations. (Note that this suggestion is aimed at team operations rather than political meetings. In a political setting, the automation described above would simply enforce the power of the persons making the rules!)</li>
<li>Exploit the worldwide <a class="thought" href="entries/internet_entry.html">Internet</a> as a combination <a class="thought" href="entries/human_entry.html">human</a>/<a class="thought" href="entries/machine_entry.html">machine</a> tool. Of all the items on the list, <a class="thought" href="entries/progress_entry.html">progress</a> in this is proceeding the fastest and may run us into the <a class="thought" href="entries/singularity_entry.html">Singularity</a> before anything else. The power and influence of even the present-day <a class="thought" href="entries/internet_entry.html">Internet</a> is vastly underestimated. For instance, I think our contemporary <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/system_entry.html">system</a>s would break under the weight of their own <a class="thought" href="entries/complexity_entry.html">complexity</a> if it weren't for the edge that the USENET "group mind" gives the <a class="thought" href="entries/system_entry.html">system</a> administration and support people! The very anarchy of the worldwide net development is evidence of its potential. As connectivity and <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> and archive size and <a class="thought" href="entries/computer_entry.html">computer</a> speed all increase, we are seeing something like Lynn Margulis' [15] vision of the biosphere as <a class="thought" href="entries/data_entry.html">data</a> processor recapitulated, but at a million times greater speed and with millions of humanly <a class="thought" href="entries/intelligent_agent_entry.html">intelligent agent</a>s (ourselves).</li>
</ul>
<p>The above examples illustrate <a class="thought" href="entries/research_entry.html">research</a> that can be done within the <a class="thought" href="entries/context_entry.html">context</a> of contemporary <a class="thought" href="entries/computer_science_entry.html">computer science</a> departments. There are other <a class="thought" href="entries/paradigm_entry.html">paradigm</a>s. For example, much of the work in <a class="thought" href="entries/ai_entry.html">Artificial Intelligence</a> and neural nets would benefit from a closer connection with <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/life_entry.html">life</a>. Instead of simply trying to model and understand <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/life_entry.html">life</a> with <a class="thought" href="entries/computer_entry.html">computer</a>s, <a class="thought" href="entries/research_entry.html">research</a> could be directed toward the creation of composite <a class="thought" href="entries/system_entry.html">system</a>s that rely on <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/life_entry.html">life</a> for guidance or for the providing features we don't understand well enough yet to implement in <a class="thought" href="entries/hardware_entry.html">hardware</a>. A long-<a class="thought" href="entries/time_entry.html">time</a> <a class="thought" href="entries/dream_entry.html">dream</a> of <a class="thought" href="entries/science_entry.html">science</a>-fiction has been direct <a class="thought" href="entries/brain_entry.html">brain</a> to <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/interface_entry.html">interface</a>s [2] [29]. In fact, there is concrete work that can be done (and is being done) in this area:</p>
<ul>
<li>Limb <a class="thought" href="entries/prosthetics_entry.html">prosthetics</a> is a topic of direct commercial applicability. <a class="thought" href="entries/nerve_entry.html">Nerve</a> to <a class="thought" href="entries/silicon_entry.html">silicon</a> transducers can be made [14]. This is an exciting, near-term step toward direct <a class="thought" href="entries/communication_entry.html">communication</a>.</li>
<li>Direct links into brains seem feasible, if the <a class="thought" href="entries/bit_entry.html">bit</a> rate is low: given <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/learning_entry.html">learning</a> flexibility, the actual <a class="thought" href="entries/brain_entry.html">brain</a> <a class="thought" href="entries/neuron_entry.html">neuron</a> targets might not have to be precisely selected. Even 100 bits per second would be of great use to stroke victims who would otherwise be confined to menu-driven <a class="thought" href="entries/interface_entry.html">interface</a>s.</li>
<li>Plugging in to the optic trunk has the potential for <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a>s of 1 Mbit/second or so. But for this, we need to know the fine-scale <a class="thought" href="entries/architecture_entry.html">architecture</a> of vision, and we need to place an enormous web of electrodes with exquisite precision. If we want our high <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> connection to be <i>in addition</i> to what paths are already present in the <a class="thought" href="entries/brain_entry.html">brain</a>, the problem becomes vastly more intractable. Just sticking a grid of high-<a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> receivers into a <a class="thought" href="entries/brain_entry.html">brain</a> certainly won't do it. But suppose that the high-<a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> grid were present while the <a class="thought" href="entries/brain_entry.html">brain</a> structure was actually setting up, as the <a class="thought" href="entries/embryo_entry.html">embryo</a> develops. That suggests:</li>
<li><a class="thought" href="entries/animal_entry.html">Animal</a> <a class="thought" href="entries/embryo_entry.html">embryo</a> experiments. I wouldn't expect any IA success in the first years of such <a class="thought" href="entries/research_entry.html">research</a>, but giving developing brains <a class="thought" href="entries/access_entry.html">access</a> to complex simulated neural structures might be very interesting to the people who study how the <a class="thought" href="entries/embryo_entry.html">embryo</a>nic <a class="thought" href="entries/brain_entry.html">brain</a> develops. In the long run, such experiments might produce <a class="thought" href="entries/animal_entry.html">animal</a>s with additional <a class="thought" href="entries/sense_entry.html">sense</a> paths and interesting intellectual abilities.</li>
</ul>
<p>Originally, I had hoped that this discussion of IA would yield some clearly safer approaches to the <a class="thought" href="entries/singularity_entry.html">Singularity</a>. (After all, IA allows our participation in a kind of transcendance.) Alas, looking back over these IA proposals, about all I am sure of is that they should be considered, that they may give us more options. But as for safety ... well, some of the suggestions are a little scarey on their face. One of my informal reviewers pointed out that IA for individual humans creates a rather sinister elite. We humans have millions of years of <a class="thought" href="entries/evolution_entry.html">evolution</a>ary baggage that makes us regard competition in a deadly <a class="thought" href="entries/light_entry.html">light</a>. Much of that deadliness may not be necessary in today's world, one where losers take on the winners' tricks and are coopted into the winners' enterprises. A creature that was built <i>de novo</i> might possibly be a much more benign <a class="thought" href="entries/entity_entry.html">entity</a> than one with a <a class="thought" href="entries/kernel_entry.html">kernel</a> based on fang and talon. And even the egalitarian view of an <a class="thought" href="entries/internet_entry.html">Internet</a> that wakes up along with all mankind can be viewed as a nightmare [26].</p>
<p>The problem is not simply that the <a class="thought" href="entries/singularity_entry.html">Singularity</a> represents the passing of humankind from center stage, but that it contradicts our most deeply held notions of being. I think a closer look at the notion of strong superhumanity can show why that is.</p><h1>Strong Superhumanity and the Best We Can Ask for</h1><p>Suppose we could tailor the <a class="thought" href="entries/singularity_entry.html">Singularity</a>. Suppose we could attain our most extravagant hopes. What then would we ask for: That humans themselves would become their own successors, that whatever injustice occurs would be tempered by our <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of our roots. For those who remained unaltered, the goal would be benign treatment (perhaps even giving the stay-behinds the appearance of being masters of godlike slaves). It could be a golden age that also involved <a class="thought" href="entries/progress_entry.html">progress</a> (overleaping Stent's barrier). <a class="thought" href="entries/immortality_entry.html">Immortality</a> (or at least a lifetime as long as we can make the <a class="thought" href="entries/universe_entry.html">universe</a> survive [10] [4]) would be achievable.</p>
<p>But in this brightest and kindest world, the philosophical problems themselves become intimidating. A mind that stays at the same capacity cannot live forever; after a few thousand years it would look more like a repeating tape loop than a person. (The most chilling picture I have seen of this is in [18].) To live indefinitely long, the mind itself must grow ... and when it becomes great enough, and looks back ... what fellow-feeling can it have with the <a class="thought" href="entries/soul_entry.html">soul</a> that it was originally? Certainly the later being would be everything the original was, but so much vastly more. And so even for the individual, the Cairns-Smith or Lynn Margulis notion of new <a class="thought" href="entries/life_entry.html">life</a> growing incrementally out of the old must still be valid.</p>
<p>This "problem" about <a class="thought" href="entries/immortality_entry.html">immortality</a> comes up in much more direct ways. The notion of ego and self-awareness has been the bedrock of the hardheaded rationalism of the last few centuries. Yet now the notion of self-awareness is under attack from the <a class="thought" href="entries/ai_entry.html">Artificial Intelligence</a> people ("self-awareness and other delusions"). <a class="thought" href="entries/intelligence_entry.html">Intelligence</a> Amplification undercuts our concept of ego from another direction. The post-<a class="thought" href="entries/singularity_entry.html">Singularity</a> world will involve extremely high-<a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> <a class="thought" href="entries/network_entry.html">network</a>ing. A central feature of strongly superhuman entities will likely be their ability to communicate at variable <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a>s, including ones far higher than speech or written messages. What happens when pieces of ego can be copied and merged, when the size of a selfawareness can grow or shrink to fit the <a class="thought" href="entries/nature_entry.html">nature</a> of the problems under consideration? These are essential features of strong superhumanity and the <a class="thought" href="entries/singularity_entry.html">Singularity</a>. <a class="thought" href="entries/thinking_entry.html">Thinking</a> about them, one begins to feel how essentially strange and different the Post-<a class="thought" href="entries/human_entry.html">Human</a> era will be-<i>no <a class="thought" href="entries/matter_entry.html">matter</a> how cleverly and benignly it is brought to be</i>.</p>
<p>From one angle, the vision fits many of our happiest dreams: a <a class="thought" href="entries/time_entry.html">time</a> unending, where we can truly know one another and understand the deepest mysteries. From another angle, it's a lot like the worst-case scenario I imagined earlier in this paper.</p>
<p>Which is the valid viewpoint? In fact, I think the new era is simply too different to fit into the classical frame of good and evil. That frame is based on the idea of isolated, immutable minds connected by tenuous, low-bandwith links. But the post-<a class="thought" href="entries/singularity_entry.html">Singularity</a> world <i>does</i> fit with the larger tradition of change and cooperation that started long ago (perhaps even before the rise of <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/life_entry.html">life</a>). I think there <i>are</i> notions of <a class="thought" href="entries/ethics_entry.html">ethics</a> that would apply in such an era. <a class="thought" href="entries/research_entry.html">Research</a> into IA and high-<a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> <a class="thought" href="entries/communication_entry.html">communication</a>s should improve this understanding. I see just the glimmerings of this now [32]. There is Good's Meta-Golden Rule; perhaps there are rules for distinguishing self from others on the basis of <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a> of connection. And while mind and self will be vastly more labile than in the past, much of what we value (<a class="thought" href="entries/knowledge_entry.html">knowledge</a>, <a class="thought" href="entries/memory_entry.html">memory</a>, <a class="thought" href="entries/thought_entry.html">thought</a>) need never be lost. I think Freeman Dyson has it right when he says [9]: "<a class="thought" href="entries/god_entry.html">God</a> is what mind becomes when it has passed beyond the scale of our comprehension."</p>
<p>[I wish to thank John Carroll of San Diego State University and Howard Davidson of <a class="thought" href="entries/sun_microsystems_entry.html">Sun Microsystems</a> for discussing the draft version of this paper with me.]</p>
<a name="r1"></a>
<p class="Reference"> [1] Alfve'n, Hannes, writing as Olof Johanneson, <i>The End of Man</i>?, Award Books, 1969 earlier published as "The Tale of the Big <a class="thought" href="entries/computer_entry.html">Computer</a>", Coward-McCann, translated from a book copyright 1966 Albert Bonniers Forlag AB with English translation copyright 1966 by Victor Gollanz, Ltd.</p>
<a name="r2"></a>
<p class="Reference">[2] Anderson, Poul, "Kings Who Die", <i>If</i>, March 1962, p8-36. Reprinted in <i>Seven Conquests</i>, Poul Anderson, MacMillan Co., 1969.</p>
<a name="r3"></a>
<p class="Reference">[3] Asimov, Isaac, "Runaround", <i>Astounding </i><a class="thought" href="entries/science_fiction_entry.html">Science Fiction</a>, March 1942, p94. Reprinted in <a class="thought" href="entries/robot_entry.html">Robot</a><i> Visions</i>, <a class="thought" href="entries/asimov_entry.html">Isaac Asimov</a>, ROC, 1990. Asimov describes the development of his <a class="thought" href="entries/robotics_entry.html">robotics</a> stories in this book.</p>
<a name="r4"></a>
<p class="Reference">[4] Barrow, John D. and Frank J. Tipler, <i>The Anthropic Cosmological Principle</i>, Oxford University Press, 1986.</p>
<a name="r5"></a>
<p class="Reference">[5] Bear, Greg, "Blood <a class="thought" href="entries/music_entry.html">Music</a>", <a class="thought" href="entries/analog_entry.html">Analog</a><i> </i><a class="thought" href="entries/science_fiction_entry.html">Science Fiction</a><i>-</i><a class="thought" href="entries/science_entry.html">Science</a><i> Fact</i>, June, 1983. Expanded into the <a class="thought" href="entries/novel_entry.html">novel</a> <i>Blood </i><a class="thought" href="entries/music_entry.html">Music</a>, Morrow, 1985.</p>
<a name="r6"></a>
<p class="Reference">[6] Cairns-Smith, A. G., <i>Seven Clues to the Origin of </i><a class="thought" href="entries/life_entry.html">Life</a>, Cambridge University Press, 1985.</p>
<a name="r7"></a>
<p class="Reference">[7] Conrad, Michael et al., "Toward an <a class="thought" href="entries/artificial_brain_entry.html">Artificial Brain</a>", <i>BioSystems</i>, vol 23, pp175-218, 1989.</p>
<a name="r8"></a>
<p class="Reference">[8] Drexler, K. Eric, <i><a class="thought" href="entries/engines_of_creation_entry.html">Engines of Creation</a></i>, Anchor Press/Doubleday, 1986.</p>
<a name="r9"></a>
<p class="Reference">[9] Dyson, Freeman, <i>Infinite in All Directions</i>, Harper &amp;&amp; Row, 1988.</p>
<a name="r10"></a>
<p class="Reference">[10] Dyson, Freeman, "<a class="thought" href="entries/physics_entry.html">Physics</a> and <a class="thought" href="entries/biology_entry.html">Biology</a> in an Open <a class="thought" href="entries/universe_entry.html">Universe</a>", <i>Review of Modern </i><a class="thought" href="entries/physics_entry.html">Physics</a>, vol 51, pp447-460, 1979.</p>
<a name="r11"></a>
<p class="Reference">[11] Good, I. J., "Speculations Concerning the First Ultraintelligent <a class="thought" href="entries/machine_entry.html">Machine</a>", in <i>Advances in <a class="thought" href="entries/computer_entry.html">Computer</a>s</i>, vol 6, Franz L. Alt and Morris Rubinoff, eds, pp31-88, 1965, Academic Press.</p>
<a name="r12"></a>
<p class="Reference">[12] Good, I. J., [Help! I can't find the source of Good's Meta-Golden Rule, though I have the clear recollection of hearing about it sometime in the 1960s. Through the help of the net, I have found pointers to a number of related items. G. Harry Stine and Andrew Haley have written about metalaw as it might relate to <a class="thought" href="entries/extraterrestrial_entry.html">extraterrestrial</a>s: G. Harry Stine, "How to Get along with <a class="thought" href="entries/extraterrestrial_entry.html">Extraterrestrial</a>s ... or Your Neighbor", <a class="thought" href="entries/analog_entry.html">Analog</a><i> </i><a class="thought" href="entries/science_entry.html">Science</a><i> Fact- </i><a class="thought" href="entries/science_fiction_entry.html">Science Fiction</a>, February, 1980, p39-47.] [13] Herbert, Frank, <i>Dune</i>, Berkley Books, 1985. However, this <a class="thought" href="entries/novel_entry.html">novel</a> was serialized in <a class="thought" href="entries/analog_entry.html">Analog</a><i> </i><a class="thought" href="entries/science_fiction_entry.html">Science Fiction</a><i>-</i><a class="thought" href="entries/science_entry.html">Science</a><i> Fact</i> in the 1960s.</p>
<a name="r13"></a>
<p class="Reference">[14] Kovacs, G. T. A. et al., "Regeneration Microelectrode Array for Peripheral <a class="thought" href="entries/nerve_entry.html">Nerve</a> Recording and Stimulation", <i>IEEE Transactions on Biomedical <a class="thought" href="entries/engine_entry.html">Engine</a>ering</i>, v 39, n 9, pp 893-902.</p>
<a name="r14"></a>
<p class="Reference">[15] Margulis, Lynn and Dorion Sagan, <a class="thought" href="entries/microcosm_entry.html">Microcosm</a>os, Four Billion Years of <a class="thought" href="entries/evolution_entry.html">Evolution</a> from Our Microbial Ancestors, Summit Books, 1986.</p>
<a name="r15"></a>
<p class="Reference">[16] Minsky, Marvin, <a class="thought" href="entries/society_of_mind_entry.html">Society of Mind</a>, Simon and Schuster, 1985.</p>
<a name="r16"></a>
<p class="Reference">[17] Moravec, Hans, <i>Mind Children</i>, Harvard University Press, 1988.</p>
<a name="r17"></a>
<p class="Reference">[18] Niven, Larry, "The <a class="thought" href="entries/ethics_entry.html">Ethics</a> of Madness", <i>If</i>, April 1967, pp82-108. Reprinted in <i><a class="thought" href="entries/neutron_entry.html">Neutron</a> <a class="thought" href="entries/star_entry.html">Star</a></i>, Larry Niven, Ballantine Books, 1968.</p>
<a name="r18"></a>
<p class="Reference">[19] Penrose, Roger, <i>The Emperor's New Mind</i>, Oxford University Press, 1989.</p>
<a name="r19"></a>
<p class="Reference">[20] Platt, Charles, Private <a class="thought" href="entries/communication_entry.html">Communication</a>.</p>
<a name="r20"></a>
<p class="Reference">[21] Rasmussen, S. et al., "<a class="thought" href="entries/computation_entry.html">Computation</a>al <a class="thought" href="entries/connectionism_entry.html">Connectionism</a> within <a class="thought" href="entries/neuron_entry.html">Neuron</a>s: a Model of Cytoskeletal Automata Subserving <a class="thought" href="entries/neural_network_entry.html">Neural Network</a>s", in <i>Emergent </i><a class="thought" href="entries/computation_entry.html">Computation</a>, Stephanie Forrest, ed., pp428-449, MIT Press, 1991.</p>
<a name="r21"></a>
<p class="Reference">[22] Searle, John R., "Minds, Brains, and <a class="thought" href="entries/program_entry.html">Program</a>s", in <i>The Behavioral and </i><a class="thought" href="entries/brain_entry.html">Brain</a><i> <a class="thought" href="entries/science_entry.html">Science</a>s</i>, vol 3, Cambridge University Press, 1980. The essay is reprinted in <i>The Mind's I</i>, edited by Douglas R. Hofstadter and Daniel <a class="thought" href="entries/c_entry.html">C</a>. Dennett, <a class="thought" href="entries/basic_entry.html">Basic</a> Books, 1981 (my source for this reference). This reprinting contains an excellent critique of the Searle essay.</p>
<a name="r22"></a>
<p class="Reference">[23] Sims, Karl, "Interactive <a class="thought" href="entries/evolution_entry.html">Evolution</a> of Dynamical <a class="thought" href="entries/system_entry.html">System</a>s", <a class="thought" href="entries/thinking_machines_entry.html">Thinking Machines Corporation</a>, Technical Report Series (published in <i>Toward a Practice of Autonomous <a class="thought" href="entries/system_entry.html">System</a>s: Proceedings of the First European Conference on </i><a class="thought" href="entries/artificial_life_entry.html">Artificial Life</a>, Paris, MIT Press, December 1991.</p>
<a name="r23"></a>
<p class="Reference">[24] Stapledon, Olaf, <i>The Starmaker</i>, Berkley Books, 1961 (but from the date on forward, probably written before 1937).</p>
<a name="r24"></a>
<p class="Reference">[25] Stent, Gunther S., The Coming of the Golden Age: A View of the End of <a class="thought" href="entries/progress_entry.html">Progress</a>, The Natural <a class="thought" href="entries/history_entry.html">History</a> Press, 1969.</p>
<a name="r25"></a>
<p class="Reference">[26] Swanwick Michael, <i><a class="thought" href="entries/vacuum_entry.html">Vacuum</a> Flowers</i>, serialized in <i><a class="thought" href="entries/asimov_entry.html">Isaac Asimov</a>'s </i><a class="thought" href="entries/science_fiction_entry.html">Science Fiction</a><i> Magazine</i>, December(?) 1986 - February 1987. Republished by Ace Books, 1988.</p>
<a name="r26"></a>
<p class="Reference">[27] Thearling, Kurt, "How We Will Build a <a class="thought" href="entries/machine_entry.html">Machine</a> that Thinks", a workshop at <a class="thought" href="entries/thinking_machines_entry.html">Thinking Machines Corporation</a>, August 24-26, 1992. Personal <a class="thought" href="entries/communication_entry.html">Communication</a>.</p>
<a name="r27"></a>
<p class="Reference">[28] Ulam, S., Tribute to John von Neumann, <i>Bulletin of the American Mathematical Society</i>, vol 64, nr 3, part 2, May 1958, pp1-49.</p>
<a name="r28"></a>
<p class="Reference">[29] Vinge, Vernor, "Bookworm, Run!", <a class="thought" href="entries/analog_entry.html">Analog</a>, March 1966, pp8-40. Reprinted in <i>True Names and Other Dangers</i>, <a class="thought" href="entries/vinge_entry.html">Vernor Vinge</a>, Baen Books, 1987.</p>
<a name="r29"></a>
<p class="Reference">[30] Vinge, Vernor, "True Names", <a class="thought" href="entries/binary_entry.html">Binary</a><i> <a class="thought" href="entries/star_entry.html">Star</a> Number 5</i>, <a class="thought" href="entries/dell_entry.html">Dell</a>, 1981. Reprinted in <i>True Names and Other Dangers</i>, <a class="thought" href="entries/vinge_entry.html">Vernor Vinge</a>, Baen Books, 1987.</p>
<a name="r30"></a>
<p class="Reference">[31] Vinge, Vernor, First Word, <i>Omni</i>, January 1983, p10.</p>
<a name="r31"></a>
<p class="Reference">[32] Vinge, Vernor, To Appear [:-) ].</p>
<a href="http://web.archive.org/web/20100621162543/http://www.ugcs.caltech.edu/~phoenix/vinge/vinge-sing.html" target="_new">Original article - The Singularity</a>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D4965" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id4966"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Star Maker by Olaf Stapledon<br><span class="mindxheader"><i>posted on 01/21/2002 3:30 PM by krobbins@neoversia.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id4966" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D4966" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Just for information, Star Maker was first published in 1937. It is considered by some to be a "follow up" to Last and First Men (Olaf Stapledon, 1931) which also deals with the future of intelligence.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id7274"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Star Maker by Olaf Stapledon<br><span class="mindxheader"><i>posted on 06/02/2002 9:54 PM by Citizen Blue</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id7274" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D7274" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Human evolution has taken many thousand years for its arrival. The rate in which computers have arrived to their present advancement has taken less then 500 years, and is doubling in power at an exponential rate.  My question put forth is: Why do some people have doubts about the eventual emergence of systems "waking up"? Is this the same kind of doubt many had concerning flying or the power of radio or remote control?  One confusion may arise when humans merge with computers creating a cybernetic amalgam; many may fear that this will cause us to no longer be human at some time in the future. Our coalescing may create a senario in which the computer may not per se become conscious, but makes us a super-entity where we do not allow for AI alone to become alive through built-in constraints.  We may wish to remain the dominant entity. At what point are we AI and AI is us? This may be the same classical conundrum as: If a sock is so darned that none of the originial yarn is there, then will this be the same sock.  I think the answer will be yes deterministically, and no relativisically.  This is the same exact answer evolution elicits. Oh but what does it really matter!!!</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id8411"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Star Maker by Olaf Stapledon<br><span class="mindxheader"><i>posted on 07/25/2002 10:20 AM by grantc4@hotmail.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id8411" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D8411" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>For a look at how man and machine might join to produce something like the singularity based on a trend that has already started, check out this article in Wired magazine:
<br>
<br>
http://www.wired.com/wired/archive/10.08/korea.html
<br>
<br>
If the Koreans (or any social group) were to devote the same organization and energy to the pursuit of science as they are devoting to game playing, who knows what they might produce?</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id8220"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 07/14/2002 10:43 AM by trait70426@aol.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id8220" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D8220" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>footnote #32 is in reference to the horrible "focused persons" from his "A Deepness In The Sky".</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id8405"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 07/25/2002 12:49 AM by tabdelgawad@yahoo.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id8405" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D8405" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>With all the discussion about IA and human-machine interfaces, couldn't the author at least hyperlink the footnotes?!
<br>
<br>
Meant more as an ironic commentary than a criticism :-)
<br>
<br>
Tamer</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id9433"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 08/30/2002 2:31 PM by cfeigel@got.net</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id9433" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D9433" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>
<br>
<br>
Still, it's pretty impressive for a species that can't even learn to use turn signals!
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id27870"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 09/16/2004 2:19 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=1434">bigdog</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id27870" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D27870" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Turn signals? Try evolutionary progression as a whole! There is nothing that brings out the stupid in people faster than putting them in front of a computer. Which brings me to wonder what will happen to the masses once humans and machines establish a well-defined link. Will they be helpless to catch up to the technological evolution explosion? Will they immediately be caught up with a Utopian-style upgrade? Or will megacorporations make millions on EACH person that wants the link installed?? If the common grunt has access to this technology, will it be like that which is commented in "Merging Mind and Machine...," where PhD's no longer matter? Any bum off the street will be put in a position of great power? I'd like to think of it as similar to a cartoon I saw, where one of the heros is losing his superhuman intelligence, and can no longer process the flood of information that his hardware-assisted sensory inputs are handing to him. Will intelligence make a difference in the long run after all, where a slight edge will put you light-years ahead of your competitors? It's certainly been true since the beginning of time. Needless to say, it's the very Law of Evolution. What if the Singularity is the very end of existence as we know it? Is it possible for machines to 'observe' an event, as defined in Quantum Physics? If not, and the machines do wipe out all humanity, then this Universe will cease to exist. On the other hand, if they are sentient, and we are no longer around, who is to say the machines exist at all? Other machines? We are definitely on the verge of something beyond all comprehension and speculation. And who is to say we will comprehend it AFTER it happens???</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id27873"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="60"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="619"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 09/16/2004 4:23 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=1417">joanneB</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id27873" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D27873" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>You ask: "Will intelligence make a difference in the long run after all, where a slight edge will put you light-years ahead of your competitors?"... Yet you make a distinction between a bum and a Phd?</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id27876"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="80"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="599"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 09/16/2004 6:20 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=1434">bigdog</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id27876" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D27876" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Read this article:
<br>
<a href="http://web.archive.org/web/20100621162543/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0552.html" target="_blank">http://www.kurzweilai.net/meme/frame.html?main=/ar  ticles/art0552.html</a>
<br>
:)</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id67286"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 09/12/2006 3:43 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=3169">mindx back-on-track</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id67286" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D67286" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>back-on-track</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id125314"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 06/19/2008 7:42 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=4884">PredictionBoy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id125314" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D125314" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Let's examine the beginning of the article carefully
<br>
<br>
<center><p class="mindxquote"> The acceleration of technological progress has been the central feature of this century. I argue in this paper that we are on the edge of change comparable to the rise of human life on Earth. The precise cause of this change is the imminent creation by technology of entities with greater than human intelligence. There are several means by which science may achieve this breakthrough (and this is another reason for having confidence that the event will occur):
<br>
<br>
There may be developed computers that are "awake" and superhumanly intelligent. (To date, there has been much controversy as to whether we can create human equivalence in a machine. But if the answer is "yes, we can", then there is little doubt that beings more intelligent can be constructed shortly thereafter.) 
<br>
Large computer networks (and their associated users) may "wake up" as a superhumanly intelligent entity. 
<br>
Computer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent. 
<br>
Biological science may provide means to improve natural human intellect. 
<br>
The first three possibilities depend in large part on improvements in computer hardware. Progress in computer hardware has followed an amazingly steady curve in the last few decades [17]. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Charles Platt [20] has pointed out that AI enthusiasts have been making claims like this for the last thirty years. Just so I'm not guilty of a relative-time ambiguity, let me more specific: I'll be surprised if this event occurs before 2005 or after 2030.)
<br>
</p></center> </p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id125315"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 06/19/2008 7:54 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=4884">PredictionBoy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id125315" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D125315" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p> <center><p class="mindxquote"> There may be developed computers that are "awake" and superhumanly intelligent. (To date, there has been much controversy as to whether we can create human equivalence in a machine. But if the answer is "yes, we can", then there is little doubt that beings more intelligent can be constructed shortly thereafter.) </p></center>
<br>
Vinge makes the same mistake as everyone else: what does he mean by 'more intelligent' than human?
<br>
<br>
<center><p class="mindxquote"> Large computer networks (and their associated users) may "wake up" as a superhumanly intelligent entity.  </p></center>
<br>
this does not seem credible or likely as a real possibility with a significant probability of occurrence. if it was, the internet would be showing at least a little of that now - and it is not
<br>
<br>
<center><p class="mindxquote"> Computer/human interfaces may become so intimate that users may reasonably be considered superhumanly intelligent.  </p></center>
<br>
the more i think about this, the less likely it seems, in terms of across-the-board increasing someone's intelligence
<br>
<br>
again, we need to define what 'more intelligent' is. in addition, we must acknowledge that the mature human brain is a trained bunch of neurons, tho individually dynamic, rather static overall, especially as one gets older. 
<br>
<br>
altho i can imagine augmenting a human's mental performance in certain, specific ways, across the board making someone brilliant at, say, tennis, who has never shown such proclivity before, how would that work - if only the brain is treated in this way
<br>
<br>
<center><p class="mindxquote"> Biological science may provide means to improve natural human intellect. </p></center>
<br>
<br>
this may prove possible, but there will be nothing moorseian about this path trajectory
<br>
<br>
<center><p class="mindxquote"> The first three possibilities depend in large part on improvements in computer hardware.  </p></center>
<br>
A-ha! At least one smoking gun. This should be corrected to say, The first three possibilities depend in large part on improvements in computer hardware AND software.
<br>
<br>
<center><p class="mindxquote"> Progress in computer hardware has followed an amazingly steady curve in the last few decades [17]. Based largely on this trend, I believe that the creation of greater than human intelligence will occur during the next thirty years. (Charles Platt [20] has pointed out that AI enthusiasts have been making claims like this for the last thirty years. Just so I'm not guilty of a relative-time ambiguity, let me more specific: I'll be surprised if this event occurs before 2005 or after 2030.)  </p></center>
<br>
vinge goes on to make a pure hardware case for greater-than-human intelligence.
<br>
<br>
tho hardware power is necessary, it is not sufficient for greater than human ai - software is just as important
<br>
<br>
this is a crucial oversight, what made vinge overlook software?
<br>
<br>
some possibilities:
<br>
 - starstruck by moore's law, as many here are as well
<br>
 - the lack of realization that innovations like the internet dont really help with the central problems of s/w dev, at least not much
<br>
 - etc</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id125316"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 06/19/2008 8:12 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=4884">PredictionBoy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id125316" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D125316" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>from the previous post, it is clear that the sing is looking a little shaky, in terms of moore's law being the main pillar of evidence
<br>
<br>
heres the next section
<br>
<br>
<center><p class="mindxquote"> What are the consequences of this event? When greater-than-human intelligence drives progress, that progress will be much more rapid. </p></center> this makes sense, with one critical caveat: who defines 'progress'? ie, who determines what is to be done in the first place?
<br>
<br>
to me, there is absolutely no reason to believe that tech is going to take that determination of objectives away from humans. humans will never relinquish that role, and advanced ai has little reason to attempt to usurp it.
<br>
<br>
therefore, the participation of humans will most definitely form a speed brake on all this automated 'progress'
<br>
<br>
<center><p class="mindxquote">  In fact, there seems no reason why progress itself would not involve the creation of still more intelligent entities--on a still--shorter time scale. </p></center>
<br>
actually, this is a somewhat flaccid statement. the real challenge vinge should have faced is, why would greater-than-human intelligence necessarily care only about increasing its own intelligence? what if that isnt in its design objectives? 
<br>
<br>
<center><p class="mindxquote">  The best analogy that I see is with the evolutionary past: Animals can adapt to problems and make inventions, but often no faster than natural selection can do its work--the world acts as its own simulator in the case of natural selection. We humans have the ability to internalize the world and conduct "what if's" in our heads; we can solve many problems thousands of times faster than natural selection.  </p></center>
<br>
this is misleading; natural selection gave us the ability to solve many problems thousands of times faster than other animals. natural selection is the one that brought us to the dance, not something to stack our own brainpower against.
<br>
<br>
<center><p class="mindxquote"> Now, by creating the means to execute those simulations at much higher speeds, we are entering a regime as radically different from our human past as we humans are from the lower animals. </p></center>
<br>
the thing is, machine intelligence, like human intelligence before it, will be shaped by the objectives that it must respond to. 
<br>
<br>
in other words, the simulation will have to be set up and managed in such a way as to produce specified outcomes. humans will never leave that to machines alone, until and unless we are positively certain that whatever an automated intelligence comes up with, corresponds with our own human interest.
<br>
<br>
<center><p class="mindxquote"> From the human point of view this change will be a throwing away of all the previous rules, perhaps in the blink of an eye, an exponential runaway beyond any hope of control. Developments that before were thought might only happen in "a million years" (if ever) will likely happen in the next century. (In [5], Greg Bear paints a picture of the major changes happening in a matter of hours.)
<br>
</p></center>
<br>
"an exponential runaway beyond any hope of control".  Why is that? even assuming the instantaneous nature of this process, a logical stretch to say the least, why is lack of control thrown into the mix, there seems to be no reason for it.
<br>
<br>
For example, why wouldnt a greater intelligence be more in control than a lower intelligence? that makes as much if not more sense
<br>
<br>
for more reasons on why the sing will be of a very definitely finite, not instant, duration:
<br>
<br>
<a href="http://web.archive.org/web/20100621162543/http://predictionboy.blogspot.com/2007/10/exploring-theoretical-post.html" target="_blank">http://predictionboy.blogspot.com/2007/10/explorin  g-theoretical-post.html</a>
<br>
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id125318"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="60"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="619"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 06/19/2008 8:35 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=4884">PredictionBoy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id125318" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D125318" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>next section:
<br>
<br>
<center><p class="mindxquote"> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any any man however clever.  </p></center>
<br>
stop right there - this is ridiculous, in terms of pragmatic reality. this is the heart of the 'just like human' fallacy of ai
<br>
<br>
for example, will this ultraintelligent machine be better at getting girls than the average man? perhaps, but why would it, if it hasnt been designed to do that?
<br>
<br>
it doesnt have a true sex drive; humans do. that sex drive has an enormous impact on our brains, much of our brains are organized around that activity
<br>
<br>
but, does that mean we'll build asexual beings that in fact have a perceived sex drive, like one that truly influences their behavior?
<br>
<br>
no, because theres no reason to. there will be a 1000 things like that, 10,000 maybe, where whats in our head has no rhyme or reason in a synthetic mind. so those 10,000 things will be omitted, or perhaps replaced with other things, more appropriate to a synthetic intelligence
<br>
<br>
and just that, by itself, guarantees that the advanced ai wont be 'just as clever' as us humans, certainly not 'in every way'
<br>
<br>
<br>
<br>
<center><p class="mindxquote"> Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an "intelligence explosion,"  </p></center>
<br>
why would there 'unquestioningly' be an intelligence explosion? do we know enough about intelligence to even be able to say that intelligence does not have practical limits, at least in certain ways. this is a most critical conclusion, given almost no logical foundation.
<br>
<br>
<center><p class="mindxquote"> and the intelligence of man would be left far behind </p></center>
<br>
not if the nature of that advanced ai is in fact complementary to human intelligence, especially if it gets more complementary as it gets more intelligent.
<br>
<br>
thats exactly the case ive demonstrated exhaustively. vinge just makes a statement, assumes its obvious, then moves on. 
<br>
<br>
<center><p class="mindxquote">  Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. ... It is more probable than not that, within the twentieth century, an ultraintelligent machine will be built and that it will be the last invention that man need make. </p></center>
<br>
this is silly, for reasons that i have analyzed exhaustively. the machine will not be 'docile', because it will not be controlled by emotion, it will be rational controlled. 
<br>
<br>
vinge would need to make a much more thorough case that such a device is essentially the last thing we'll ever need to do, especially as ive described that we humans will still have plenty to do, but advanced ai will help of course
<br>
<br>
<center><p class="mindxquote"> Good has captured the essence of the runaway, but does not pursue its most disturbing consequences. </p></center>
<br>
he may have captured it, but he certainly hasnt explained why its logically inevitable.
<br>
<br>
<center><p class="mindxquote">  Any intelligent machine of the sort he describes would not be humankind's "tool"--any more than humans are the tools of rabbits or robins or chimpanzees.
<br>
</p></center>
<br>
the answer to that depends on its nature. there is no reason a hyperintelligent device cant be a tool. just as pcs are 'thousands of times more intelligent' when it comes to multiplying numbers. yet, humans are not obsolete, by a long shot</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id125335"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="80"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="599"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 06/20/2008 1:29 AM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id125335" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D125335" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Some important points could be made:
<br>
<br>
1. We are understandably afraid of even imagening the things that are possible, much less discussing them - so terrible they look for outsider. Well, we must take history one step at a time. We will cross every bridge when we get there. Every day has enough worries of its own.
<br>
<br>
2. Some special "Superhuman intelligence" is an oximoron, since "intelligence" covers everything.
<br>
<br>
3. The challenge that SAI will face intellectually, is not simply to be higher than summ of human intelligences. The reason for that is, the collective ways that humans iteract.
<br>
<br>
4. The actual physical limitations on hardware powers are likely to allow creation of higher than human intelligence, but not millions times higher. No computer Gods in cards, sorry.
<br>
<br>
The thing still is, that even moderate handicap in intelligence can totally screw us up and produce unemployment in intellectually charged professions - if we will have to compete with machines.
<br>
<br>
:)
<br>
<br>
eS
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id125723"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="100"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="579"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 06/23/2008 12:27 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=6436">BLUEquist</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id125723" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D125723" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I dunno...
<br>
<br>
Though the wormoform ones are not as as much, the omnicrom ones are purdy on the outside; moveover, the wormoform ones are confusing to look at. Omnicrom are all orderly in appearance from the middle outward (from their outside), and the wormoform move strangely and tend to hide. Both look like they are extremely frustrated, one on the outside, the other in the middle, respectively.
<br>
<br>
Other than that... certainly things remain uncertain for now &amp; never will. 
<br>
<br>
*Vomits*</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id177808"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="120"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="559"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 02/18/2010 5:37 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=471">eldras</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id177808" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D177808" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>the recent development at Michigan University where Prof John Holland has his seat (he discovered Genetic Algorithms) has received US gvmt's $25 Million to set up a GA experiment/lab in addition to the oldest running GA sequence (E Coli 50,000 mutations c. 1988--&gt; present).
<br>
<br>
<br>
there is a mooted hope tat sufficient guided simulations can be done to make intelligence.
<br>
<br>
If human intelligence took x many evolutions from non intelligents, it may be possible with design and guidance to do that intelligence level is a fraction of time.
<br>
<br>
This is dependent on sufficient computing resources.
<br>
<br>
However Computers are very different now to when Vernor wrote this famous article.
<br>
<br>
Farming is possible.
<br>
<br>
A few Sony playstations (P3's @ present)can be linked to give a petaflops of processing power.
<br>
<br>
The GRIDs have become sophisticated...a systems where supercomputers join on a shared-time basis round the world.
<br>
<br>
The resulting power is awesome.
<br>
<br>
CERN is developing the smart grid to handle the masses of data generated by its 7,000 strong team of high caibre scientists in Europe. 
<br>
<br>
But this grid can be used for other things.
<br>
<br>
By 2018 IBM and others possibly from Chiunba and Germany are expected to bring 10/\18 flops computers which when gridded will produce enough power to enable simple heuristic trials of evolutionary simulations.
<br>
<br>
AGI conferences are underway in an attempt to build SAI that Vinge warns about as being potentially dangerous.
<br>
<br>
<br>
His window is 2005-2030 for the advent of post-human intelligence based on trends and work underway as at 1993.
<br>
<br>
Kurzweil has noted a double exponential trend in accelerating technology, which if continuing as forecast could deliver something utterly beyond our total present technologies in the 2020's as intelligence amplifies.
<br>
<br>
Can we handle it... because I seriously doubt it)
<br>
and how exactly do we?
<br>
<br>
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id177815"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="140"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="539"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 02/18/2010 6:17 PM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=8304">Pandemonium1323</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id177815" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D177815" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p> <center><p class="mindxquote"> Can we handle it... because I seriously doubt it) 
<br>
and how exactly do we? 
<br>
</p></center>
<br>
By merging with our technology before SAI comes along
<br>
Thank you
<br>
Drive thru</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id177890"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="160"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="519"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The Technological Singularity<br><span class="mindxheader"><i>posted on 02/19/2010 5:08 AM by <a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/profile.php?id=471">eldras</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4965%23id177890" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621162543/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D177890" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>pandemonium, 
<br>
<br>
that is certainly part of the answer.
<br>
<br>
But we must struggle to the utmost to find others as well.
<br>
<br>
<br>
Such a safety is an emergence of Intelligence Amplification, and when you take the whole together it is evident the CIVILISATION is superintelligent.
<br>
<br>
I cannot e satisfied with concluding thus,
<br>
<br>
I still search for better, safer, more complete ways.
<br>
<br>
A project of AGI may well succeed- one we haven't heard of yet- that could threaten all life.
<br>
<br>
The only sure way to pre-empt that is to build a successful one first and neutralize the others to make things safe.
<br>
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621162543im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>