<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>The Age of Intelligent Machines, Chapter Seven: The Moving Frontier</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/meme/memelist.html?m=7">Visions of the Future</a> &gt; 
<a href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/meme/memelist.html?m=12">The Age of Intelligent Machines</a> &gt; 
The Age of Intelligent Machines, Chapter Seven: The Moving Frontier
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20100614002219/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0312.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0312.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/articles/art0312.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">The Age of Intelligent Machines, Chapter Seven: The Moving Frontier</span>
<br>
<span class="Subtitle">Pattern Recognition: The search for Order</span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td><td><span class="Authors"> <a class="Authors" href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0005.html" target="_top">    Raymond   Kurzweil </a><br>
</span></td>
</table>
<br>
<div class="TeaserText">From <a class="thought" href="entries/kurzweil_entry.html">Ray Kurzweil</a>'s revolutionary book <a class="thought" href="entries/age_of_intelligent_machines_entry.html">The Age of Intelligent Machines</a>, published in 1990.</div>
<br>
<br>
<blockquote>The digitization of <a class="thought" href="entries/information_entry.html">information</a> in all its forms will probably be known as the most fascinating development of the twentieth century. </blockquote>
<blockquote>An Wang, founder of Wang Laboratories</blockquote>
<blockquote>Most probably, we think, the human brain is, in the main, composed of large numbers of relatively small distributed systems, arranged by embryology into a complex <a class="thought" href="entries/society_entry.html">society</a> that is controlled in part (but only in part) by serial, symbolic systems that are added later. But the subsymbolic systems that do most of the work from underneath must, by their very character, block all the other parts of the brain from knowing much about how they work. And this, itself, could help explain how people do so many things yet have such incomplete ideas of how those things are actually done. </blockquote>
<blockquote>
<a class="thought" href="entries/minsky_entry.html">Marvin Minsky</a> and Seymour Papert, the 1988 epilogue to <i>Perceptrons</i>
</blockquote><h1>Vision</h1><h2>Two types of <a class="thought" href="entries/thinking_entry.html">thinking</a></h2><p>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones01.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<br>Try not to think about elephants. For the next sixty seconds, do not let the image of these huge mammals with their large ears and swaying trunks enter your mind. Now look across the room and focus your vision on an <a class="thought" href="entries/object_entry.html">object</a>. Without closing your eyes or turning them away, try not to determine what the <a class="thought" href="entries/object_entry.html">object</a> is. Finally, consider the Tower of Hanoi problem described in the last chapter. For the next sixty seconds, do not solve this problem.</p>
<p>You are undoubtedly having difficulty avoiding the mental image of an elephant. Assuming that the <a class="thought" href="entries/object_entry.html">object</a> that you selected to look at is not unknown to you, you were probably unsuccessful as well in not determining its <a class="thought" href="entries/identity_entry.html">identity</a>. On the other hand, unless you have an unusual passion for <a class="thought" href="entries/mathematics_entry.html">mathematics</a>, you are probably experiencing little difficulty in not solving the Tower of Hanoi problem.</p>
<p>Two types of <a class="thought" href="entries/thought_entry.html">thought</a> processes coexist in our brains, and the above exercises illustrate one of the profound differences between them. Perhaps most often cited as a uniquely human form of <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is the logical process involved in solving problems and playing games. A more ubiquitous form of <a class="thought" href="entries/intelligence_entry.html">intelligence</a> that we share with most of the earth's higher <a class="thought" href="entries/animal_entry.html">animal</a> <a class="thought" href="entries/species_entry.html">species</a> is the ability to recognize patterns from our visual, auditory, and tactile senses. We appear to have substantial control over the sequential steps required for logical <a class="thought" href="entries/thought_entry.html">thought</a>. In contrast, <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a>, while very complex and involving several levels of <a class="thought" href="entries/abstraction_entry.html">abstraction</a>, seems to happen without our conscious direction.<sup>1</sup> It is often said that a master chess player can "see" his or her next move without going through all of the conscious sequences of <a class="thought" href="entries/thinking_entry.html">thinking</a> required of less experienced players. It may be that after being exposed to tens of thousands of board situations, the master player is able to replace at least some of the logical processes usually used to play games with <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition methods.<sup>2</sup>
</p>
<p>There are several key differences between these two forms of <a class="thought" href="entries/intelligence_entry.html">intelligence</a>, including the level of success the <a class="thought" href="entries/ai_entry.html">AI</a> field has had in emulating them.<sup>3</sup> Ironically, we find it easier to create an artificial mathematician or master chess player than to emulate the abilities of animals. While there are many <a class="thought" href="entries/animal_entry.html">animal</a> capabilities that our machines have not yet mastered, including the intricacies of fine motor coordination, the most difficult barrier has been the subtleties of vision, our most powerful sense and a prime example of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a>.</p>
<p>One <a class="thought" href="entries/attribute_entry.html">attribute</a> that the two types of <a class="thought" href="entries/thinking_entry.html">thinking</a> have in common is the use of <a class="thought" href="entries/imagination_entry.html">imagination</a>. The first example cited above, imagining an elephant, is a direct exercise in <a class="thought" href="entries/imagination_entry.html">imagination</a>. The second example, identifying an <a class="thought" href="entries/object_entry.html">object</a>, also involves <a class="thought" href="entries/imagination_entry.html">imagination</a>, particularly in the latter stages of the process. If part of the <a class="thought" href="entries/object_entry.html">object</a> we are trying to identify is blocked or if its orientation toward us prevents the most useful view, we use our <a class="thought" href="entries/imagination_entry.html">imagination</a> to visualize in our minds what the full <a class="thought" href="entries/object_entry.html">object</a> might look like and then determine if the imagined <a class="thought" href="entries/object_entry.html">object</a> matches what we can see. In fact, we almost always use our <a class="thought" href="entries/imagination_entry.html">imagination</a> to visualize an <a class="thought" href="entries/object_entry.html">object</a>, because invariably there are sides we cannot see.<sup>4</sup> The technical term for this technique is "<a class="thought" href="entries/hypothesis_entry.html">hypothesis</a> and test"; we use our <a class="thought" href="entries/imagination_entry.html">imagination</a> to hypothesize the answer and then test its validity.<sup>5</sup> <a class="thought" href="entries/hypothesis_entry.html">Hypothesis</a> and test is also used in logical <a class="thought" href="entries/thought_entry.html">thought</a>. We often imagine an answer to a logical problem based on methods of <a class="thought" href="entries/intuition_entry.html">intuition</a> that are only partially understood and then work backward to the original problem statement.</p>
<p>If we examine the <a class="thought" href="entries/nature_entry.html">nature</a> of our visual <a class="thought" href="entries/imagination_entry.html">imagination</a>, we can gain some insight into the most important way in which <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> differs from logical rule-based <a class="thought" href="entries/thinking_entry.html">thinking</a>. Consider again your <a class="thought" href="entries/imagination_entry.html">imagination</a> of an elephant. Your mental picture probably does not include a great deal of detail: it is essentially a line drawing, probably one that is moving (I'll bet the trunk is swaying back and forth). The <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> of the line drawing-the fact that we recognize a line drawing of an <a class="thought" href="entries/object_entry.html">object</a> (a face, for example) as representing that <a class="thought" href="entries/object_entry.html">object</a> though the drawing is markedly simpler than the original image-provides us with an important clue to the <a class="thought" href="entries/nature_entry.html">nature</a> of the transformations performed during the process of human vision. Very important to the recognition of visual objects is the identification of edges, which we model in two dimensions as lines. If we explore what is required to extract edges from an image, we shall gain an appreciation of one major way in which visual <a class="thought" href="entries/perception_entry.html">perception</a> differs from logic.</p>
<p>There are many aspects of visual <a class="thought" href="entries/perception_entry.html">perception</a> that we do not yet understand, but some understanding of the identification of edge location and orientation has been achieved.<sup>6</sup> A particular set of computations has been discovered that is capable of detecting edges with reasonable accuracy. There is some evidence that similar techniques are used in visual processing by mammals. The technique is based on two observations. The first observation is that we need to smooth the data; changes involving tiny regions can probably be considered to be non-<a class="thought" href="entries/information_entry.html">information</a>-bearing visual noise. Thus, small defects in edges can be ignored, at least initially, in locating all of the edges in an image. Second, we note that changes in the visual <a class="thought" href="entries/information_entry.html">information</a> (across any spatial dimension) are more important than the <a class="thought" href="entries/information_entry.html">information</a> itself. In other words, we are primarily interested in sudden and consistent alterations in color or shading from one region to another.</p>
<p>I shall now describe a <a class="thought" href="entries/method_entry.html">method</a> for inferring edges from visual images.<sup>7</sup> The following two paragraphs are somewhat technical. Yet it is not necessary to understand all of these details to appreciate some of the implications of the <a class="thought" href="entries/method_entry.html">method</a>. The image itself is represented by a two-dimensional array of pixels, or points of <a class="thought" href="entries/information_entry.html">information</a>. In a black and white image, each pixel can be represented by a single <a class="thought" href="entries/number_entry.html">number</a> representing a shade of gray. In a color image, several numbers (usually three) are required to represent the color and shade. We can take this initial raw image and modify it to take advantage of the two observations cited above. The modification is achieved by applying what is called a filter, in which each pixel has an influence on its surrounding pixels. For example, a Gaussian filter designates certain pixels as propagating pixels; it then increases the intensity of each pixel in the vicinity of each propagating pixel on the basis of the intensity of the propagating pixel and the distance to the neighboring pixel. The function of intensity to distance is based on the familiar Gaussian (normal) curve, with the peak of the curve representing zero distance (that is, the propagating pixel itself). A Gaussian filter is applied to an image by making every pixel a propagating pixel; thus, all pixels bleed into their surrounding pixels. This has the impact of smoothing the image, with the sharpness of the resulting image being a function of the width of the Gaussian curve. A different filter, the Laplacian, can then be applied to detect changes. This filter replaces the value of every pixel with the rate of change of the rate of change (that is, the second derivative) of the pixel values.</p>
<p>These two processes-smoothing and determining rates of rates of change-can be combined into a single filter in which every pixel influences all of the pixels within its vicinity. This filter, with the appropriate, if forbidding, name of "Laplacian of a Gaussian convolver," has a graph with the shape of an upside-down Mexican hat, so it is often called a <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a>. As the figure shows, each pixel has a positive influence on the pixels in its immediate vicinity and a negative influence on pixels in a band surrounding the immediate vicinity. Once the <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a> has been applied, edges can be inferred by looking for zero crossings, places where values change from negative to positive.<sup>8</sup>
</p>
<p>Let us consider some of the implications of this process. First, the technique is not particularly complicated. Second, experiments have shown that it is reasonably successful. In general, edges are correctly inferred. False hypotheses are generated, but these can be eliminated by later processing that incorporates <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the types of objects we expect to see in the environment and the <a class="thought" href="entries/nature_entry.html">nature</a> of their edges.<sup>9</sup> Third, there is evidence that the <a class="thought" href="entries/hardware_entry.html">hardware</a> exists in mammalian brains to perform this type of transformation. For example, David H. Hubel and Torsten N. Wiesel of Harvard Medical School have discovered specialized edge detector cells in the outer (early) layers of the visual cortex of the human brain.<sup>10</sup>
</p>
<p>Most important is a conclusion we can draw regarding the amount of <a class="thought" href="entries/computation_entry.html">computation</a> required to perform <a class="thought" href="entries/edge_detection_entry.html">edge detection</a>. While it has not been proved that this precise filter, the Laplacian of a Gaussian convolver, is used in <a class="thought" href="entries/mammal_entry.html">mammal</a> vision, it can be shown that any <a class="thought" href="entries/algorithm_entry.html">algorithm</a> that could possibly perform <a class="thought" href="entries/edge_detection_entry.html">edge detection</a> with the facility of human (and apparently most <a class="thought" href="entries/mammal_entry.html">mammal</a>) vision must use a center-surround filter (a filter in which each pixel influences all pixels within a certain distance) that requires a comparable amount of <a class="thought" href="entries/computation_entry.html">computation</a>. This amount turns out to be vast and is determined by a six-dimensional <a class="thought" href="entries/computation_entry.html">computation</a>. First, the filter must be applied for every pixel, and the pixels are organized in a two-dimensional array. For each pixel we must apply the filter to all pixels in a two-dimensional array surrounding that pixel, which gives us a four-dimensional <a class="thought" href="entries/computation_entry.html">computation</a>. We noted earlier that the sharpness of our edge analysis was a function of the size of the Gaussian (normal) curve applied. In the combined <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a>, the size of the Mexican hat has the same impact. A large sombrero will enable us to detect the edges of large objects; a small sombrero will detect smaller features. We thus need to perform this entire <a class="thought" href="entries/computation_entry.html">computation</a> several times, which is a fifth dimension. The sixth dimension is time; since vision must be capable of dealing with moving images, this entire <a class="thought" href="entries/computation_entry.html">computation</a> must be repeated many times each second. Undoubtedly, some optimizations can be applied. For example, if we note that portions of the image are not changing, it is not necessary to repeat all of the computations. Nonetheless, the <a class="thought" href="entries/number_entry.html">number</a> of computations required is essentially determined by this six-dimensional array.<sup>11</sup>
</p>
<p>Let us plug in some numbers to get a feeling for the orders of magnitude involved. <a class="thought" href="entries/human_entry.html">Human</a> vision is estimated to have a resolution of 10,000 positions along each of the two axes of vision, or about 100 million pixels (there are indeed about 100 million rod cells in each eye to detect shape and <a class="thought" href="entries/motion_entry.html">motion</a> and 6 million cone cells to detect color and fine detail).<sup>12</sup> The diameter of typical sombrero fitters used in <a class="thought" href="entries/computer_entry.html">computer</a>-vision experiments range from 10 to 30 pixels, but these experiments are based on images of only 1,000 pixels on a side. A reasonable average size for a human <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a> would be about 100 by 100 pixels. If we assume about 3 different sombreros for different size objects and a refresh rate of recomputing the image of 30 times per second, we have the following <a class="thought" href="entries/number_entry.html">number</a> of multiplications per second: 10,000 x 10,000 x 100 x 100 x 3 x 30, or about 100 trillion. Now, a typical <a class="thought" href="entries/personal_computer_entry.html">personal computer</a> can perform about 100,000 multiplications per second. Thus, we would need about a billion personal computers to match the <a class="thought" href="entries/edge_detection_entry.html">edge detection</a> capability of human vision, and that's just for one eye!<sup>13</sup>
</p>
<p>Typical <a class="thought" href="entries/computer_entry.html">computer</a> vision systems have somewhat less demanding specifications. Typically image resolution is about 1,000 by 1,000 pixels, which requires smaller filters of about 25 by 25 pixels. With three filters of different sizes and a refresh rate of 30 images per second, we have 1,000 x 1,000 x 25 x 25 x 3 x 30, or only 60 billion multiplications per second, which could be handled in real time by a mere 600,000 personal computers.</p>
<p>This brings us back to the issue of <a class="thought" href="entries/digital_entry.html">digital</a> versus <a class="thought" href="entries/analog_entry.html">analog</a> <a class="thought" href="entries/computation_entry.html">computation</a>. As mentioned earlier, the need for massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> (doing many computations at the same time) may reverse, at least partially, the trend away from <a class="thought" href="entries/analog_entry.html">analog</a> computing. While it is possible to achieve billions of <a class="thought" href="entries/digital_entry.html">digital</a> computations per second in our more powerful supercomputers, these systems are large and expensive. The computations described above for the <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a> do not need high degrees of accuracy or repeatability, so <a class="thought" href="entries/analog_entry.html">analog</a> multiplications would be satisfactory. Multiplying 60 billion <a class="thought" href="entries/analog_entry.html">analog</a> numbers per second (600,000 computing elements each performing 100,000 multiplications per second) could be achieved using <a class="thought" href="entries/vlsi_entry.html">VLSI</a> circuits in a relatively compact <a class="thought" href="entries/system_entry.html">system</a>. Even the 100 trillion multiplications per second required for human vision, though out of the question using <a class="thought" href="entries/digital_entry.html">digital</a> circuits, is not altogether impractical using <a class="thought" href="entries/analog_entry.html">analog</a> techniques. After all, the human brain accomplishes image-filtering tasks using just this combination of methods: massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> and <a class="thought" href="entries/analog_entry.html">analog</a> <a class="thought" href="entries/computation_entry.html">computation</a>.<sup>14</sup>
</p>
<p>The human visual <a class="thought" href="entries/system_entry.html">system</a> picks up an image with 100 million specialized (rod and cone) cells. Multiple layers, each of a comparable <a class="thought" href="entries/number_entry.html">number</a> of cells, would have the capability to perform transformations similar to the <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a> described above. In fact, the visual cortex of the brain contains hundreds of layers, so these filtering steps are but the first transformations in the long (but quick) journey of processing that a visual image undergoes.<sup>15</sup>
</p>
<p>The images from both eyes need to be processed, and then the two images need to be fused into one through a technique called stereopsis. As a result of having two eyes, we can detect depth; that is, we can determine the relative distance of different objects we see.<sup>16</sup> Because our eyes are a few inches apart, the same <a class="thought" href="entries/object_entry.html">object</a> will be slightly shifted in the images they receive. The amount of shift is determined by simple trigonometric relationships. Distant objects will have little shift, whereas close objects will have larger shifts. However, before our visual <a class="thought" href="entries/system_entry.html">system</a> can apply <a class="thought" href="entries/trigonometry_entry.html">trigonometry</a> to the problem of determining depth it needs to line up the corresponding objects in the two visual fields. This is more difficult than it sounds. Experiments indicate that matching the image of each <a class="thought" href="entries/object_entry.html">object</a> in the visual field of one eye to the image of that <a class="thought" href="entries/object_entry.html">object</a> in the visual field of the other must take place after the detection of edges.<sup>17</sup> Once <a class="thought" href="entries/edge_detection_entry.html">edge detection</a> has taken place, the edges can be matched using additional <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques.<sup>18</sup>
</p>
<p>Once the edges are detected and the dual images fused with corresponding <a class="thought" href="entries/information_entry.html">information</a> regarding depth, it becomes possible for more subtle processes of discrimination to begin. Edges and depths can be organized into surfaces, the texture of the surfaces can be estimated, and finally the objects themselves identified.<sup>19</sup> In this process a great deal of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the types of objects we expect to see in our environment is used. The <a class="thought" href="entries/paradigm_entry.html">paradigm</a> of <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a> and test is clearly used here in that people typically see what they expect to see in a situation. Visual experiments have shown that people often misrecognize objects that are not expected if they appear to be similar to those that are anticipated. This indicates that the testing of the hypotheses has given a positive result. If an unusual <a class="thought" href="entries/object_entry.html">object</a> does not match our <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a> (i.e., the test fails), then that <a class="thought" href="entries/object_entry.html">object</a> is likely to grab our focus of attention.</p>
<p>We have now described a fundamental way in which <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> in general, and vision in particular, differs from the logical processes of <a class="thought" href="entries/thought_entry.html">thought</a>. The essence of logic is sequential, whereas vision is parallel. I am not suggesting that the human brain does not incorporate any <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> in its logical analyses, but logical <a class="thought" href="entries/thinking_entry.html">thinking</a> generally involves considering only one transformation and its implications at a time. When speaking of parallelism in human vision (and in any attempt to truly emulate vision in a <a class="thought" href="entries/machine_entry.html">machine</a>), we are speaking not of a few computations at the same time but rather of billions simultaneously. The steps after <a class="thought" href="entries/edge_detection_entry.html">edge detection</a> also involve vast amounts of <a class="thought" href="entries/computation_entry.html">computation</a>, most of which are also accomplished through massive parallelism.<sup>20</sup> Only in the final stages of the process do we begin to <a class="thought" href="entries/reason_entry.html">reason</a> about what we have seen and thereby to introduce more sequential logical transformations. Though vision involves vastly greater amounts of <a class="thought" href="entries/computation_entry.html">computation</a> than logical processes, it is accomplished much more quickly because the <a class="thought" href="entries/number_entry.html">number</a> of processing stages are relatively fewer. The trillions of computations required for the human visual <a class="thought" href="entries/system_entry.html">system</a> to view and recognize a scene can take place in a split second.</p>
<p>This explains the relatively automatic (not consciously controlled) <a class="thought" href="entries/nature_entry.html">nature</a> of vision: these tremendously parallel circuits are constantly processing <a class="thought" href="entries/information_entry.html">information</a> and piping their results to the next stage. It is not a process we can turn off unless we close our eyes. Even then we have trouble preventing our <a class="thought" href="entries/imagination_entry.html">imagination</a> from presenting images for analysis.</p>
<p>Logical <a class="thought" href="entries/thought_entry.html">thought</a> appears to be a more recent evolutionary development than <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a>, one that requires more conscious control over each sequential step.<sup>21</sup> The amount of <a class="thought" href="entries/computation_entry.html">computation</a> required is not as vast, and less massive parallelism appears to be involved. This is one <a class="thought" href="entries/reason_entry.html">reason</a> that we have been more successful in emulating these more "advanced" logical processes in our "intelligent" machines. Despite the relatively slow speed of neuronal circuits, the massive parallelism of the human brain makes it capable of vastly more <a class="thought" href="entries/computation_entry.html">computation</a> than today's computers. Thus, the relative lack of computational capability of computers to date (less <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a>) have rendered them inadequate for a level of visual processing comparable to human vision. On the less computationally intensive (and more sequential) tasks of solving problems and playing games, even the very early computers were sufficient to perform at credible levels. Conversely, the brain's <a class="thought" href="entries/capacity_entry.html">capacity</a> for massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> is at least one of the keys to the apparent superiority of human versus <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/thought_entry.html">thought</a> in areas such as vision.<sup>22</sup>
</p><h2><a class="thought" href="entries/parallel_processing_entry.html">Parallel processing</a></h2><p>The realization of this superiority has focused attention on breaking the von Neumann bottleneck of conventional, single-processor computers. W. <a class="thought" href="entries/hillis_entry.html">Daniel Hillis</a>'s <a class="thought" href="entries/connection_machine_entry.html">Connection Machine</a>, for example, is capable of 65,536 computations at the same time, and machines with a millionfold parallelism are on the way.<sup>23</sup> Billions of simultaneous processes, particularly if <a class="thought" href="entries/analog_entry.html">analog</a> methods are combined with <a class="thought" href="entries/digital_entry.html">digital</a>, are not out of the question.<sup>24</sup>
</p>
<p>The realization that certain critical mental processes are inherently massively parallel rather than sequential has also refocused attention on the neural net as an approach to building intelligent machines.<sup>25</sup> The 1960s <a class="thought" href="entries/concept_entry.html">concept</a> of a neural net <a class="thought" href="entries/machine_entry.html">machine</a> incorporated very simple <a class="thought" href="entries/neuron_entry.html">neuron</a> models and a relatively small <a class="thought" href="entries/number_entry.html">number</a> of neurons (hundreds or thousands) organized in one or two layers. They were provided with no specific task-oriented algorithms and were expected to organize themselves by rearranging the interneuronal connections on the basis of feedback from the human trainer. These systems were capable of recognizing simple shapes, but Minsky and Papert showed, in their classic <i>Perceptrons</i>, that the machines were essentially just matching <a class="thought" href="entries/individual_entry.html">individual</a> pixel values against stored templates. These early neural nets were simply not capable of more sophisticated discriminations.<sup>26</sup> As noted earlier, the 1980s school of neural nets uses potentially more capable <a class="thought" href="entries/neuron_entry.html">neuron</a> models that can incorporate their own algorithms.<sup>27</sup> Designers are targeting systems with millions of such artificial neurons organized into many layers. Though the self-organizing <a class="thought" href="entries/paradigm_entry.html">paradigm</a> is still popular, its role can be limited. Predetermined algorithms can be built into both the <a class="thought" href="entries/neuron_entry.html">neuron</a> models themselves and the organization of each layer. For example, a layer designed to detect edges should be organized differently from a layer designed to integrate edges into surfaces. Of course, this is still a far cry from the human visual <a class="thought" href="entries/system_entry.html">system</a>, with its billions of neurons organized into hundreds of layers. We still have very limited understanding of the algorithms incorporated in most of the layers or even what their functions are. Greater insight into these issues will be required before neural nets can solve real problems. Minsky and Papert remain critical of the excessive reliance of the new connectionists on the self-organizing <a class="thought" href="entries/paradigm_entry.html">paradigm</a> of neural nets. In the prologue to a new edition of <i>Perceptrons</i> (1988) they state, "Our position remains what it was when we wrote the book: We believe this realm of work to be immensely important and rich, but we expect its <a class="thought" href="entries/growth_entry.html">growth</a> to require a degree of critical analysis that its more romantic advocates have always been reluctant to pursue-perhaps because the <a class="thought" href="entries/spirit_entry.html">spirit</a> of <a class="thought" href="entries/connectionism_entry.html">connectionism</a> seems itself to go somewhat against the grain of analytic rigor."<sup>28</sup>
</p>
<p>Another difference between logical and imaginal <a class="thought" href="entries/thinking_entry.html">thinking</a> is the issue of gradual versus catastrophic degradation.<sup>29</sup> In <a class="thought" href="entries/animal_entry.html">animal</a> vision the failure of any <a class="thought" href="entries/neuron_entry.html">neuron</a> to perform its task correctly is irrelevant. Even substantial portions of the visual cortex could be defective with relatively little impact on the quality of the end result. Leaving aside physical damage to the eyes themselves, the ability of the human brain to process visual images typically degrades the same way that a holographic (three-dimensional, laser-generated) picture degrades. Failure of <a class="thought" href="entries/individual_entry.html">individual</a> elements subtract only marginally from the overall result. Logical processes are quite different. Failure of any step in a chain of logical deductions and inferences dooms the rest of the <a class="thought" href="entries/thought_entry.html">thought</a> process. Most mistakes are catastrophic (in that they lead to an invalid result). We have some ability to detect problems in later stages, realize that earlier assumptions must have been faulty, and then attempt to correct them, but our ability to do this is limited.</p>
<p>The difference between parallel <a class="thought" href="entries/thinking_entry.html">thinking</a> and sequential <a class="thought" href="entries/thinking_entry.html">thinking</a> is significant in skill acquisition. When we first learn to perform a <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition task (<a class="thought" href="entries/learning_entry.html">learning</a> a new type of alphabet, for example, or, on a higher level, a new <a class="thought" href="entries/language_entry.html">language</a>), we use our rational facilities to <a class="thought" href="entries/reason_entry.html">reason</a> through the decision-making tasks required. This tends to be slow, deliberate, and conscious.<sup>30</sup> As we "master" the new task, <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones01.1.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">The promise of <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a>.  Hidehiko Tanaka of the University of Tokyo designs super fast computers that incorporate thousands of tiny processors working in unison on a single problem.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones01.2.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">Yuichiro Anzai, one of the leading <a class="thought" href="entries/ai_entry.html">AI</a> authorities in Japan.</span>
<br> our parallel facilities take over and we no longer need to consciously think through each step. It seems just to happen automatically. We have programmed our parallel <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition systems to take over the job. The process of recognition becomes substantially faster, and we are no longer conscious of the steps in the process. Visual-<a class="thought" href="entries/perception_entry.html">perception</a> experiments have indicated that when we read, we do not perform recognition on <a class="thought" href="entries/individual_entry.html">individual</a> characters and then group the characters into words but rather recognize entire words and even groups of words in parallel. If we had to <a class="thought" href="entries/reason_entry.html">reason</a> through each discrimination (e.g., "Now there's a semicircle with a straight line to the left of it, so that must be a <i>p</i>"), our reading speeds would be extremely slow. Indeed, a child's reading speed is very slow until the child has succeeded in programming his parallel <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition facilities to recognize first <a class="thought" href="entries/individual_entry.html">individual</a> letters, then words, finally, after years, groups of words.</p>
<p>There is a similar <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> on the output side of human <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. When we learn to perform a certain task that involves the coordination of our muscles (<a class="thought" href="entries/learning_entry.html">learning</a> a sport or even speaking a new <a class="thought" href="entries/language_entry.html">language</a>), we start out very deliberately and conscious of each step in the process. After we "master" the new skill, we are conscious only of the higher-level tasks, not of the <a class="thought" href="entries/individual_entry.html">individual</a> steps. We have gone from sequential to parallel <a class="thought" href="entries/thinking_entry.html">thinking</a>.</p>
<p>One of the objections that philosophers such as Hubert Dreyfus have made of <a class="thought" href="entries/ai_entry.html">AI</a> is that computers appear to lack the ability for this type of parallel <a class="thought" href="entries/thought_entry.html">thought</a> (the objection is generally expressed in the much vaguer terms that computers lack <a class="thought" href="entries/intuition_entry.html">intuition</a>).<sup>31</sup> It is true that the purely logical processes of most expert systems do not have the <a class="thought" href="entries/capacity_entry.html">capacity</a> for achieving this vital category of massively parallel <a class="thought" href="entries/thought_entry.html">thought</a>. It is not valid, however, to conclude that machines are inherently incapable of using this approach.</p>
<p>One might point out that even massively parallel machines ultimately use logic in their transformations. <a class="thought" href="entries/logic_entry.html">Logic</a> alone, however, is not the appropriate level of analysis to understand such systems. It is similar to trying to understand meteorology using the laws of <a class="thought" href="entries/physics_entry.html">physics</a>.<sup>32</sup> Obviously, cloud particles do follow the laws of <a class="thought" href="entries/physics_entry.html">physics</a>, but it is hopeless to attempt to predict the weather by means of the <a class="thought" href="entries/physics_entry.html">physics</a> of <a class="thought" href="entries/particle_entry.html">particle</a> interactions alone (not that we are very successful at weather forecasting even with "appropriate" methods). As an example of the weakness of rule-based methodologies in mastering certain intelligent tasks, consider the problem of describing how to recognize faces using logic alone. Face recognition is a process we are very good at despite our having little awareness of how the process actually works. No one has been able to <a class="thought" href="entries/program_entry.html">program</a> a <a class="thought" href="entries/computer_entry.html">computer</a> to perform this task, in part because no one can begin to describe how we perform this feat. In general, we find it far easier to reconstruct our mental processes for sequential <a class="thought" href="entries/thinking_entry.html">thinking</a> than for parallel <a class="thought" href="entries/thinking_entry.html">thinking</a> because we are consciously aware of each step in the process.</p><h2>Building a brain</h2><p>We can draw conclusions from the above discussion regarding some of the capabilities required to simulate the human brain (i.e., to emulate its functionality). Clearly, we need a <a class="thought" href="entries/capacity_entry.html">capacity</a> for hundreds of levels of massively parallel computations (with the parallelism of each stage potentially in the billions). These levels cannot be fully self-organizing, although the algorithms will in some cases allow for "growing" new interneuronal connections. Each level will embody an <a class="thought" href="entries/algorithm_entry.html">algorithm</a>, although the algorithms must permit <a class="thought" href="entries/learning_entry.html">learning</a>. The algorithms are implemented in two ways: the transformations performed by the neurons themselves and the <a class="thought" href="entries/architecture_entry.html">architecture</a> of how the neurons are connected. The multiple layers of parallel neuronal analysis permit <a class="thought" href="entries/information_entry.html">information</a> to be encoded on multiple levels of <a class="thought" href="entries/abstraction_entry.html">abstraction</a>. For example, in vision, images are first analyzed in terms of edges; edges form surfaces; surfaces form objects; objects form scenes.<sup>33</sup>
</p>
<p>Another example is human written <a class="thought" href="entries/language_entry.html">language</a>. Lines and curves form letters, which form words, which form phrases, which form sentences, and so on. In spoken <a class="thought" href="entries/language_entry.html">language</a>, we have sounds forming phonemes, which form words, and so on. <a class="thought" href="entries/knowledge_entry.html">Knowledge</a> regarding the constraints of each level of <a class="thought" href="entries/abstraction_entry.html">abstraction</a> is used in the appropriate layer. The <a class="thought" href="entries/knowledge_entry.html">knowledge</a> itself is not built in (though algorithms for manipulating it may be) and methods need to be provided to acquire, represent, <a class="thought" href="entries/access_entry.html">access</a>, and utilize the <a class="thought" href="entries/domain_entry.html">domain</a>-specific <a class="thought" href="entries/knowledge_entry.html">knowledge</a>.</p>
<p>Each level of analysis reduces <a class="thought" href="entries/information_entry.html">information</a>. In vision, for example, we start with the signals received from the hundred million rod and cone cells in each eye. This is equivalent to tens of billions of bits of <a class="thought" href="entries/information_entry.html">information</a> per second. Intermediate representations in terms of surfaces and surface qualities can be represented with far less <a class="thought" href="entries/information_entry.html">information</a>. The <a class="thought" href="entries/knowledge_entry.html">knowledge</a> we finally extract from this analysis is a <a class="thought" href="entries/reduction_entry.html">reduction</a> of the original massive stream of data by many orders of magnitude. Here too we see the selective (i.e., intelligent) destruction of <a class="thought" href="entries/information_entry.html">information</a> discussed earlier as the purpose of <a class="thought" href="entries/computation_entry.html">computation</a>.<sup>34</sup>
</p>
<p>The human brain has a certain degree of plasticity in that different areas of the brain can often be used to represent the same type of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. This property enables stroke victims to relearn lost skills by training other portions of the brain that were not damaged. The process of <a class="thought" href="entries/learning_entry.html">learning</a> (or relearning) requires our sequential conscious processes to repetitively expose the appropriate parallel unconscious mechanisms to the <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and constraints of a <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition or physical-skill task. There are substantial limits to this plasticity, however. The visual cortex, for example, is specifically designed for vision and cannot be used for most other tasks (although it is involved in visual <a class="thought" href="entries/imagination_entry.html">imagination</a>, which does impact many other areas of <a class="thought" href="entries/thought_entry.html">thought</a>).</p>
<p>We can also draw a conclusion regarding the type of physical construction required to achieve human-level performance. The human brain achieves massive parallelism in all stages of its processing measured in the tens or hundreds of billions of simultaneous computations in a package substantially under one cubic foot, about the size of a typical <a class="thought" href="entries/personal_computer_entry.html">personal computer</a>. It is capable of this immense level of performance because it is organized in three dimensions, whereas our <a class="thought" href="entries/electronic_entry.html">electronic</a> circuits are currently organized in only two. Our integrated-<a class="thought" href="entries/circuit_entry.html">circuit</a> chips, for example, are essentially flat. With the <a class="thought" href="entries/number_entry.html">number</a> of components on each side of a chip measured in the thousands, we are limited to a few million components per chip. If, on the other hand, we could build three-dimensional chips (that is, with a thousand or so layers of circuitry on each chip instead of just one), we would add three orders of magnitude to their <a class="thought" href="entries/complexity_entry.html">complexity</a>: we would have chips with billions rather than mere millions of components. This appears to be necessary to achieve <a class="thought" href="entries/hardware_entry.html">hardware</a> capable of human performance. <a class="thought" href="entries/evolution_entry.html">Evolution</a> certainly found it necessary to use the third dimension when designing <a class="thought" href="entries/animal_entry.html">animal</a> brains.<sup>35</sup> Interestingly, one way that the design of the human brain uses the third dimension is by elaborately folding the surface of the <a class="thought" href="entries/cerebral_cortex_entry.html">cerebral cortex</a> to achieve a very large surface area.</p>
<p>A primary <a class="thought" href="entries/reason_entry.html">reason</a> that the third dimension is not utilized is thermal problems. Transistors generate heat, and multiple layers would cause chip circuitry to melt. However, a solution may be on the horizon in the form of <a class="thought" href="entries/superconductivity_entry.html">superconductivity</a>: because of their lack of electrical resistance, superconducting circuits generate virtually no heat. This may enable <a class="thought" href="entries/circuit_entry.html">circuit</a> designers to further reduce the size of each <a class="thought" href="entries/transistor_entry.html">transistor</a> as well as to exploit the unexplored third dimension for a potential millionfold improvement in performance.<sup>36</sup>
</p>
<p>David Marr and <a class="thought" href="entries/poggio_entry.html">Tomaso Poggio</a> pointed out another <a class="thought" href="entries/salient_entry.html">salient</a> difference between human brains and today's computers in their first paper on stereo vision in 1976.<sup>37</sup> While the ratio of connections to components in a conventional <a class="thought" href="entries/computer_entry.html">computer</a> is about 3, this ratio for the mammalian cortex can be as high as 10,000. In a <a class="thought" href="entries/computer_entry.html">computer</a> virtually every <a class="thought" href="entries/component_entry.html">component</a> and connection is vital. Although there are special fail-safe computers that provide a small measure of redundancy, most computers depend on a very high degree of reliability in all of their components. The design of mammalian brains appears to use a radically different methodology in which none of the components or connections are crucial; massive redundancy allows major portions of the process to fail with little or no effect on the final results.<sup>38</sup>
</p>
<p>In summary, there are two fundamentally different forms of <a class="thought" href="entries/thinking_entry.html">thinking</a>: logical <a class="thought" href="entries/thinking_entry.html">thinking</a> and parallel <a class="thought" href="entries/thinking_entry.html">thinking</a>. Logical <a class="thought" href="entries/thinking_entry.html">thinking</a> is sequential and conscious. It involves deliberate control over each step. It tends to be slow and errors in early stages propagate throughout the rest of the process often with catastrophic results. The amount of <a class="thought" href="entries/computation_entry.html">computation</a> required tends to be limited. Thus, computers lacking in parallel-processing capabilities (nearly all computers to date) have been relatively successful in emulating some forms of logical <a class="thought" href="entries/thought_entry.html">thought</a>. Most <a class="thought" href="entries/ai_entry.html">AI</a> through the mid 1980s has been concerned with emulating this type of problem solving, with parallel <a class="thought" href="entries/thought_entry.html">thought</a> processes often being overlooked.<sup>39</sup> This has led to criticism of <a class="thought" href="entries/ai_entry.html">AI</a>, often with the unjustified conclusion that computers are inherently incapable of parallel <a class="thought" href="entries/thought_entry.html">thought</a>. Parallel <a class="thought" href="entries/thinking_entry.html">thinking</a> is massively parallel. It is capable of simultaneously processing multiple levels of <a class="thought" href="entries/abstraction_entry.html">abstraction</a>, with each level incorporating substantial <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and constraints. It tends to be relatively fast because of its highly parallel construction. It generally takes place without either conscious direction or even awareness of the <a class="thought" href="entries/nature_entry.html">nature</a> of the transformations being made. Skill acquisition generally involves the sequential mind repeatedly training the parallel mind.</p><h2>The principles of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a></h2><p>Building on the observations above, we can describe several principles that govern successful <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> systems. While specific implementations and techniques will differ from one problem <a class="thought" href="entries/domain_entry.html">domain</a> to another, the principles remain the same.</p>
<p>It is clear that <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> is important, particularly in the early stages of the process, since the quantity of <a class="thought" href="entries/information_entry.html">information</a> is greatly reduced by each stage of processing. <a class="thought" href="entries/pattern_entry.html">Pattern</a>-recognition tasks generally require a <a class="thought" href="entries/hierarchy_entry.html">hierarchy</a> of decisions. Each stage has its own manner of representing <a class="thought" href="entries/information_entry.html">information</a> and its own methods for deriving the <a class="thought" href="entries/information_entry.html">information</a> from the previous stage. For example, in vision we represent the original image data in terms of pixel intensity values. Hypothesized line segments, on the other hand, are probably represented in terms of the coordinates of the ends of each segment along with additional <a class="thought" href="entries/information_entry.html">information</a> about the characteristics of each segment (curvature, edge noise, shading, etc.). Similarly, surfaces are represented by a large <a class="thought" href="entries/number_entry.html">number</a> of coordinates plus <a class="thought" href="entries/information_entry.html">information</a> regarding the surface characteristics. A variety of methods have been devised for representing objects, including the <a class="thought" href="entries/primal_sketch_entry.html">primal sketch</a>, the 2&#189;D sketch, and the world model.<sup>40</sup>
</p>
<p>A key issue in analyzing each stage of representation is <i>segmentation</i>. In speech recognition, for example, we need to divide a sample of continuous speech into smaller segments such as words or perhaps phonemes (basic sounds).<sup>41</sup> Choosing the appropriate types of segments for each stage is one of the most important decisions in designing a <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition <a class="thought" href="entries/system_entry.html">system</a>. Once segments in the data have been located, they can be labeled (described). In vision, for example, once we have segmented a scene into line segments, we can describe the <a class="thought" href="entries/nature_entry.html">nature</a> of the segments. We then segment the edge representation into surfaces and go on to label the surfaces with their characteristics.</p>
<p>After we have determined the stages of processing, the representation of <a class="thought" href="entries/information_entry.html">information</a> contained in each stage, the segments to be extracted, and the type of labeling desired for each segment, we are still faced with the heart of the problem: designing methods to make the <a class="thought" href="entries/segmentation_entry.html">segmentation</a> and labeling decisions. The most successful <a class="thought" href="entries/paradigm_entry.html">paradigm</a> I have found for accomplishing this is that of <a class="thought" href="entries/multiple_experts_entry.html">multiple experts</a>.<sup>42</sup> Usually the only methods available to perform specific recognition tasks are very imperfect ones. <a class="thought" href="entries/information_theory_entry.html">Information theory</a> tells us that with several <i>independent</i> methods of relatively low accuracy we can still achieve high levels of accuracy if we combine them in a certain way. These multiple methods, called experts, are considered independent if they have what are called <a class="thought" href="entries/orthogonal_invariances_entry.html">orthogonal invariances</a>, that is, independent strengths. Another way of saying the same thing is that the different experts (sometimes also called <a class="thought" href="entries/knowledge_entry.html">knowledge</a> sources) tend to make different types of mistakes. The goal is to assemble a group of experts diverse enough that for each <a class="thought" href="entries/pattern_entry.html">pattern</a> that arises, at least one of the experts will have the proficiency to respond correctly. (Of course, we still have to decide which expert is right, just as in ordinary life! I shall come back to this question.)</p><h2>Character recognition</h2><p>As an example, consider the recognition of printed letters.<sup>43</sup> One useful expert we can call on would detect a feature called the loop, which is an area of white completely surrounded by black. The capital <i>A</i>, for example, has one loop; <i>B</i> has two. Another useful expert would detect concavities, which are concave regions facing in a particular direction. For example, <i>A</i> has one concavity facing south, <i>F</i> has one concave region facing east, and <i>E</i> has two concavities facing east.</p>
<p>Our loop expert would be proficient at distinguishing an <i>O</i> from a <i>C</i> in that <i>O</i> has a loop and <i>C</i> does not. It would not be capable, however, of discriminating <i>C</i> from <i>I</i> (no loops in either case) or <i>O</i> from <i>6</i> (each has one loop). The concavity expert could help us here, since it can distinguish <i>C</i> from <i>I</i> and <i>6</i> from <i>O</i> by the presence of an east concavity in <i>C</i> and <i>6</i>. Similarly, the concavity expert by itself would be unable to distinguish <i>C</i> from <i>6</i> (since they both have an east concavity), but the loop expert could identify <i>6</i> by its single loop. Clearly, the two experts together give us far greater recognition capability than either one alone. In fact, using just these two experts (a loop detector and a concavity detector), we can sort all 62 sans-serif roman characters, excluding punctuation (<i>A</i> through <i>Z</i>, <i>a</i> through <i>z</i>, and <i>0</i> through <i>9</i>) into about two dozen distinct groups with only a few characters in each group. For example, the group characterized by no loops with north and south concavities contains only the characters <i>H</i> and <i>N</i>. In other words, if the loop expert examined a <a class="thought" href="entries/pattern_entry.html">pattern</a> and indicated it had found no loops and the concavity expert indicated concave regions facing south and north, we could conclude that the character was (probably) either an <i>H</i> or an <i>N</i>. Additional experts that examined the location and orientation of line segments or angle vertices could then help us to make a final identification.</p>
<p>It is clear that in addition to a set of experts that can provide us with the ability to make all of the discriminations necessary, we also need a process to direct and organize the efforts of these experts. Such a <a class="thought" href="entries/system_entry.html">system</a>, often called the expert manager, is programmed with the <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of which expert to use in each situation.<sup>44</sup> It knows the relative strengths and weaknesses of each expert and how to <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter70.1.jpg" vspace="10"><br>
<br>
<span class="Caption">The loop feature.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter70.2.jpg" vspace="10"><br>
<br>
<span class="Caption">The concavity feature.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter70.3.jpg" vspace="10"><br>
<br>
<span class="Caption">Typical defects in real print.</span>
<br> combine their insights into making final decisions. It would know, for example, that the loop expert is relatively useless in discriminating <i>6</i> from <i>O</i> but very helpful for determining whether a character is a <i>6</i> or a <i>C</i>, and so on.</p>
<p>In a real <a class="thought" href="entries/system_entry.html">system</a> (one that deals with images from the real world), classifications are rarely as straightforward as the examples above suggest. For example, it is entirely possible that an <i>A</i> as actually printed might not contain a loop because a printing error caused the loop to be broken. An <i>a</i> (which should contain one loop) might actually contain two loops if an ink smear caused the upper portion to close. Real-world patterns rarely display the expected patterns perfectly. Even a well-printed document contains a surprisingly large <a class="thought" href="entries/number_entry.html">number</a> of defects. One way to deal with the vagaries of real-world patterns is to have redundant experts and multiple ways of describing the same type of <a class="thought" href="entries/pattern_entry.html">pattern</a>. There are a <a class="thought" href="entries/number_entry.html">number</a> of different ways of describing what an <i>A</i> should look like. Thus, if one of our experts failed (e.g., the loop expert), we still have a good chance of correctly recognizing the <a class="thought" href="entries/pattern_entry.html">pattern</a>.</p>
<p>There are many sources of variability. One, called noise for obvious reasons, consists of random changes to a <a class="thought" href="entries/pattern_entry.html">pattern</a>, particularly near the edges, caused by defects in the <a class="thought" href="entries/pattern_entry.html">pattern</a> itself as well as imperfections in the sensing mechanism that visualizes the <a class="thought" href="entries/pattern_entry.html">pattern</a> (e.g., an <a class="thought" href="entries/image_scanner_entry.html">image scanner</a>). Another source of variability derives from the inherent <a class="thought" href="entries/nature_entry.html">nature</a> of patterns defined at a high level of <a class="thought" href="entries/abstraction_entry.html">abstraction</a>. For example, the <a class="thought" href="entries/concept_entry.html">concept</a> of an <i>A</i> allows for a great deal of variation. There are hundreds of different styles of type in common use and many more if ornamental styles are considered. If one considers only a single type style, then one could obtain accurate recognition using a relatively small <a class="thought" href="entries/number_entry.html">number</a> of experts. If, on the other hand, we attempt to recognize printed characters drawn from a wide multitude of styles, then it is clear that a substantially more diverse set <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter701.jpg" vspace="10"><br>
<br>
<span class="Caption">What letter is this?  <a class="thought" href="entries/hofstadter_entry.html">Douglas Hofstadter</a> uses these images to illustrate the superiority of human <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> over the machines of today.  Humans have little difficulty recognizing these variations (although a few of these may indeed be problematical if presented in isolation).  While machines exist today that can accurately recognize the many type styles in common usage, no <a class="thought" href="entries/machine_entry.html">machine</a> can successfully deal with the level of <a class="thought" href="entries/abstraction_entry.html">abstraction</a> required by these ornamental forms.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter701.1.jpg" vspace="10"><br>
<br>
<span class="Caption">The <a class="thought" href="entries/classification_entry.html">classification</a> of roman well-printed sans-serif characters by loop and concavity features.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter701.2.jpg" vspace="10"><br>
<br>
<span class="Caption">Disambiguating N from H using a line segment expert.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter701.3.jpg" vspace="10"><br>
<br>
<span class="Caption">The loop and concavity features of 62 roman sans-serif characters.  Some of the concavities are ambiguous or marginal.  For example, the southern concavity in the letter a is so small it may be overlooked by the concavity expert.  Thus, in a practical <a class="thought" href="entries/system_entry.html">system</a> a would be classified in both the "has one southern concavity" and "has no southern concavity" categories.  To account for multiple type fonts most characters will in fact have multiple classifications.</span>
<br> of experts is required.<sup>45</sup> Allowing such variability in the patterns to be recognized also complicates the task of the expert manager.</p>
<p>Since the <a class="thought" href="entries/classification_entry.html">classification</a> of patterns in the real world is often not clear cut, it is desirable for our experts to provide their "opinions" on a continuous scale. Rather than stating that this <a class="thought" href="entries/pattern_entry.html">pattern</a> has a loop, it would be of greater value for the expert to indicate its relative level of confidence in the presence of such a property (e.g., "There is a 95 percent probability of there being one loop in this <a class="thought" href="entries/pattern_entry.html">pattern</a>, a 3 percent probability of there being two loops"). A less-than-certain result might indicate that the loop expert almost found a loop, that the "loop" found is broken by a few pixels. Even if the loop is entirely closed, there is always the possibility that it really should not be there at all but is only an <a class="thought" href="entries/artifact_entry.html">artifact</a> of a printing or scanning error. If all of the experts provide their analyses in terms of probabilities, then the expert manager can use <a class="thought" href="entries/information_theory_entry.html">information theory</a> to combine these results in an optimal way.</p>
<p>In cases of significant print distortion, even human <a class="thought" href="entries/perception_entry.html">perception</a> can fail on the level of <a class="thought" href="entries/individual_entry.html">individual</a> letters. Yet we are often able to correct for printing defects by using our <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of <a class="thought" href="entries/language_entry.html">language</a> <a class="thought" href="entries/context_entry.html">context</a>. For example, if we have trouble distinguishing a <i>t</i> from a <i>c</i> because of poor printing, we generally look (consciously or unconsciously) at the <a class="thought" href="entries/context_entry.html">context</a> of the letter. We might determine, for example, that "<a class="thought" href="entries/computer_entry.html">computer</a>" makes more sense than "compucer." This introduces the <a class="thought" href="entries/concept_entry.html">concept</a> of experts that use <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the constraints of higher levels of <a class="thought" href="entries/context_entry.html">context</a>. Knowing that "compucer" is not a word in English but that "<a class="thought" href="entries/computer_entry.html">computer</a>" is enables us to disambiguate an otherwise ambiguous <a class="thought" href="entries/pattern_entry.html">pattern</a>. Similarly, in the field of speech recognition, the only possible way to distinguish the spoken word "to" from "too" and from "two" (all of which sound identical) is from <a class="thought" href="entries/context_entry.html">context</a>. In the sentence "I am going to the store," we can eliminate "too" and "two" from consideration by relying on our higher-level syntactic <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. Perceptual experiments indicate that human <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> relies heavily on such contextual discrimination. Attempting to recognize printed letters without a word <a class="thought" href="entries/context_entry.html">context</a>, human speech without a sentence <a class="thought" href="entries/context_entry.html">context</a>, and musical timbres without a melodic <a class="thought" href="entries/context_entry.html">context</a> sharply reduces the accuracy of human <a class="thought" href="entries/perception_entry.html">perception</a>. Similarly, machines dealing with highly variable types of patterns require extensive use of <a class="thought" href="entries/context_entry.html">context</a> experts with substantial <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about their domains. A word-<a class="thought" href="entries/context_entry.html">context</a> expert in a character-recognition <a class="thought" href="entries/system_entry.html">system</a> requires <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of all the possible words in the <a class="thought" href="entries/language_entry.html">language</a>. A syntactic expert in a speech-recognition <a class="thought" href="entries/system_entry.html">system</a> requires <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of possible word sequences. Again, an expert that can say, "'Phthisis' has a probability of .0001," is more valuable than one who can only say, "'Phthisis' is possible."</p>
<p>All of the experts mentioned above deal with relatively abstract concepts. Concavity is not a perfectly defined <a class="thought" href="entries/concept_entry.html">concept</a>. Detecting this property is not straightforward and requires a relatively complex <a class="thought" href="entries/program_entry.html">program</a>. A very different category of experts, low-level experts (as distinguished from the high-level experts described above), deal with features that are simple transformations of the original input data. For example, in any type of visual recognition we could have a low-level property associated with every pixel whose value is simply the value of the pixel. This is, of course, the simplest possible property. A slightly higher level property (but still low levee could detect the amount of black in a particular region of the image. For <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter701.4.jpg" vspace="10"><br>
<br>
<span class="Caption">Varieties of low-level (minimal) property sets.</span>
<br> example, a <i>T</i> will tend to have more black in the upper region of the image than an <i>L</i>, which will tend to be more black in the lower region. In actual use, minimal properties tend to be more complex than in the above two examples but nonetheless use straightforward and well-defined transformations of the original input.</p>
<p>It turns out that such low-level properties are quite useful in recognizing patterns when the possible types of patterns are highly constrained. For example, in character recognition, if we restrict the problem to a single style of type, then a <a class="thought" href="entries/system_entry.html">system</a> built entirely with low-level property experts is capable of a very high level of accuracy (potentially less than one error in over ten thousand printed characters). This limited problem is often attacked with <a class="thought" href="entries/template_matching_entry.html">template matching</a>, so called because it involves matching the image under consideration to stored templates of every letter in the character set.<sup>46</sup> <a class="thought" href="entries/template_matching_entry.html">Template matching</a> (and other methods of minimal-property extraction) also work well for recognizing printed letters drawn from a small <a class="thought" href="entries/number_entry.html">number</a> of type styles. If we are trying to recognize any nonornamental type style (called omnifont, or intelligent, character recognition), then an approach using only minimal property extraction does not work at all. In this case, we must use the higher-level (more intelligent) experts that are based on such abstract topological concepts as loops, concavities, and line segments. The minimal properties can still play an important role, however. Fortunately, printed material does not generally combine multiple type styles in anything like a random fashion. Any particular document (e.g., a book or magazine) will tend to use a limited <a class="thought" href="entries/number_entry.html">number</a> of type styles in a consistent way.<sup>47</sup> When an omnifont character-recognition <a class="thought" href="entries/machine_entry.html">machine</a> first encounters a new document, it has no choice but to use its intelligent experts (its loop expert, concavity expert, etc.) to recognize the characters. As it begins successfully to recognize characters, its higher-level experts can actually train its lower-level experts to do the job, and its expert manager (which directs the overall recognition process) can begin to rely more heavily on the lower-level experts for recognition. The higher-level experts train the lower-level ones by presenting actual examples of recognized characters and telling them, in essence, "Here are examples of characters as they actually appear in this document, and this is what we believe their correct identifications to be." The advantages of such an automatic <a class="thought" href="entries/learning_entry.html">learning</a> process include both speed and accuracy. The lower-level experts are not only potentially much faster, they can also be less sensitive to image noise.</p>
<p>To return to the first theme of this chapter, the higher-level experts in such a character-recognition <a class="thought" href="entries/system_entry.html">system</a> are representative of logical analysis, whereas the lower-level experts represent a more parallel type of <a class="thought" href="entries/thinking_entry.html">thinking</a>. The lower-level experts use much simpler algorithms, so they are more amenable to massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a>, which is a major <a class="thought" href="entries/reason_entry.html">reason</a> for their potential speed advantage.</p>
<p>Interestingly, perceptual experiments indicate that the human visual <a class="thought" href="entries/system_entry.html">system</a> works in a similar way. When we first encounter a new typeface, to recognize it, we rely on our conceptual understanding of print (a logical type of analysis), and our recognition speeds are relatively slow. Once we get used to the style, our recognition process becomes less analytic, and our speed and accuracy increase substantially. This is another example of our logical mind training our parallel mind.</p>
<p>The <a class="thought" href="entries/paradigm_entry.html">paradigm</a> of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> described above is common to most serious recognition problems: multiple stages of processing based on a <a class="thought" href="entries/hierarchy_entry.html">hierarchy</a> of levels, massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> (particularly in the early stages), <a class="thought" href="entries/segmentation_entry.html">segmentation</a> and labeling, <a class="thought" href="entries/multiple_experts_entry.html">multiple experts</a> on both high and low levels, expert management, disambiguation using the constraints of higher levels of <a class="thought" href="entries/context_entry.html">context</a>, and <a class="thought" href="entries/learning_entry.html">learning</a> from actual recognition examples.<sup>48</sup> The actual <a class="thought" href="entries/content_entry.html">content</a> of the <a class="thought" href="entries/paradigm_entry.html">paradigm</a>, however, will differ substantially from one problem area to another. Most of the <a class="thought" href="entries/technology_entry.html">technology</a> of any successful <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition <a class="thought" href="entries/system_entry.html">system</a> is <i>domain specific</i>; that is, it is based on the detailed <a class="thought" href="entries/nature_entry.html">nature</a> of the types of patterns to be recognized. Every so often one hears claims regarding a general-purpose <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition <a class="thought" href="entries/system_entry.html">system</a> that can recognize any type of <a class="thought" href="entries/pattern_entry.html">pattern</a>-printed characters, spoken words, land-terrain maps-regardless of their source. As mentioned earlier, while such systems do recognize many types of patterns, they perform these tasks poorly. To perform any specific <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition task well with commercially acceptable rates of accuracy requires substantial <a class="thought" href="entries/knowledge_entry.html">knowledge</a> deeply embedded in the algorithms and specific to the <a class="thought" href="entries/domain_entry.html">domain</a> of inquiry.</p><h1>The Real World</h1><h2>Looking at the real world</h2><p>Attempts to emulate the general capabilities of human vision are being pursued at a <a class="thought" href="entries/number_entry.html">number</a> of leading <a class="thought" href="entries/ai_entry.html">AI</a> laboratories. One is an ambitious project to create an artificial eye-head <a class="thought" href="entries/system_entry.html">system</a> at the MIT Vision Laboratory under the direction of <a class="thought" href="entries/poggio_entry.html">Tomaso Poggio</a>.<sup>49</sup> The MIT work includes <a class="thought" href="entries/edge_detection_entry.html">edge detection</a> (using the Laplacian Gaussian convolver described above and other similar algorithms), fusing stereo images to provide <a class="thought" href="entries/information_entry.html">information</a> on depth, understanding color <a class="thought" href="entries/perception_entry.html">perception</a>, reconstructing surfaces and their properties, tracking trajectories of moving objects, and the ultimate problem of describing the <a class="thought" href="entries/content_entry.html">content</a> of what is seen. One of the most interesting aspects of the MIT work is the development of a new type of <a class="thought" href="entries/computer_entry.html">computer</a> that combines <a class="thought" href="entries/digital_entry.html">digital</a> control with massive <a class="thought" href="entries/analog_entry.html">analog</a> parallelism.<sup>50</sup> Experiments conducted by Poggio, his associate Christof Koch, and others have already suggested that the human nervous <a class="thought" href="entries/system_entry.html">system</a> appears to be capable of substantial parallelism (hundreds) of <i>analog</i> computations within a <i>single </i><i>neuron</i>.</p>
<p>This work is exemplary of the <a class="thought" href="entries/paradigm_entry.html">paradigm</a> of <a class="thought" href="entries/multiple_experts_entry.html">multiple experts</a>. A <a class="thought" href="entries/number_entry.html">number</a> of different systems, each with extensive <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of a specific aspect of the visual-<a class="thought" href="entries/perception_entry.html">perception</a> task, are combined in the MIT eye-head <a class="thought" href="entries/system_entry.html">system</a>.<sup>51</sup> For example, an expert being developed by Anya Hurlbert and <a class="thought" href="entries/poggio_entry.html">Tomaso Poggio</a> uses <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the spectral (color) reflectance of surfaces to help describe them.<sup>52</sup> The project also addresses the issues of integrating visual <a class="thought" href="entries/perception_entry.html">perception</a> with the mechanical control of a robot and includes a head with two solid-state cameras for eyes (that is, cameras with a special chip called a charge-coupled <a class="thought" href="entries/device_entry.html">device</a> as an <a class="thought" href="entries/electronic_entry.html">electronic</a> <a class="thought" href="entries/retina_entry.html">retina</a>).</p>
<p>A major center for the development of vision systems and their application to the field of <a class="thought" href="entries/robotics_entry.html">robotics</a> is the <a class="thought" href="entries/robotics_entry.html">Robotics</a> Institute (RI) at Carnegie-Mellon University under the direction of <a class="thought" href="entries/ai_entry.html">AI</a> pioneer <a class="thought" href="entries/reddy_entry.html">Raj Reddy</a>. A particularly ambitious project at RI, funded by the Defense Advanced <a class="thought" href="entries/research_entry.html">Research</a> Projects Agency (<a class="thought" href="entries/darpa_entry.html">DARPA</a>) is an autonomous vehicle called Terregator (Terrestrial Navigator) which combines a high-resolution vision <a class="thought" href="entries/system_entry.html">system</a>, <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a>, and advanced decision-making capabilities.<sup>53</sup>
</p>
<p>In view of the strong Japanese commitment to the application of <a class="thought" href="entries/robotics_entry.html">robotics</a> to production techniques, Japanese researchers have targeted vision as a priority <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones02.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption"><a class="thought" href="entries/computer_entry.html">Computer</a> vision pioneer <a class="thought" href="entries/poggio_entry.html">Tomaso Poggio</a> at the MIT Vision Lab.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones02.1.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption"><a class="thought" href="entries/reddy_entry.html">Raj Reddy</a>, director of the <a class="thought" href="entries/robotics_entry.html">Robotics</a> Institute of Carnegie-Mellon University.  Reddy has been a pioneer in the development of <a class="thought" href="entries/voice_recognition_entry.html">voice recognition</a>, <a class="thought" href="entries/computer_entry.html">computer</a> vision, and <a class="thought" href="entries/robotics_entry.html">robotics</a>.  Now working on the Terregator (Terrestrial Navigator) for the Defense Advanced <a class="thought" href="entries/research_entry.html">Research</a> Projects Agency (<a class="thought" href="entries/darpa_entry.html">DARPA</a>), Reddy predicts that <a class="thought" href="entries/future_entry.html">future</a> robotic-vision systems will eventually revolutionize driving and provide cars with effective collision-control and road-following capabilities.</span>
<br>
<a class="thought" href="entries/research_entry.html">research</a> topic. Building on the work of Poggio and his associates, Yoshiaki Shirai (of the Electrotechnical Laboratory, Ibaraki, Japan) and Yoshiro Nishimoto (of the <a class="thought" href="entries/research_entry.html">Research</a> Laboratory, Kobe Steel, Kobe, Japan) are attempting to build a practical <a class="thought" href="entries/system_entry.html">system</a> for fusing stereo images. Based on parallel <a class="thought" href="entries/hardware_entry.html">hardware</a>, the Shirai-Nishimoto <a class="thought" href="entries/system_entry.html">system</a> uses a Laplacian of a Gaussian convolver (a <a class="thought" href="entries/sombrero_filter_entry.html">sombrero filter</a>) as well as more advanced <a class="thought" href="entries/pattern_entry.html">pattern</a>-matching techniques. Japanese development efforts are emphasizing the integration of vision with real-time robotic control to provide a new generation of robots that can see their environment, perceive and understand the relevant features of objects, and <a class="thought" href="entries/reason_entry.html">reason</a> about what they have seen. Hirochika Inoue and Hiroshi Mizoguchi (of the University of Tokyo) have developed a <a class="thought" href="entries/system_entry.html">system</a> that can detect, recognize, and track rapidly moving objects in real time.</p>
<p>One promising approach to organizing the massive parallelism required for <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition tasks is to develop specialized chips to perform those tasks requiring the most <a class="thought" href="entries/computation_entry.html">computation</a>. One researcher pursuing this approach is Carver A. Mead (of the California Institute of <a class="thought" href="entries/technology_entry.html">Technology</a>), one of the original pioneers in the development of design methodologies for large-scale integrated circuits. Mead and his associates have developed an artificial-<a class="thought" href="entries/retina_entry.html">retina</a> chip that performs such early-vision tasks as <a class="thought" href="entries/edge_detection_entry.html">edge detection</a> and the adjustment of an image for the effects of varying levels of illumination.<sup>54</sup> One of the innovations of Mead's approach is his reliance on massively parallel <a class="thought" href="entries/analog_entry.html">analog</a> circuits to provide the bulk of the <a class="thought" href="entries/computation_entry.html">computation</a>. Mead is also working on an artificial-cochlea chip based on similar principles.</p>
<p>While <a class="thought" href="entries/research_entry.html">research</a> is just beginning on systems that emulate the full range of human visual processing, machines that perform more limited tasks of visual <a class="thought" href="entries/perception_entry.html">perception</a> have already found significant <a class="thought" href="entries/commercial_entry.html">commercial</a> applications. For example, <a class="thought" href="entries/ocr_entry.html">optical character recognition</a> (<a class="thought" href="entries/ocr_entry.html">OCR</a>) was a $100 million industry in 1986 and is projected to grow to several hundred million dollars in 1990.<sup>55</sup> Applications include reading aloud for the blind, as well as scanning printed and typed documents for entry into word processing, <a class="thought" href="entries/electronic_entry.html">electronic</a> publishing, transaction processing, and <a class="thought" href="entries/database_entry.html">database</a> systems.<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones03.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">Seeing and believing. At the Tsukuba <a class="thought" href="entries/research_entry.html">Research</a> Center in Japan, Yoshiaki Shirai's <a class="thought" href="entries/research_entry.html">research</a> in <a class="thought" href="entries/robotics_entry.html">robotics</a> focuses on the development of three-dimensional vision systems.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones03.1.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">Hirochika Inoue, a pioneer in robotic vision systems, at the University of Tokyo.</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones04.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">Makoto Nagao of Kyoto University explores <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> by means of shadows and surfaces.</span>
<br>
</p>
<p>Systems using <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques are revolutionizing the handling of fingerprints by law enforcement agencies. A <a class="thought" href="entries/system_entry.html">system</a> called the Automated Fingerprint Identification <a class="thought" href="entries/system_entry.html">System</a> (AFIS) developed by NEC of Japan enables agencies across the United States to rapidly identify suspects from fingerprints or even small fragments of fingerprints by intelligently matching them against the stored prints of hundreds of thousands of previously arrested men and women. A report by the U.S. Bureau of Justice Statistics stated, "AFIS may well have the greatest impact of any technological development on law enforcement effectiveness since the introduction of computers to widespread use in the criminal justice <a class="thought" href="entries/system_entry.html">system</a> in the 1960s."<sup>56</sup> AFIS is capable of identifying a suspect in several minutes; the manual methods it replaces took months or even years.</p>
<p>Similar techniques are being used in security devices. Systems manufactured by Fingermatrix, Thumbscan, and other firms include a small optical scanner into which a person inserts his finger.<sup>57</sup> The <a class="thought" href="entries/device_entry.html">device</a> quickly reads the person's finger <a class="thought" href="entries/pattern_entry.html">pattern</a> and uses <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques to match it against stored images. The <a class="thought" href="entries/system_entry.html">system</a> can control entry to restricted areas and protect <a class="thought" href="entries/information_entry.html">information</a> in computers from unauthorized <a class="thought" href="entries/access_entry.html">access</a>. Such systems could eventually replace ordinary locks and keys in homes and cars.</p>
<p>One of the largest applications of <a class="thought" href="entries/commercial_entry.html">commercial</a> vision systems so far can be found in factories, where the systems are used for inspection, assembly, and process control. Such systems typically use solid-state cameras with specialized <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones04.1.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">Robert Shillman</span>
<br>
<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones04.2.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">The Cognex <a class="thought" href="entries/machine_entry.html">Machine</a> Vision <a class="thought" href="entries/system_entry.html">System</a>.</span>
<br> electronics to digitize moving images and provide for the computationally intensive early phases of processing.<sup>58</sup> A general-purpose <a class="thought" href="entries/computer_entry.html">computer</a> with custom <a class="thought" href="entries/software_entry.html">software</a> provides for the higher levels of analysis. One of the more sophisticated of such systems has been developed by Cognex Corporation, founded by Robert Shillman and a team of MIT <a class="thought" href="entries/ai_entry.html">AI</a> researchers in 1981. One Cognex product can scan manufactured products streaming by on a conveyor belt and detect and recognize such <a class="thought" href="entries/information_entry.html">information</a> as serial numbers embossed in metal or even glass. Other Cognex products can identify specific objects and their orientation for inspection and to assist robotic assemblers. Other major providers of vision systems include Automatix, Defracto, <a class="thought" href="entries/perceptron_entry.html">Perceptron</a>, Robotic Vision Systems, and View Engineering. A major player has been General Motors, which has provided investments and contracts for several of the players. According to DM <a class="thought" href="entries/data_entry.html">Data</a>, overall revenues for the factory-vision industry were over $300 million in 1987 and are projected to hit $800 million in 1990.<sup>59</sup>
</p>
<p>
<a class="thought" href="entries/military_entry.html">Military</a> systems account for another major application of artificial vision.<sup>60</sup> The ability to scan and recognize terrain at very low altitudes is a crucial <a class="thought" href="entries/element_entry.html">element</a> of the cruise missile, which can be launched thousands of miles from its intended target. Modern fighter planes have a similar ability to track terrain and provide pilots with a continually updated display of the location and trajectory of the aircraft. <a class="thought" href="entries/smart_weapons_entry.html">Smart weapons</a> (bombs, missiles, and other munitions) use a variety of sensing mechanisms including vision to locate, identify, and reach intended targets.</p>
<p>The advent of weapons that can see has resulted in profound changes in <a class="thought" href="entries/military_entry.html">military</a> tactics and strategy. As recently as the Vietnam War, it was generally necessary to launch enormous numbers of passive blind weapons in relatively indiscriminate patterns to assure the destruction of a target. Modern battlefield tactics emphasize instead the carefully targeted destruction of the enemy with weapons that can recognize their objective. Intelligent missiles allow planes, ships, and submarines to destroy targets from relatively safe distances. For example, a plane can launch an intelligent missile to destroy a ship from tens or even hundreds of miles away, well out of range of the ship's guns. A new generation of pilotless aircraft use <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition-based vision systems to navigate and launch weapons without human crews.<sup>61</sup> Vision systems and other <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition technologies are also deployed in defensive tactics to recognize an incoming missile, but such defense is generally much more difficult than offense. The result is an increasing degree of vulnerability for such slow-moving targets as tanks and ships.</p>
<p>An area of emerging importance is the application of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> to <a class="thought" href="entries/medicine_entry.html">medicine</a>. Medical diagnosis is, after all, a <a class="thought" href="entries/matter_entry.html">matter</a> of perceiving relevant patterns from symptoms, test results, and other diagnostic data. Experimental systems can look at images from a variety of imaging sources-X-ray machines, <a class="thought" href="entries/cat_entry.html">CAT</a> (computerized axial tomography) scanners, and <a class="thought" href="entries/mri_entry.html">MRI</a> (magnetic resonance imaging) systems-and provide tentative diagnoses. Few, if any, medical professionals are ready to replace their own perceptions with those of such systems, but many are willing to augment their own analysis. Often an automatic <a class="thought" href="entries/system_entry.html">system</a> will detect and report a diagnosis that manual analysis would have overlooked.<sup>62</sup>
</p>
<p>One particularly promising medical application is the analysis of blood-cell images. Certain types of <a class="thought" href="entries/cancer_entry.html">cancer</a> can be diagnosed by finding telltale precursor<br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter7jones05.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Lou Jones www.fotojones.com</span>
<br>
<span class="Caption">Shigeru Eiho, director of Engineering at Kyoto University, with his medical imaging <a class="thought" href="entries/system_entry.html">system</a>.</span>
<br> malignant cells in a blood sample. Analysis of blood samples by human technicians typically involve the examination of only about a hundred cells. By the time a malignant cell shows up in a sample that small, the <a class="thought" href="entries/cancer_entry.html">cancer</a> is often too advanced to be effectively treated. Unhampered by fatigue or tedium and able to operate at speeds hundreds of times greater than human technicians, artificial "technicians" using <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques can <a class="thought" href="entries/search_entry.html">search</a> for signs of <a class="thought" href="entries/cancer_entry.html">cancer</a> in hundreds of thousands or even millions of cells and thus potentially detect the presence of <a class="thought" href="entries/disease_entry.html">disease</a> at a treatable stage.</p>
<p>Ultimately, medical applications of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> will have enormous benefit. Medical testing comprises a major fraction of all of <a class="thought" href="entries/medicine_entry.html">medicine</a> and costs several hundred billion dollars per year. Examining the images and other data resulting from such tests is extremely tedious for human technicians, and many studies have cited the relatively low level of accuracy that results. Many doctors routinely order tests to be conducted in duplicate just to improve their accuracy. Once computers have mastered the requisite <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition tasks, the potential exists for a major transformation of medical testing and diagnosis.</p>
<p>One area of <a class="thought" href="entries/medicine_entry.html">medicine</a> that is already being revolutionized is medical imaging.<sup>63</sup> Using a variety of <a class="thought" href="entries/computer_entry.html">computer</a>-based image-enhancement techniques, physicians now have <a class="thought" href="entries/access_entry.html">access</a> to unprecedented views inside our bodies and brains. Similar techniques also allow scientists to visualize such extremely small biochemical phenomena as viruses for the first time.</p>
<p>One of the more surprising results of <a class="thought" href="entries/image_processing_entry.html">image processing</a> and recognition took place recently when Lillian Schwartz observed the striking unity of the juxtaposed halves of the "Mona Lisa" and the reversed "Self-Portrait" by <a class="thought" href="entries/da_vinci_entry.html">Leonardo da Vinci</a>. Further investigation by Schwartz led her to identify Leonardo as the model used to complete the "Mona Lisa," thereby suggesting a remarkable conclusion to the 500-year-old riddle of the <a class="thought" href="entries/identity_entry.html">identity</a> of the celebrated painting.<sup>64</sup>
</p><h2>Listening to the real world</h2><p>Another human sense that computers are attempting to emulate is hearing. While input to the auditory sense involves substantially less data than the visual sense (about a million bits per second from both ears versus about fifty billion bits per second from both eyes), the two senses are of comparable importance in our understanding of the world. As an <a class="thought" href="entries/experiment_entry.html">experiment</a>, try watching a television news <a class="thought" href="entries/program_entry.html">program</a> without sound. Then try listening to a similar <a class="thought" href="entries/broadcast_entry.html">broadcast</a> without looking at the picture. You will probably find it easier to follow the news stories with yours ears alone than with your eyes alone. Try the same <a class="thought" href="entries/experiment_entry.html">experiment</a> with a situation comedy; the result should be the same.</p>
<p>Part of the importance of our auditory sense is the close link of verbal <a class="thought" href="entries/language_entry.html">language</a> to our conscious <a class="thought" href="entries/thinking_entry.html">thinking</a> process. A theory popular until recently held that <a class="thought" href="entries/thinking_entry.html">thinking</a> was subvocalized speech.<sup>65</sup> While we now recognize that our thoughts incorporate both <a class="thought" href="entries/language_entry.html">language</a> and visual images, the crucial importance of the auditory sense in the acquisition of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> is widely accepted.</p>
<p>Blindness is often considered to be a more serious handicap than deafness. A careful consideration of the issues, however, shows this to be a misconception. With modern mobility techniques, blind persons with appropriate training have little difficulty in travelling from place to place, reading machines can provide <a class="thought" href="entries/access_entry.html">access</a> to the world of print, and the visually impaired <a class="thought" href="entries/experience_entry.html">experience</a> few barriers to communicating with other persons in groups and meetings large or small. For the deaf, however, there is a barrier to engaging in a very fundamental activity-understanding what other people are saying in person to person contact, on the phone, and in meetings. The hearing impaired are often cut off from basic human <a class="thought" href="entries/communication_entry.html">communication</a> and feel anger at <a class="thought" href="entries/society_entry.html">society</a>'s failure to accommodate or understand their situation.</p>
<p>We hear many things: music, speech, the varied noises of our environment. Of these, the sounds that are the most important in terms of interacting with and <a class="thought" href="entries/learning_entry.html">learning</a> about the world are those of human speech. Appropriately, the area of auditory recognition that has received the most attention is that of speech recognition in both human and <a class="thought" href="entries/machine_entry.html">machine</a>.</p>
<p>As with human vision, the stages of human auditory processing that we know the most about are the early ones. From the work of Stephanie Seneff, Richard Goldhor, and others at the MIT Speech Laboratory and from similar work at other laboratories, we have some <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the specific transformations applied by the auditory nerve and early stages of the auditory cortex. As with vision, our <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the details of the later stages is relatively slight due mostly to our inability to <a class="thought" href="entries/access_entry.html">access</a> these inner circuits of the brain. Since it is difficult for us to analyze the human auditory <a class="thought" href="entries/system_entry.html">system</a> directly, the bulk of the <a class="thought" href="entries/research_entry.html">research</a> in speech recognition to date has been devoted to teaching machines to understand speech. As with vision, the success of such efforts may provide us with workable theories as to how the human auditory <a class="thought" href="entries/system_entry.html">system</a> might work, and these theories may subsequently be verified by neurophysical experimentation.</p>
<p>&#160;Let us examine <a class="thought" href="entries/automated_speech_rec_entry.html">automatic speech recognition</a> (<a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>) in terms of the <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition <a class="thought" href="entries/paradigm_entry.html">paradigm</a> described above for vision. Speech is created by the human vocal tract, which, like a complex musical <a class="thought" href="entries/instrument_entry.html">instrument</a>, has a <a class="thought" href="entries/number_entry.html">number</a> of different ways of shaping sound. The vocal cords vibrate, creating a characteristic pitched sound. The length and tautness of the vocal cords determines pitch in the same way that the length and tautness of a violin or piano string determines pitch. We can control the tautness of our vocal cords (hence our ability to sing). We shape the overtones produced by our vibrating vocal cords by moving our tongue, teeth, and lips, which has the effect of changing the shape of the vocal tract. The vocal tract is a chamber that acts like a pipe in a pipe organ, the harmonic resonances of which emphasize certain overtones and diminish others. Finally, we control a small piece of tissue called the alveolar flap, which opens and closes the nasal cavity. When the alveolar flap is open, the nasal cavity provides an additional resonant chamber similar to the opening of another organ pipe.</p>
<p>In addition to the pitched sound produced by the vocal cords, we can produce a noiselike sound by the rush of air through the speech cavity. This sound does not have specific overtones but is rather a complex spectrum of many frequencies mixed together. Like the musical tones produced by the vocal cords, the spectra of these noise sounds are also shaped by the changing resonances of the moving vocal tract.<sup>66</sup>
</p>
<p>This apparatus allows us to create the varied sounds that comprise human speech. While many animals communicate with others of their <a class="thought" href="entries/species_entry.html">species</a> with sound, we humans are unique in our ability to shape sound into <a class="thought" href="entries/language_entry.html">language</a>. Vowel sounds (/a/, /e/) are produced by shaping the overtones from the vibrating vocal cords into distinct frequency bands called formants. Sibilant sounds (/s/, /z/) are created by the rush of air through particular configurations of tongue and teeth. Plosive consonants (/p/, /k/, /t/) are transitory sounds created by the percussive movement of lips, tongue, and mouth cavity. Nasal sounds (/n/, /m/) are created by invoking the resonances of the nasal cavity.<sup>67</sup>
</p>
<p>Each of the several dozen basic sounds, called phonemes, requires an intricate movement involving precise coordination of the vocal cords, alveolar flap, tongue, lips, and teeth. We typically speak about 3 words per second. So with an average of 6 phonemes per word, we make about 18 complex phonetic gestures each second. We do this without <a class="thought" href="entries/thinking_entry.html">thinking</a> about it, of course. Our thoughts remain on the conceptual (that is, the highest) level of the <a class="thought" href="entries/language_entry.html">language</a> <a class="thought" href="entries/hierarchy_entry.html">hierarchy</a>. In our first two years of life, however, we <a class="thought" href="entries/thought_entry.html">thought</a> a lot about how to make speech sounds (and how to meaningfully string them together). This is another example of our sequential (logical) conscious mind training our parallel (<a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a>) mind.</p>
<p>The mechanisms described above for creating speech sounds-vocal cord vibrations, the noise of rushing air, articulatory gestures of the mouth and tongue, the shaping of the vocal and nasal cavities-produce different rates of vibration. A physicist measures these rates of vibration as frequencies; we perceive them as pitches. Though we normally consider speech to be a single time-varying sound, it is actually a composite of many different sounds, each of which has a different frequency. With this insight, most <a class="thought" href="entries/commercial_entry.html">commercial</a> <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems start by breaking up the speech waveform into a <a class="thought" href="entries/number_entry.html">number</a> of different bands of frequencies. A typical <a class="thought" href="entries/commercial_entry.html">commercial</a> or <a class="thought" href="entries/research_entry.html">research</a> <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> will produce between three and a few dozen frequency bands. The front end of the human auditory <a class="thought" href="entries/system_entry.html">system</a> does exactly the same thing; each of the nerve endings in the cochlea (inner ear) responds to different frequencies and emits a pulsed <a class="thought" href="entries/digital_entry.html">digital</a> signal when activated by an appropriate pitch. The cochlea differentiates several thousand overlapping bands of frequency, which gives the human auditory <a class="thought" href="entries/system_entry.html">system</a> its extremely high degree of sensitivity to frequency. Experiments have shown that increasing the <a class="thought" href="entries/number_entry.html">number</a> of overlapping frequency bands in an <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> (and thus bringing it closer to the thousands of bands of the human auditory <a class="thought" href="entries/system_entry.html">system</a>) substantially increases the ability of that <a class="thought" href="entries/system_entry.html">system</a> to recognize human speech.<sup>68</sup>
</p>
<p>Typically, <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> is used in the front-end frequency analysis of an <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a>, although not as massively as in vision systems, since the quantity of data is much less. (If one were to approach the thousands of frequency bands used by the human auditory <a class="thought" href="entries/system_entry.html">system</a>, then massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> would be required.) Once the speech signal has been transformed into the frequency <a class="thought" href="entries/domain_entry.html">domain</a>, it is normalized (adjusted) to remove the effects of loudness and background noise. At this point we can detect a <a class="thought" href="entries/number_entry.html">number</a> of features of frequency-band signals and consider the problems of <a class="thought" href="entries/segmentation_entry.html">segmentation</a> and labeling.</p>
<p>As in vision systems, minimal property extraction is one popular technique. One can use as a feature set either the normalized frequency data itself or various transformations of this data. Now, in matching such minimal-property sets, we need to consider the <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> of nonlinear time compression.<sup>69</sup> When we speak, we change our speed according to <a class="thought" href="entries/context_entry.html">context</a> and other factors. If we speak one word more quickly, we do not increase the rate evenly throughout the entire word. The duration of certain portions of the word, such as plosive consonants, will remain fairly constant, while other portions, such as vowels, will undergo most of the change. In matching a spoken word to a stored template, we need to align the corresponding acoustic events, or the match will never succeed. This problem is similar to the matching of visual cues in fusing the stereo images from our two eyes. A mathematical technique called <a class="thought" href="entries/dynamic_programming_entry.html">dynamic programming</a> has been developed to accomplish this temporal alignment.<sup>70</sup>
</p>
<p>As with vision systems, high-level features are also used in <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems. As mentioned above, speech is made up of strings of phonemes, which comprise the basic "alphabet" of spoken <a class="thought" href="entries/language_entry.html">language</a>. In English and other European romance languages, there are about 16 vowels and 24 consonants; Japanese primarily uses only 5 vowels and 15 consonants. The <a class="thought" href="entries/nature_entry.html">nature</a> of a particular <a class="thought" href="entries/phoneme_entry.html">phoneme</a> (such as /a/) is an abstract <a class="thought" href="entries/concept_entry.html">concept</a> in the same way that the inherent <a class="thought" href="entries/nature_entry.html">nature</a> of a printed character (such as <i>A</i>) is: neither can be simply defined. Identifying phonemes in human speech requires intelligent algorithms and recognition of high-level features (something like the loops and concavities found in printed characters). The task of segmenting speech into distinct time slices representing different phonemes is also a formidable task. The time-varying spectrum of frequencies characterizing a <a class="thought" href="entries/phoneme_entry.html">phoneme</a> in one <br>
<img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/articles/images/aimchapter702.jpg" vspace="10"><br>
<span class="PhotoCredit">Photo by Vladimir Sejnoha and courtesy of <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> Applied <a class="thought" href="entries/intelligence_entry.html">Intelligence</a></span>
<br>
<span class="Caption">Nonlinear alignment of speech events.  The lines in the middle graph show the optimal alignment of the spectographs of two utterances of the word "further" spoken by the same <a class="thought" href="entries/female_entry.html">female</a> speaker.</span>
<br>
<a class="thought" href="entries/context_entry.html">context</a> may be dramatically different in a different <a class="thought" href="entries/context_entry.html">context</a>. In fact, in many instances no time slice corresponding to a particular <a class="thought" href="entries/phoneme_entry.html">phoneme</a> can be found; we detect it only from the subtle influence it has on phonemes surrounding it.</p>
<p>As with vision and character recognition, both high-level and low-level features have value in speech-recognition systems. For recognizing a relatively small vocabulary (a few hundred words) for a single speaker, low-level feature detection and <a class="thought" href="entries/template_matching_entry.html">template matching</a> by means of <a class="thought" href="entries/dynamic_programming_entry.html">dynamic programming</a> is usually sufficient, and most small-vocabulary systems use this approach. For more advanced systems, a combination of techniques is usually required: generally, <a class="thought" href="entries/multiple_experts_entry.html">multiple experts</a> and an expert manager that knows the strengths and weaknesses of each.<sup>71</sup>
</p>
<p>High-level <a class="thought" href="entries/context_entry.html">context</a> experts are also vital for large vocabulary systems. For example, phonemes cannot appear in any order. Indeed, many sequences are impossible to articulate (try saying "ptkee"). More important, only certain <a class="thought" href="entries/phoneme_entry.html">phoneme</a> sequences will correspond to a word or word fragment in the <a class="thought" href="entries/language_entry.html">language</a>. On a higher level, the <a class="thought" href="entries/syntax_entry.html">syntax</a> and <a class="thought" href="entries/semantics_entry.html">semantics</a> of the <a class="thought" href="entries/language_entry.html">language</a> put constraints on possible word orders. While the set of phonemes is similar from one <a class="thought" href="entries/language_entry.html">language</a> to another, <a class="thought" href="entries/context_entry.html">context</a> factors differ dramatically. English, for example, has over 10,000 possible syllables, whereas Japanese has only 120.</p>
<p>
<a class="thought" href="entries/learning_entry.html">Learning</a> is also vital in speech recognition. Adaptation to the particular characteristics of each speaker is a powerful technique in each stage of processing. <a class="thought" href="entries/learning_entry.html">Learning</a> must take place on a <a class="thought" href="entries/number_entry.html">number</a> of different levels: the frequency and time relationships characterizing each <a class="thought" href="entries/phoneme_entry.html">phoneme</a>, the dialect (pronunciation) patterns of each word, and the syntactic patterns of possible phrases and sentences.</p>
<p>In sum, we see in speech recognition the full <a class="thought" href="entries/paradigm_entry.html">paradigm</a> of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> that we first encountered in vision and character recognition systems: <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> in the front-end, <a class="thought" href="entries/segmentation_entry.html">segmentation</a> and labeling, <a class="thought" href="entries/multiple_experts_entry.html">multiple experts</a> on both high and low levels, expert management, disambiguation by <a class="thought" href="entries/context_entry.html">context</a> experts, and <a class="thought" href="entries/learning_entry.html">learning</a> from actual recognition examples. But while the <a class="thought" href="entries/paradigm_entry.html">paradigm</a> is the same, the <a class="thought" href="entries/content_entry.html">content</a> is dramatically different. Only a small portion of the <a class="thought" href="entries/technology_entry.html">technology</a> in a successful <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> consists of classical <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques. The bulk of it consists of extensive <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the <a class="thought" href="entries/nature_entry.html">nature</a> of human speech and <a class="thought" href="entries/language_entry.html">language</a>: the shape of the speech sounds and the phonology, <a class="thought" href="entries/syntax_entry.html">syntax</a>, and <a class="thought" href="entries/semantics_entry.html">semantics</a> of spoken <a class="thought" href="entries/language_entry.html">language</a>.<sup>72</sup>
</p>
<p>
<a class="thought" href="entries/automated_speech_rec_entry.html">Automatic speech recognition</a> is receiving considerable attention because of its potential for <a class="thought" href="entries/commercial_entry.html">commercial</a> applications.<sup>73</sup> We learn to understand and produce spoken <a class="thought" href="entries/language_entry.html">language</a> in our first year of life, years before we can understand or create written <a class="thought" href="entries/language_entry.html">language</a>. Thus, being able to communicate with computers using verbal <a class="thought" href="entries/language_entry.html">language</a> would provide an optimal modality of <a class="thought" href="entries/communication_entry.html">communication</a>. A major goal of <a class="thought" href="entries/ai_entry.html">AI</a> is to make our interactions with computers more natural and intuitive. Being able to converse with them by talking and listening is a vital part of that process.</p>
<p>For years <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems have been used in situations where users necessarily have their hands and eyes busy, making it impossible to use ordinary <a class="thought" href="entries/computer_entry.html">computer</a> keyboards and display screens. For example, a laboratory technician examining an image through a microscope or other technical equipment can speak results into the microphone of an <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> while continuing to view the image being examined. Similarly, factory workers can verbalize inspection data and other <a class="thought" href="entries/information_entry.html">information</a> on the production or shop floor directly to a <a class="thought" href="entries/computer_entry.html">computer</a> without having to occupy their hands with a keyboard. Other systems are beginning to automate routine business transactions over telephone lines. Such <a class="thought" href="entries/telecommunications_entry.html">telecommunications</a> applications include credit-card verification, sales-order entry, accessing data base and inventory records, conducting banking and other financial transactions, and many others.</p>
<p>The applications mentioned are usually highly structured and thus require the <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> to recognize only a small vocabulary, typically a few hundred words or less. The largest markets are projected for systems that can handle vocabularies that are relatively unrestricted and much larger, say, ten thousand words or more. The most obvious application of large vocabulary <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems is the creation of written documents by voice. The creation of written documents is a ubiquitous activity engaged in by almost everyone in offices, schools, and homes. Just copying all of the written documents created each year is a $25 billion industry. Tens of billions of dollars are spent each year in the creation of original written works from books to interoffice memoranda. Being able to dictate a document to a <a class="thought" href="entries/computer_entry.html">computer</a> based <a class="thought" href="entries/machine_entry.html">machine</a> and see the document appear on a screen as it is being dictated has obvious advantages in terms of speed, accuracy, and convenience. Large-vocabulary <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>-based dictation systems are beginning to see use by doctors in writing medical reports and by many other professionals.</p>
<p>There are also significant applications of <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/technology_entry.html">technology</a> to the handicapped. Persons who are unable to use their hands because of quadriplegia (paralysis due to spinal-cord injury), cerebral palsy, or other neurological impairment can still create written documents, interact with computers, and otherwise control their environment by speaking to a <a class="thought" href="entries/computer_entry.html">computer</a> equipped with <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>. Certain types of brain damage cause a person's speech to be slurred and distorted. While such speech patterns are unrecognizable by human listeners (unless they have received special training), it is distorted in a consistent way and thus can be recognized by <a class="thought" href="entries/machine_entry.html">machine</a>. A <a class="thought" href="entries/system_entry.html">system</a> consisting of an <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> to recognize the distorted speech connected to a speech <a class="thought" href="entries/synthesizer_entry.html">synthesizer</a> can act as a translator, allowing the person to be understood by others. <a class="thought" href="entries/research_entry.html">Research</a> has been conducted at Children's Hospital, Boston, by Dr. Howard Shane on assisting both the hand-impaired and the speaking-impaired using <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>. Of perhaps the greatest potential benefit to the handicapped, and also representing the greatest technological challenge, would be an <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>-based <a class="thought" href="entries/device_entry.html">device</a> for the deaf that could provide a visual read-out of what people are saying.</p>
<p>There are three fundamental attributes that characterize a particular <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a>: vocabulary size, training requirements, and its ability to handle continuous speech. Vocabulary size is the <a class="thought" href="entries/number_entry.html">number</a> of different words that a <a class="thought" href="entries/system_entry.html">system</a> can handle at one time. Text creation requires a large basic vocabulary as well as the ability to add additional words to the active personal vocabulary of each <a class="thought" href="entries/individual_entry.html">individual</a> user. For most other applications, small vocabularies suffice.</p>
<p>Most <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems require each user to train the <a class="thought" href="entries/system_entry.html">system</a> on their particular pronunciation patterns. This is typically accomplished by making the user provide the <a class="thought" href="entries/system_entry.html">system</a> with one or more spoken samples of each word in the vocabulary. However, for large vocabulary systems, speaking every word in the vocabulary is often not practical. It is preferable if the <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> can infer how the speaker is likely to pronounce words never actually spoken to the <a class="thought" href="entries/machine_entry.html">machine</a>. Then the user needs to train the <a class="thought" href="entries/system_entry.html">system</a> on only a subset of the full vocabulary.</p>
<p>Some small-vocabulary systems have been preprogrammed with all of the dialectic patterns anticipated from the population expected to use the <a class="thought" href="entries/system_entry.html">system</a> and thus do not require any prior training by each user. This capability, called <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a>, is generally required for telephone-based systems, where a single <a class="thought" href="entries/system_entry.html">system</a> can be accessed by a large group of users.</p>
<p>Most <a class="thought" href="entries/commercial_entry.html">commercial</a> systems to date require users to speak with brief pauses (usually around 100 milliseconds) between words. This helps the <a class="thought" href="entries/system_entry.html">system</a> make a crucial <a class="thought" href="entries/segmentation_entry.html">segmentation</a> decision: where words start and end. Speaking with such pauses reduces the speed of a typical speaker by 20 to 50 percent. <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems that can handle continuous speech exist, but they are limited today to small vocabularies. Continuous-speech systems that can handle large vocabularies are expected in the early 1990s.</p>
<p>Other characteristics that are important in describing practical <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems include the accuracy rate, response time, immunity to background noise, requirements for correcting errors, and integration of the speech recognition capability with specific <a class="thought" href="entries/computer_entry.html">computer</a> applications. In general, it is not desirable to simply insert a speech recognition <a class="thought" href="entries/system_entry.html">system</a> as a front-end to ordinary <a class="thought" href="entries/computer_entry.html">computer</a> applications. The human requirements for controlling <a class="thought" href="entries/computer_entry.html">computer</a> applications by voice are substantially different from those of more conventional input devices such as keyboards, so the design of the overall <a class="thought" href="entries/system_entry.html">system</a> needs to take this into account.</p>
<p>While <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems continue to fall far short of human performance, their capabilities are rapidly improving, and <a class="thought" href="entries/commercial_entry.html">commercial</a> applications are taking root. As of 1989 <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> systems could either recognize a large vocabulary (10,000 words or more), recognize continuous speech, or provide <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a> (no user training), but they could provide only one of these capabilities at a time. In 1990, <a class="thought" href="entries/commercial_entry.html">commercial</a> systems were introduced that combined <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a> with the ability to recognize a large vocabulary. I expect it to be possible in the early 1990s to combine any two of these attributes in the same <a class="thought" href="entries/system_entry.html">system</a>. In other words, we will see large vocabulary systems that can handle continuous speech while still requiring training for each speaker; there will be speaker-independent systems that can handle continuous speech but only for small vocabularies; and so on. The <a class="thought" href="entries/holy_grail_entry.html">Holy Grail</a> of speech recognition is to combine all <i>three</i> of these abilities, as human speech recognition does.</p><h2>Other types of auditory <a class="thought" href="entries/perception_entry.html">perception</a></h2><p>There are applications of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> to auditory <a class="thought" href="entries/perception_entry.html">perception</a> other than recognizing speech. A field closely related to <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> is speaker identification, which attempts to reliably identify the <i>person speaking</i> by his or her speech profile. The techniques used are drawn from the <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> field. Common applications include entry and <a class="thought" href="entries/access_entry.html">access</a> control.</p>
<p>
<a class="thought" href="entries/military_entry.html">Military</a> applications of auditory <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> consist primarily of ship and submarine detection. It is not possible to see through the ocean, but sound waves can travel through the water for long distances. By sending sound waves and analyzing the reflected patterns, marine vehicles can be recognized. One of the challenges is to correctly characterize the patterns of different types of submarines and ships as opposed to whales and other natural undersea phenomena.</p>
<p>There are also a <a class="thought" href="entries/number_entry.html">number</a> of medical applications. Scanning the body with sound waves has become a powerful noninvasive diagnostic tool, and systems are being developed to apply <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques to data from sonogram scanners. Perhaps the most significant medical application lies in the area of listening to the human heart. Many irregularities in heartbeat (arrhythmias) occur infrequently. So a technique has been developed that involves recording an electrocardiogram for a 24-hour period. In such Holter monitoring the patient goes through a normal day while a portable unit makes a tape recording of his heart <a class="thought" href="entries/pattern_entry.html">pattern</a>. The recording is then reviewed on a special screen by a human technician at 24 times normal speed, thus requiring an hour of analysis. But reading an electrocardiogram at 24 times real time for an hour is extremely demanding and studies have indicated a significant rate of errors. Using <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition techniques, computers have been programmed to analyze the recording at several hundred times real time and thus have the potential for providing lower costs and higher accuracy. Similar systems are used to monitor the vital functions of critical-care patients. Eventually wristwatch systems will be able to monitor our vital functions on a continuous basis. Heart patients could be told by their wristwatches to slow down or take other appropriate <a class="thought" href="entries/action_entry.html">action</a> if it determines they are overexerting themselves or otherwise getting into difficulty.</p>
<p>Next to human speech, perhaps the most important type of sound that we are exposed to is music. It might be argued that music is the most fundamental of the arts: it is the only one universal to all known cultures. The musical-tone qualities created by complex acoustic instruments such as the piano or violin have unique psychological effects, particularly when combined with the other elements of music (melody, rhythm, <a class="thought" href="entries/harmony_entry.html">harmony</a>, and <a class="thought" href="entries/expression_entry.html">expression</a>). What is it that makes a piano sound the way it does? Every piano sounds somewhat different, yet they are all recognizable as the same type of <a class="thought" href="entries/instrument_entry.html">instrument</a>. This is essentially a <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition question, and insight into the answer has provided the ability to recreate such sounds using <a class="thought" href="entries/computer_entry.html">computer</a>-based synthesizers. It also provides the ability to create entirely new synthetic sounds that have the same richness, depth, and musical relevance as acoustically produced sounds.<sup>74</sup>
</p>
<p>Analogs to the other human senses are being developed as well. Chemical-analysis systems are beginning to emulate the functions of taste and smell. A variety of tactile sensors have been developed to provide robots with a sense of touch to augment their sight.</p>
<p>Taken together, applications of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> comprise fully half of the <a class="thought" href="entries/ai_entry.html">AI</a> industry. It is surprising that many discussions of <a class="thought" href="entries/ai_entry.html">AI</a> overlook this area entirely. Unlike expert systems and other areas that primarily emphasize logical rules and relationships, the field of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a> combines both parallel and sequential types of <a class="thought" href="entries/thinking_entry.html">thinking</a>. Because of their need for enormous amounts of <a class="thought" href="entries/computation_entry.html">computation</a>, <a class="thought" href="entries/pattern_entry.html">pattern</a>-recognition systems tend to require the cutting edge of advanced <a class="thought" href="entries/computer_entry.html">computer</a> architectures.</p>
<p>It is widely recognized that computers will require extensive <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the world to perform useful intelligent functions. It is not feasible for <a class="thought" href="entries/computer_entry.html">computer</a> scientists to explicitly teach our computers all there is to know about the entire world. Like children, <a class="thought" href="entries/ai_entry.html">AI</a> systems will need to acquire their own <a class="thought" href="entries/knowledge_entry.html">knowledge</a> by reading, looking, listening, and drawing their own conclusions based on their own perceptions, perceptions based on <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a>.</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D4246" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id4247"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Linear vs parallel<br><span class="mindxheader"><i>posted on 11/24/2001 5:25 PM by richard@controlvision.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D4246%23id4247" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614002219/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D4247" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>The simplest illustration is a child learning to ride a bicycle. At a certain point in the process, the abstract or global or parallel community of proccesing centers creates an overwhelming sense of fatigue in the linear process that has kept the child focusing on control of the front wheel and overcoming the tendency to lean left or right. At this moment, the suspension of analog direction gives way to an apparatus that senses the gyroscopic forces making innate balance possible and provocative. No other skill rewards this leap of faith with they enjoyment of a dynamically stable environment. It should be part of public school. (sic).</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614002219im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>