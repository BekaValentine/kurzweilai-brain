<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>Nanotechnology Dangers and Defenses</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/meme/memelist.html?m=10">Kurzweil Archives</a> &gt; 
Nanotechnology Dangers and Defenses
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20100614004803/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0653.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0653.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/articles/art0653.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">Nanotechnology Dangers and Defenses</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0005.html" target="_top">Ray Kurzweil</a><br></span></td>
</table>
<br>
<div class="TeaserText">To avoid dangers such as unrestrained nanobot replication, we need relinquishment at the right level and to place our highest priority on the continuing advance of defensive technologies, staying ahead of destructive technologies. An overall strategy should include a streamlined regulatory process, a global program of monitoring for unknown or evolving biological pathogens, temporary moratoriums, raising public awareness, international cooperation, software reconnaissance, and fostering values of liberty, tolerance, and respect for knowledge and diversity. </div>
<br>
<br>
<p><i>Originally published in </i><a href="http://web.archive.org/web/20100614004803/http://pages.unibas.ch/colbas/ntp/" target="_blank">Nanotechnology 
              Perceptions: A Review of Ultraprecision Engineering and Nanotechnology</a><i>, 
              Volume 2, No. 1, March 27, 2006. Reprinted with permission on KurzweilAI.net, 
              March 27, 2005.</i></p>
<p>The first half of the 21st century will be characterized by three 
              overlapping revolutions&#8212;in <a class="thought" href="entries/genetics_entry.html">Genetics</a>, <a class="thought" href="entries/nanotechnology_entry.html">Nanotechnology</a>, and <a class="thought" href="entries/robotics_entry.html">Robotics</a> 
              (<a class="thought" href="entries/gnr_entry.html">GNR</a>). The deeply intertwined promise and peril of these technologies 
              has led some serious thinkers to propose that we go very cautiously, 
              possibly even to abandon them altogether. </p>
<p>A few years ago, <a class="thought" href="entries/computer_entry.html">computer</a> maven <a class="thought" href="entries/joy_entry.html">Bill Joy</a> wrote, "We are being propelled 
              into a new century with no plan, no control, no brakes&#8230; The 
              only realistic alternative I see is <a class="thought" href="entries/relinquishment_entry.html">relinquishment</a>: to limit the 
              development of the technologies that are too dangerous, by limiting 
              our pursuit of certain kinds of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>."<sup>1</sup></p>
<p>Joy's deep concern about the <a class="thought" href="entries/future_entry.html">future</a> grew out of a <a class="thought" href="entries/conversation_entry.html">conversation</a> 
              we had in 1998 about these emerging technologies, and an early draft 
              of <i>The <a class="thought" href="entries/age_of_spiritual_machines_entry.html">Age of Spiritual Machines</a></i> that I gave him. Although 
              I have a reputation as a <a class="thought" href="entries/technology_entry.html">technology</a> optimist, it turns out that 
              at public discussions of "promise and peril," I often spend much 
              of my <a class="thought" href="entries/time_entry.html">time</a> defending Joy's position on the feasibility of the dangers 
              that concern him. Indeed, Joy and I agree on both promise and peril. 
            </p>
<p>Technology has always been a mixed blessing, bringing us benefits 
              such as longer and healthier lifespans, <a class="thought" href="entries/freedom_entry.html">freedom</a> from physical and 
              mental drudgery, and many <a class="thought" href="entries/novel_entry.html">novel</a> creative possibilities on the one 
              hand, while introducing new dangers. Technology empowers both our 
              creative and destructive <a class="thought" href="entries/nature_entry.html">nature</a>s.</p>
<p>Broad relinquishment is contrary to economic <a class="thought" href="entries/progress_entry.html">progress</a> and is ethically 
              unjustified given the opportunity to alleviate <a class="thought" href="entries/disease_entry.html">disease</a>, overcome 
              <a class="thought" href="entries/poverty_entry.html">poverty</a>, and clean up the environment. Joy and I also agree that 
              relinquishment of major fields such as genetics ("G"), nanotechnology 
              ("N"), or <a class="thought" href="entries/strong_ai_entry.html">strong AI</a>/robotics ("R") is not the answer. There is, 
              however, a growing movement advocating exactly that. Bill McKibben, 
              the environmentalist who first brought <a class="thought" href="entries/global_warming_entry.html">global warming</a> to our attention, 
              argues in his book <i>Enough</i> that we have had "enough" technology 
              and should not pursue more. However, regulations on safety&#8212;essentially 
              fine-grained relinquishment&#8212;will remain an appropriate strategy. 
              In that <a class="thought" href="entries/spirit_entry.html">spirit</a>, Joy and I recently wrote a joint op ed piece ("Recipe 
              for Destruction") published in the New York Times on October 17, 
              2005 criticizing the publication of the 1918 flu <a class="thought" href="entries/genome_entry.html">genome</a> on the web. 
            </p>
<h3> Dangers to Defend Against</h3>
<p>As technology accelerates toward the full realization of GNR, we 
              will see interweaving potentials: a feast of <a class="thought" href="entries/creativity_entry.html">creativity</a> resulting 
              from <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> expanded manyfold, combined with many grave 
              new dangers. A quintessential concern that has received considerable 
              attention is unrestrained <a class="thought" href="entries/nanobot_entry.html">nanobot</a> replication. Early proposals for 
              molecular manufacturing required trillions of intelligently designed 
              <a class="thought" href="entries/device_entry.html">device</a>s to be useful. To scale up to such levels it would have been 
              necessary to enable them to self-replicate, essentially the same 
              approach used in the <a class="thought" href="entries/biological_entry.html">biological</a> world (that's how one fertilized 
              egg <a class="thought" href="entries/cell_entry.html">cell</a> becomes the trillions of cells in a human).</p>
<p>Although the <a class="thought" href="entries/self_replication_entry.html">self-replication</a> can be hidden and blocked in a variety 
              of ways (for example, Ralph Merkle's proposal<sup>1</sup> for a 
              "<a class="thought" href="entries/broadcast_entry.html">broadcast</a> <a class="thought" href="entries/architecture_entry.html">architecture</a>" in which each replicating <a class="thought" href="entries/entity_entry.html">entity</a> needs 
              to get the replicating codes from a secure <a class="thought" href="entries/server_entry.html">server</a>), the overall 
              <a class="thought" href="entries/system_entry.html">system</a> will have self-replication at some level. And in the same 
              way that biological self-replication gone awry (that is, <a class="thought" href="entries/cancer_entry.html">cancer</a>) 
              results in biological destruction, a defect in a mechanism curtailing 
              nanobot self-replication&#8212;the so-called gray goo scenario&#8212;would 
              endanger all physical entities, biological or otherwise.</p>
<p>Modern proposals, such as the use of large integrated manufacturing 
              systems rather than trillions of quasi-independent nanobots, appear 
              to prevent inadvertent release of destructive self-replication, 
              but in general these safeguards can be worked around by a determined 
              adversary. We see a similar situation today in biological technologies. 
              The ethical guidelines for gene modification technologies adopted 
              at the Asilomar Conference have worked well for over a quarter of 
              a century, but these guidelines would not restrict a would-be bioterrorist 
              because they don't have to follow the guidelines (they don't have 
              to put their "<a class="thought" href="entries/invention_entry.html">invention</a>s" through the FDA either). </p>
<p>These guidelines and strategies are likely to be effective for 
              preventing accidental release of dangerous self-replicating nanotechnology 
              entities. But dealing with the intentional design and release of 
              such entities is a more complex and challenging problem. A sufficiently 
              determined and destructive opponent could possibly defeat each of 
              these layers of protections. Take, for example, the broadcast architecture. 
              When properly designed, each entity is unable to replicate without 
              first obtaining replication codes, which are not repeated from one 
              replication generation to the next. However, a modification to such 
              a design could bypass the destruction of the replication codes and 
              thereby pass them on to the next generation. To counteract that 
              possibility it has been recommended that the <a class="thought" href="entries/memory_entry.html">memory</a> for the replication 
              codes be limited to only a subset of the full <a class="thought" href="entries/code_entry.html">code</a>. However, this 
              guideline could be defeated by expanding the size of the memory.</p>
<p>Another protection that has been suggested is to encrypt the codes 
              and build in protections in the decryption systems, such as time-expiration 
              limitations. However, we can see how easy is has been to defeat 
              protections against unauthorized replications of <a class="thought" href="entries/intellectual_property_entry.html">intellectual property</a> 
              such as <a class="thought" href="entries/music_entry.html">music</a> files. Once replication codes and protective layers 
              are stripped away, the <a class="thought" href="entries/information_entry.html">information</a> can be replicated without these 
              restrictions.</p>
<p>This doesn't mean that that protection is impossible. Rather, each 
              level of protection will work only to a certain level of sophistication. 
              The meta lesson here is that we will need to place twenty-first-century 
              <a class="thought" href="entries/society_entry.html">society</a>'s highest priority on the continuing advance of defensive 
              technologies, keeping them one or more steps ahead of the destructive 
              technologies (or at least no more than a quick step behind).</p>
<p>Living creatures&#8212;including humans&#8212;would be the primary 
              victims of an exponentially spreading nanobot attack. The principal 
              designs for nanobot construction use <a class="thought" href="entries/carbon_entry.html">carbon</a> as a primary building 
              block. Because of carbon's unique ability to form four-way bonds, 
              it is an ideal building block for molecular assemblies. Because 
              <a class="thought" href="entries/biology_entry.html">biology</a> has made the same use of carbon, pathological nanobots would 
              find the <a class="thought" href="entries/earth_entry.html">Earth</a>'s biomass an ideal source of this primary ingredient.</p>
<p>How long would it take an out-of-control replicating nanobot to 
              destroy the Earth's biomass? The biomass has on the <a class="thought" href="entries/order_entry.html">order</a> of 10<sup>45</sup> 
              carbon atoms. A <a class="thought" href="entries/reason_entry.html">reason</a>able estimate of the <a class="thought" href="entries/number_entry.html">number</a> of carbon atoms 
              in a single replicating nanobot is about 10<sup>6</sup>. (Note that 
              this analysis is not very sensitive to the accuracy of these figures, 
              only to the approximate order of magnitude.) This malevolent nanobot 
              would need to create on the order of 10<sup>34</sup> copies of itself 
              to replace the biomass, which could be accomplished with 113 replications 
              (each of which would potentially double the destroyed biomass). 
              Rob Freitas has estimated a minimum replication time of approximately 
              100 seconds, so 113 replication cycles would require about three 
              hours.<sup>2</sup> However, the actual rate of destruction would 
              be slower because biomass is not "efficiently" laid out. The limiting 
              factor would be the actual movement of the front of destruction. 
              Nanobots cannot travel very quickly because of their small size. 
              It's likely to take weeks for such a destructive process to circle 
              the globe.</p>
<p>Based on this observation we can envision a more insidious possibility. 
              In a two-phased attack, the nanobots take several weeks to spread 
              throughout the biomass but use up an insignificant portion of the 
              carbon atoms, say one out of every thousand trillion (10<sup>15</sup>). 
              At this extremely low level of concentration, the nanobots would 
              be as stealthy as possible. Then, at an "optimal" point, the second 
              phase would begin with the seed nanobots expanding rapidly in place 
              to destroy the biomass. For each seed nanobot to multiply itself 
              a thousand trillionfold would require only about 50 <a class="thought" href="entries/binary_entry.html">binary</a> replications, 
              or about 90 minutes. With the nanobots having already spread out 
              in position throughout the biomass, movement of the destructive 
              <a class="thought" href="entries/wave_entry.html">wave</a> front would no longer be a limiting factor.</p>
<p>The point is that without defenses, the available biomass could 
              be destroyed by gray goo very rapidly. Clearly, we will need a nanotechnology 
              <a class="thought" href="entries/immune_system_entry.html">immune system</a><sup>3</sup> in place <i>before</i> these scenarios 
              become a possibility. This immune system would have to be capable 
              of contending not just with obvious destruction but any potentially 
              dangerous (stealthy) replication, even at very low concentration.</p>
<p>Eric Drexler, <a class="thought" href="entries/freitas_entry.html">Robert Freitas</a>, Ralph Merkle, Mike Treder, Chris 
              Phoenix, and others have pointed out that future <a class="thought" href="entries/nanotechnology_entry.html">nanotech</a> manufacturing 
              devices can be created with safeguards that would prevent the accidental 
              creation of self-replicating nanodevices.<sup>4</sup> However, this 
              observation, although <a class="thought" href="entries/import_entry.html">import</a>ant, does not eliminate the threat of 
              gray goo as I pointed out above. There are other reasons (beyond 
              manufacturing) that self-replicating nanobots will need to be created. 
              The nanotechnology immune system mentioned above, for example, will 
              ultimately require self-replication; otherwise it would be unable 
              to defend us against the development of increasingly sophisticated 
              types of goo. It is also likely to find extensive <a class="thought" href="entries/military_entry.html">military</a> applications. 
              Moreover, a determined adversary or terrorist can defeat safeguards 
              against unwanted self-replication; hence, the need for defense.</p>
<p>Bill Joy and other observers have pointed out that such an immune 
              system would itself be a danger because of the potential of "autoimmune" 
              reactions (that is, the immune-system nanobots attacking the world 
              they are supposed to defend). However, this possibility is not a 
              compelling reason to avoid the creation of an immune system. No 
              one would argue that humans would be better off without an immune 
              system because of the potential of developing autoimmune diseases. 
              Although the biological immune system can itself present a danger, 
              humans would not last more than a few weeks (barring extraordinary 
              efforts at isolation) without one. And even so, the development 
              of a technological immune system for nanotechnology will happen 
              even without explicit efforts to create one. This has effectively 
              happened with regard to <a class="thought" href="entries/software_entry.html">software</a> viruses, creating an immune system 
              not through a formal grand-design project but rather through incremental 
              responses to each new challenge and by developing <a class="thought" href="entries/heuristic_entry.html">heuristic</a> <a class="thought" href="entries/algorithm_entry.html">algorithm</a>s 
              for early detection. We can expect the same thing will happen as 
              challenges from nanotechnology-based dangers emerge. The point for 
              public policy will be to specifically invest in these defensive 
              technologies.</p>
<p>As a test case, we can take a small measure of comfort from how 
              we have dealt with one recent technological challenge. There exists 
              today a new fully <a class="thought" href="entries/nonbiological_entry.html">nonbiological</a> self-replicating entity that didn't 
              exist just a few decades ago: the computer <a class="thought" href="entries/virus_entry.html">virus</a>. When this form 
              of destructive intruder first appeared, strong concerns were voiced 
              that as they became more sophisticated, software <a class="thought" href="entries/pathogen_entry.html">pathogen</a>s had the 
              potential to destroy the computer-<a class="thought" href="entries/network_entry.html">network</a> medium in which they live. 
              Yet the "immune system" that has evolved in response to this challenge 
              has been largely effective. Although destructive self-replicating 
              software entities do cause damage from time to time, the injury 
              is but a small fraction of the benefit we receive from the computers 
              and <a class="thought" href="entries/communication_entry.html">communication</a> links that harbor them.</p>
<p>One might counter that computer viruses do not have the lethal 
              potential of biological viruses or of destructive nanotechnology. 
              This is not always the case; we rely on software to operate our 
              911 call centers, monitor patients in critical-care units, fly and 
              land airplanes, guide intelligent <a class="thought" href="entries/weapon_entry.html">weapon</a>s in our military campaigns, 
              handle our financial transactions, operate our municipal utilities, 
              and many other mission-critical tasks. To the extent that software 
              viruses do not yet pose a lethal danger, however, this observation 
              only strengthens my argument. The fact that computer viruses are 
              not usually deadly to humans only means that more people are willing 
              to create and release them. The vast majority of software virus 
              authors would not release viruses if they <a class="thought" href="entries/thought_entry.html">thought</a> they would kill 
              people. It also means that our response to the danger is that much 
              less intense. Conversely, when it comes to self-replicating entities 
              that are potentially lethal on a large scale, our response on all 
              levels will be vastly more serious.</p>
<p>Although software pathogens remain a concern, the danger exists 
              today mostly at a nuisance level. Keep in <a class="thought" href="entries/mind_entry.html">mind</a> that our success 
              in combating them has taken place in an industry in which there 
              is no regulation and minimal certification for practitioners. The 
              largely unregulated computer industry is also enormously productive. 
              One could argue that it has contributed more to our technological 
              and economic progress than any other enterprise in human <a class="thought" href="entries/history_entry.html">history</a>.</p>
<p>But the battle concerning software viruses and the panoply of software 
              pathogens will never end. We are becoming increasingly reliant on 
              mission-critical software systems, and the sophistication and potential 
              destructiveness of self-replicating software weapons will continue 
              to escalate. When we have software running in our brains and bodies 
              and controlling the world's nanobot immune system, the stakes will 
              be immeasurably greater.</p>
<h3> <br>
              The Right Level of Relinquishment</h3>
<p>The only conceivable way that the accelerating pace of GNR technology 
              advancement could be stopped would be through a worldwide totalitarian 
              system that relinquishes the very idea of progress. Even this specter 
              would be likely to fail in averting the dangers of GNR because the 
              resulting underground activity would tend to favor the more destructive 
              applications. This is because the responsible practitioners that 
              we rely on to quickly develop defensive technologies would not have 
              easy <a class="thought" href="entries/access_entry.html">access</a> to the needed tools. Fortunately, such a totalitarian 
              outcome is unlikely because the increasing decentralization of knowledge 
              is inherently a democratizing force.</p>
<p>I do think that relinquishment at the right level needs to be part 
              of our ethical response to the dangers of 21st century technologies. 
              One constructive example of this is the ethical guideline proposed 
              by the <a class="thought" href="entries/foresight_institute_entry.html">Foresight Institute</a>: namely, that nanotechnologists agree 
              to relinquish the development of physical entities that can self-replicate 
              in a natural environment. In my view, there are two exceptions to 
              this guideline. First, we will ultimately need to provide a nanotechnology-based 
              <a class="thought" href="entries/planet_entry.html">planet</a>ary immune system (nanobots embedded in the natural environment 
              to protect against rogue self-replicating nanobots). Robert Freitas 
              and I have discussed whether or not such an immune system would 
              itself need to be self-replicating. Freitas writes: "A comprehensive 
              surveillance system coupled with prepositioned resources&#8212;resources 
              including high-<a class="thought" href="entries/capacity_entry.html">capacity</a> nonreplicating nanofactories able to churn 
              our large numbers of nonreplicating defenders in response to specific 
              threats&#8212;should suffice."<sup>5</sup> I agree with Freitas that 
              a prepositioned immune system with the ability to augment the defenders 
              will be sufficient in early stages. But once strong AI is merged 
              with nanotechnology, and the <a class="thought" href="entries/ecology_entry.html">ecology</a> of nanoengineered entities 
              becomes highly varied and complex, my own expectation is that we 
              will find that the defending nanorobots need the ability to replicate 
              in place quickly. Biological <a class="thought" href="entries/evolution_entry.html">evolution</a> essentially made the same 
              "discovery." The other exception is the need for self-replicating 
              nanobot-based probes to explore planetary systems outside of our 
              <a class="thought" href="entries/solar_system_entry.html">solar system</a>.</p>
<p>Broad relinquishment of GNR technologies would be unwise for several 
              reasons. However, I do think we need to take seriously the increasingly 
              strident voices that advocate for it, even though many of these 
              advocates are motivated by a general distrust of technology, and 
              their proposals are not well considered. Although blanket relinquishment 
              is not the answer, rational fear could lead to irrational solutions, 
              and those solutions may cause severe negative consequences.</p>
<p>A summary of an overall strategy for defending against the downsides 
              of emerging GNR technologies would include the following:</p>
<p>
<ul>
<li>We need to streamline the regulatory process for genetic and 
                medical technologies. The regulations do not impede the malevolent 
                use of technology but significantly delay the needed defenses. 
                As mentioned, we need to better balance the risks of new technology 
                (for example, new medications) against the known harm of delay.</li>
</ul>
<p>
<ul>
<li>A global <a class="thought" href="entries/program_entry.html">program</a> of confidential, random serum monitoring for 
                unknown or evolving biological pathogens should be funded. Diagnostic 
                tools exist to rapidly identify the <a class="thought" href="entries/existence_entry.html">existence</a> of unknown <a class="thought" href="entries/protein_entry.html">protein</a> 
                or nucleic acid sequences. Intelligence is key to defense, and 
                such a program could provide invaluable early warning of an impending 
                epidemic. Such a 'pathogen sentinel' program has been proposed 
                for many years by public health authorities but has never received 
                adequate funding.</li>
</ul>
<p>
<ul>
<li>Well-defined and targeted temporary moratoriums, such as the 
                one that occurred in the genetics field in 1975, may be needed 
                from time to time. But such moratoriums are unlikely to be necessary 
                with nanotechnology. Broad efforts at relinquishing major areas 
                of technology serve only to continue vast human suffering by delaying 
                the beneficial aspects of new technologies, and actually make 
                the dangers worse.</li>
</ul>
<p>
<ul>
<li>Efforts to define safety and ethical guidelines for nanotechnology 
                should continue. Such guidelines will inevitably become more detailed 
                and refined as we get closer to molecular manufacturing.</li>
</ul>
<p>
<ul>
<li>To create the political support to fund the efforts suggested 
                above, it is necessary to <i>raise public awareness of these dangers</i>. 
                Because, of course, there exists the downside of raising alarm 
                and generating uninformed backing for broad antitechnology mandates, 
                we also need to create a public understanding of the profound 
                benefits of continuing advances in technology.</li>
</ul>
<p>
<ul>
<li>These risks cut across international boundaries&#8212;which is, 
                of course, nothing new; biological viruses, software viruses, 
                and missiles already cross such boundaries with impunity. <i>International 
                cooperation</i> was vital to containing the SARS virus and will 
                become increasingly vital in confronting future challenges. Worldwide 
                organizations such as the World Health Organization, which helped 
                coordinate the SARS response, and is now dealing with the possibility 
                of a bird flu pandemic, need to be strengthened. </li>
</ul>
<p>
<ul>
<li>A <a class="thought" href="entries/content_entry.html">content</a>ious contemporary political issue is the need for preemptive 
                <a class="thought" href="entries/action_entry.html">action</a> to combat threats, such as terrorists with access to weapons 
                of mass destruction or rogue nations that support such terrorists. 
                Such measures will always be controversial, but the potential 
                need for them is clear. A nuclear explosion can destroy a city 
                in seconds. A self-replicating pathogen, whether biological or 
                nanotechnology based, could destroy our <a class="thought" href="entries/civilization_entry.html">civilization</a> in a <a class="thought" href="entries/matter_entry.html">matter</a> 
                of days or weeks. We cannot always afford to wait for the massing 
                of armies or other overt indications of ill intent before taking 
                protective action.</li>
</ul>
<p>
<ul>
<li>Intelligence agencies and policing authorities will have a vital 
                role in forestalling the vast majority of potentially dangerous 
                incidents. Their efforts need to involve the most powerful technologies 
                available. For example, before this decade is out devices the 
                size of dust <a class="thought" href="entries/particle_entry.html">particle</a>s will be able to carry out reconnaissance 
                missions. When we reach the 2020s and have software running in 
                our bodies and brains, <a class="thought" href="entries/government_entry.html">government</a> authorities will have a legitimate 
                need on occasion to monitor these software streams. The potential 
                for abuse of such powers is obvious. We will need to achieve a 
                middle road of preventing catastrophic events while preserving 
                our <a class="thought" href="entries/privacy_entry.html">privacy</a> and liberty.</li>
</ul>
<p>
<ul>
<li>The above approaches will be inadequate to deal with the danger 
                from pathological R (strong AI). Our primary strategy in this 
                area should be to optimize the likelihood that future nonbiological 
                intelligence will reflect our values of liberty, tolerance, and 
                respect for knowledge and <a class="thought" href="entries/diversity_entry.html">diversity</a>. The best way to accomplish 
                this is to foster those values in our society today and going 
                forward. If this sounds vague, it is. But there is no purely technical 
                strategy that is workable in this area because greater intelligence 
                will always find a way to circumvent measures that are the product 
                of a lesser intelligence. The nonbiological intelligence we are 
                creating is and will be embedded in our societies and will reflect 
                our values, as inconsistent and conflicted as these may appear 
                to be. The transbiological phase will involve nonbiological intelligence 
                deeply integrated with biological intelligence. This will amplify 
                our abilities, and our application of these greater intellectual 
                powers will be governed by the values of its creators. The transbiological 
                era will ultimately give way to the postbiological era, but it 
                is to be hoped that our values will remain influential. This strategy 
                is certainly not foolproof, but it is the primary means we have 
                today to influence the future course of strong AI.</li>
</ul>
<p>Technology will remain a double-edged sword. It represents vast 
              power to be used for all humankind's purposes. GNR will provide 
              the means to overcome age-old problems such as illness and poverty, 
              but also will empower destructive ideologies. We have no choice 
              but to strengthen our defenses while we apply these quickening technologies 
              to advance our human values, despite an apparent lack of consensus 
              on what those values should be. </p>
<hr>
<p>1 Bill Joy, "Why the future doesn't need us," <a class="thought" href="entries/wired_entry.html">Wired</a> April 
              2000, <a href="http://web.archive.org/web/20100614004803/http://www.wired.com/wired/archive/8.04/joy_pr.html" target="_blank">http://www.wired.com/wired/archive/8.04/joy_pr.html</a></p>
<p>2 "Self replicating systems and low cost manufacturing" (1994) 
              <a href="http://web.archive.org/web/20100614004803/http://www.zyvex.com/nanotech/selfRepNATO.html" target="_blank">http://www.zyvex.com/nanotech/selfRepNATO.html</a>
<a href="http://web.archive.org/web/20100614004803/http://www.kurzweilai.net/articles/art0142.html" target="_top"> 
              http://www.kurzweilai.net/articles/art0142.html</a></p>
<p>3 More fully discussed in my book, <i><a href="http://web.archive.org/web/20100614004803/http://www.singularity.com/" target="_blank">The 
              Singularity is Near</a></i>, Chapter 8</p>
<p>4 "Gray Goo is a Small Issue," Center for Responsible 
              Nanotechnology, Dec. 14, 2003, <a href="http://web.archive.org/web/20100614004803/http://www.crnano.org/BD-Goo.htm" target="_blank">http://www.crnano.org/BD-Goo.htm</a></p>
<p>5 Private correspondence</p>
<p><i>&#169; 2006 <a class="thought" href="entries/kurzweil_entry.html">Ray Kurzweil</a></i></p>
</p></p></p></p></p></p></p></p></p></td><td>&#160;</td><td class="sidebar" valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p>
<p>&#160;</p>
<p>Other articles by members of the Global Task Force of The Center 
              for Responsible Nanotechnology, published in <i>Nanotechnology Perceptions</i>, 
              Volume 2, Number 1, March 27, 2006, are listed <a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0654.html" target="_top"><b>here</b></a>. 
            </p>
</p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D57937" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id57938"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Kurzweil defensive strategy<br><span class="mindxheader"><i>posted on 04/06/2006 2:59 PM by <a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/profile.php?id=2710">davidishalom1</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D57937%23id57938" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D57938" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>again the restless genius Kurzweil places himself in the cutting edge of technology prognostication. This time in the  nano-AI-defensive strategy required for human survival toward the singularity and beyond. David Ish Shalom</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id67282"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Nanotechnology Dangers and Defenses<br><span class="mindxheader"><i>posted on 09/12/2006 3:41 PM by <a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/profile.php?id=3169">mindx back-on-track</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D57937%23id67282" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614004803/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D67282" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>back-on-track</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614004803im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>