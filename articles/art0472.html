<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>A Computational Foundation for the Study of Cognition</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/meme/memelist.html?m=4">Will Machines Become Conscious?</a> &gt; 
A Computational Foundation for the Study of Cognition
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20071011224129/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0472.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0472.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/articles/art0472.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">A Computational Foundation for the Study of Cognition</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0190.html" target="_top">David Chalmers</a><br></span></td>
</table>
<br>
<div class="TeaserText">Computation is central to the foundations of modern cognitive science, but its role is controversial. Questions about computation abound: What is it for a physical system to implement a computation? Is computation sufficient for thought? What is the role of computation in a theory of cognition? What is the relation between different sorts of computational theory, such as connectionism and symbolic computation? This article develops a systematic framework that addresses all of these questions. A careful analysis of computation and its relation to cognition suggests that the ambitions of artificial intelligence and the centrality of computation in cognitive science are justified. </div>
<br>
<br><i>Originally published at </i><a href="http://web.archive.org/web/20071011224129/http://www.u.arizona.edu/%7Echalmers/ai-papers.html" target="_blank">Papers 
            on AI and Computation (David Chalmers</a>)<i>. Published on KurzweilAI.net 
            June 4, 2002. </i>
<h2>Long Abstract</h2>
<p><a class="thought" href="entries/computation_entry.html">Computation</a> is central to the foundations of modern <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>, 
              but its role is controversial. Questions about <a class="thought" href="entries/computation_entry.html">computation</a> abound: 
              What is it for a physical <a class="thought" href="entries/system_entry.html">system</a> to <a class="thought" href="entries/implement_entry.html">implement</a> a <a class="thought" href="entries/computation_entry.html">computation</a>? Is 
              <a class="thought" href="entries/computation_entry.html">computation</a> sufficient for <a class="thought" href="entries/thought_entry.html">thought</a>? What is the role of <a class="thought" href="entries/computation_entry.html">computation</a> 
              in a theory of cognition? What is the relation between different 
              sorts of <a class="thought" href="entries/computation_entry.html">computation</a>al theory, such as <a class="thought" href="entries/connectionism_entry.html">connectionism</a> and <a class="thought" href="entries/symbol_entry.html">symbol</a>ic 
              <a class="thought" href="entries/computation_entry.html">computation</a>? In this paper I develop a <a class="thought" href="entries/system_entry.html">system</a>atic framework that 
              addresses all of these questions. </p>
<p>Justifying the role of <a class="thought" href="entries/computation_entry.html">computation</a> requires analysis of <i><a class="thought" href="entries/implement_entry.html">implement</a>ation</i>, 
              the nexus between abstract <a class="thought" href="entries/computation_entry.html">computation</a>s and concrete physical <a class="thought" href="entries/system_entry.html">system</a>s. 
              I give such an analysis, based on the idea that a <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s 
              a <a class="thought" href="entries/computation_entry.html">computation</a> if the causal <a class="thought" href="entries/structure_entry.html">structure</a> of the <a class="thought" href="entries/system_entry.html">system</a> mirrors the 
              formal <a class="thought" href="entries/structure_entry.html">structure</a> of the <a class="thought" href="entries/computation_entry.html">computation</a>. This account can be used to 
              justify the central commitments of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al 
              <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>: the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency, which 
              holds that the right kind of <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/structure_entry.html">structure</a> suffices for 
              the possession of a <a class="thought" href="entries/mind_entry.html">mind</a>, and the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation, 
              which holds that <a class="thought" href="entries/computation_entry.html">computation</a> provides a general framework for the 
              explanation of cognitive processes. The theses are consequences 
              of the facts that (a) <a class="thought" href="entries/computation_entry.html">computation</a> can specify general <a class="thought" href="entries/pattern_entry.html">pattern</a>s of 
              causal organization, and (b) mentality is an <i>organizational invariant</i>, 
              rooted in such <a class="thought" href="entries/pattern_entry.html">pattern</a>s. Along the way I answer various challenges 
              to the <a class="thought" href="entries/computation_entry.html">computation</a>alist position, such as those put forward by Searle. 
              I close by advocating a kind of minimal <a class="thought" href="entries/computation_entry.html">computation</a>alism, compatible 
              with a very wide variety of empirical approaches to the <a class="thought" href="entries/mind_entry.html">mind</a>. This 
              allows <a class="thought" href="entries/computation_entry.html">computation</a> to serve as a true foundation for <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>. 
            </p>
<p><b><a class="thought" href="entries/keyword_entry.html">Keyword</a>s</b>: <a class="thought" href="entries/computation_entry.html">computation</a>; cognition; <a class="thought" href="entries/implement_entry.html">implement</a>ation; explanation; 
              <a class="thought" href="entries/connectionism_entry.html">connectionism</a>; <a class="thought" href="entries/computation_entry.html">computation</a>alism; representation; <a class="thought" href="entries/ai_entry.html">artificial intelligence</a>.</p>
<h2>1 Introduction</h2>
<p>Perhaps no <a class="thought" href="entries/concept_entry.html">concept</a> is more central to the foundations of modern 
              <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> than that of <a class="thought" href="entries/computation_entry.html">computation</a>. The ambitions of artificial 
              <a class="thought" href="entries/intelligence_entry.html">intelligence</a> rest on a <a class="thought" href="entries/computation_entry.html">computation</a>al framework, and in other areas 
              of <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>, models of cognitive processes are most frequently 
              cast in <a class="thought" href="entries/computation_entry.html">computation</a>al terms. The foundational role of <a class="thought" href="entries/computation_entry.html">computation</a> 
              can be expressed in two <a class="thought" href="entries/basic_entry.html">basic</a> theses. First, underlying the belief 
              in the possibility of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> there is a thesis 
              of <i><a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency</i>, stating that the right kind 
              of <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/structure_entry.html">structure</a> suffices for the possession of a <a class="thought" href="entries/mind_entry.html">mind</a>, 
              and for the possession of a wide variety of mental properties. Second, 
              facilitating the <a class="thought" href="entries/progress_entry.html">progress</a> of <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> more generally there 
              is a thesis of <i><a class="thought" href="entries/computation_entry.html">computation</a>al explanation</i>, stating that <a class="thought" href="entries/computation_entry.html">computation</a> 
              provides a general framework for the explanation of cognitive processes 
              and of behavior.</p>
<p>These theses are widely held within <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>, but they 
              are quite controversial. Some have questioned the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al 
              sufficiency, arguing that certain <a class="thought" href="entries/human_entry.html">human</a> abilities could never be 
              duplicated <a class="thought" href="entries/computation_entry.html">computation</a>ally (Dreyfus 1974; Penrose 1989), or that 
              even if a <a class="thought" href="entries/computation_entry.html">computation</a> could duplicate <a class="thought" href="entries/human_entry.html">human</a> abilities, instantiating 
              the relevant <a class="thought" href="entries/computation_entry.html">computation</a> would not suffice for the possession of 
              a <a class="thought" href="entries/mind_entry.html">mind</a> (Searle 1980). Others have questioned the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al 
              explanation, arguing that <a class="thought" href="entries/computation_entry.html">computation</a> provides an inappropriate 
              framework for the explanation of cognitive processes (Edelman 1989; 
              Gibson 1979), or even that <a class="thought" href="entries/computation_entry.html">computation</a>al descriptions of a <a class="thought" href="entries/system_entry.html">system</a> 
              are vacuous (Searle 1990, 1991).</p>
<p>Advocates of <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> have done their best 
              to repel these negative critiques, but the positive justification 
              for the foundational theses remains murky at best. Why should <i><a class="thought" href="entries/computation_entry.html">computation</a></i>, 
              rather than some other technical notion, play this foundational 
              role? And why should there be the intimate link between <a class="thought" href="entries/computation_entry.html">computation</a> 
              and cognition that the theses suppose? In this paper, I will develop 
              a framework that can answer these questions and justify the two 
              foundational theses.</p>
<p>In <a class="thought" href="entries/order_entry.html">order</a> for the foundation to be stable, the notion of <a class="thought" href="entries/computation_entry.html">computation</a> 
              itself has to be clarified. The mathematical theory of <a class="thought" href="entries/computation_entry.html">computation</a> 
              in the abstract is well understood, but <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> and artificial 
              <a class="thought" href="entries/intelligence_entry.html">intelligence</a> ultimately deal with physical <a class="thought" href="entries/system_entry.html">system</a>s. A bridge between 
              these <a class="thought" href="entries/system_entry.html">system</a>s and the abstract theory of <a class="thought" href="entries/computation_entry.html">computation</a> is required. 
              Specifically, we need a theory of <i><a class="thought" href="entries/implement_entry.html">implement</a>ation</i>: the relation 
              that holds between an abstract <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/object_entry.html">object</a> (a "computation" 
              for short) and a physical <a class="thought" href="entries/system_entry.html">system</a>, such that we can say that in some 
              <a class="thought" href="entries/sense_entry.html">sense</a> the <a class="thought" href="entries/system_entry.html">system</a> "realizes" the <a class="thought" href="entries/computation_entry.html">computation</a>, and that 
              the <a class="thought" href="entries/computation_entry.html">computation</a> "describes" the <a class="thought" href="entries/system_entry.html">system</a>. We cannot justify 
              the foundational role of <a class="thought" href="entries/computation_entry.html">computation</a> without first answering the 
              question: <i>What are the conditions under which a physical <a class="thought" href="entries/system_entry.html">system</a>
<a class="thought" href="entries/implement_entry.html">implement</a>s a given <a class="thought" href="entries/computation_entry.html">computation</a>?</i> Searle (1990) has argued that 
              there is no <a class="thought" href="entries/object_entry.html">object</a>ive answer to this question, and that any given 
              <a class="thought" href="entries/system_entry.html">system</a> can be seen to <a class="thought" href="entries/implement_entry.html">implement</a> any <a class="thought" href="entries/computation_entry.html">computation</a> if interpreted appropriately. 
              He argues, for <a class="thought" href="entries/instance_entry.html">instance</a>, that his wall can be seen to <a class="thought" href="entries/implement_entry.html">implement</a> 
              the Wordstar <a class="thought" href="entries/program_entry.html">program</a>. I will argue that there is no <a class="thought" href="entries/reason_entry.html">reason</a> for such 
              pessimism, and that <a class="thought" href="entries/object_entry.html">object</a>ive conditions can be straightforwardly 
              spelled out.</p>
<p>Once a theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation has been provided, we can use it 
              to answer the second key question: <i>What is the relationship between 
              <a class="thought" href="entries/computation_entry.html">computation</a> and cognition?</i> The answer to this question lies 
              in the fact that the properties of a physical cognitive <a class="thought" href="entries/system_entry.html">system</a> that 
              are relevant to its <a class="thought" href="entries/implement_entry.html">implement</a>ing certain <a class="thought" href="entries/computation_entry.html">computation</a>s, as given 
              in the answer to the first question, are precisely those properties 
              in virtue of which (a) the <a class="thought" href="entries/system_entry.html">system</a> possesses mental properties and 
              (b) the <a class="thought" href="entries/system_entry.html">system</a>'s cognitive processes can be explained.</p>
<p>The <a class="thought" href="entries/computation_entry.html">computation</a>al framework developed to answer the first question 
              can therefore be used to justify the theses of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency 
              and <a class="thought" href="entries/computation_entry.html">computation</a>al explanation. In addition, I will use this framework 
              to answer various challenges to the centrality of <a class="thought" href="entries/computation_entry.html">computation</a>, and 
              to clarify some difficult questions about <a class="thought" href="entries/computation_entry.html">computation</a> and its role 
              in <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>. In this way, we can see that the foundations 
              of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> are 
              solid.</p>
<h2>2 A Theory of <a class="thought" href="entries/implement_entry.html">Implement</a>ation</h2>
<p>The short answer to question (1) is straightforward. It goes as 
              follows: A physical <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s a given <a class="thought" href="entries/computation_entry.html">computation</a> when the 
              causal <a class="thought" href="entries/structure_entry.html">structure</a> of the physical <a class="thought" href="entries/system_entry.html">system</a> mirrors the formal <a class="thought" href="entries/structure_entry.html">structure</a> 
              of the <a class="thought" href="entries/computation_entry.html">computation</a>.</p>
<p>In a little more detail, this comes to: A physical <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s 
              a given <a class="thought" href="entries/computation_entry.html">computation</a> when there exists a grouping of physical states 
              of the <a class="thought" href="entries/system_entry.html">system</a> into state-types and a one-to-one mapping from formal 
              states of the <a class="thought" href="entries/computation_entry.html">computation</a> to physical state-types, such that formal 
              states related by an abstract state-transition relation are mapped 
              onto physical state-types related by a corresponding causal state-transition 
              relation.</p>
<p>This is still a little vague. To spell it out fully, we must specify 
              the class of <a class="thought" href="entries/computation_entry.html">computation</a>s in question. <a class="thought" href="entries/computation_entry.html">Computation</a>s are generally 
              specified relative to some formalism, and there is a wide variety 
              of formalisms: these include <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s, Pascal <a class="thought" href="entries/program_entry.html">program</a>s, cellular 
              automata, and <a class="thought" href="entries/neural_network_entry.html">neural network</a>s, among others. The story about <a class="thought" href="entries/implement_entry.html">implement</a>ation 
              is similar for each of these; only the details differ. All of these 
              can be subsumed under the class of <i>combinatorial-state automata</i> 
              (CSAs), which I will outline shortly, but for the purposes of illustration 
              I will first deal with the special case of <i>simple finite-state 
              automata</i> (FSAs).</p>
<p>An FSA is specified by giving a <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of input states <i>I_1,...,I_k</i>, 
              a <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of internal states <i>S_1,...,S_m</i>, and a <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of output 
              states <i>O_1,...,O_n</i>, along with a <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of state-transition 
              relations of the form <i>(S, I) -&gt; (S'</i>, <i>O'</i>), for each 
              pair <i>(S, I)</i> of internal states and input states, where <i>S'</i> 
              and <i>O'</i> are an internal state and an output state respectively. 
              S and I can be <a class="thought" href="entries/thought_entry.html">thought</a> of as the "old" internal state 
              and the input at a given time; S' is the "new" internal 
              state, and <i>O'</i> is the output produced at that <a class="thought" href="entries/time_entry.html">time</a>. (There 
              are some variations in the ways this can be spelled out - e.g. one 
              need not include outputs at each <a class="thought" href="entries/time_entry.html">time</a> step, and it is common to 
              designate some internal state as a "final" state - but 
              these variations are unimportant for our purposes.) The conditions 
              for the <i><a class="thought" href="entries/implement_entry.html">implement</a>ation</i> of an FSA are the following: </p>
<p>A physical <a class="thought" href="entries/system_entry.html">system</a> <i>P</i> <a class="thought" href="entries/implement_entry.html">implement</a>s an FSA <i>M</i> if there 
              is a mapping f that maps internal states of <i>P</i> to internal 
              states of <i>M</i>, inputs to <i>P</i> to input states of <i>M</i>, 
              and outputs of <i>P</i> to output states of <i>M</i>, such that: 
              for every state-transition relation <i>(S, I) -&gt; (S', O')</i> 
              of M, the following conditional holds: if P is in internal state 
              s and receiving input i where <i>f(s)=S</i> and <i>f(i)=I</i>, this 
              reliably causes it to enter internal state s' and produce output 
              o' such that <i>f(s')=S'</i> and <i>f(o')=O'.</i><sup>1</sup></p>
<p>This definition uses maximally specific physical states s rather 
              than the grouped state-types referred to above. The state-types 
              can be recovered, however: each corresponds to a <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> <i>{s | f(s) 
              = S_i}</i>, for each <i>S_i \in M</i>. From here we can see that 
              the definitions are equivalent. The causal relations between physical 
              state-types will precisely mirror the abstract relations between 
              formal states.</p>
<p>There is a lot of room to play with the details of this definition. 
              For <a class="thought" href="entries/instance_entry.html">instance</a>, it is generally useful to put restrictions on the 
              way that inputs and outputs to the <a class="thought" href="entries/system_entry.html">system</a> map onto inputs and outputs 
              of the FSA. We also need not map <i>all</i> possible internal states 
              of <i>P</i>, if some are not reachable from certain initial states. 
              These <a class="thought" href="entries/matter_entry.html">matter</a>s are unimportant here, however. What is <a class="thought" href="entries/import_entry.html">import</a>ant is 
              the overall form of the definition: in particular, the way it ensures 
              that the formal state-transitional <a class="thought" href="entries/structure_entry.html">structure</a> of the <a class="thought" href="entries/computation_entry.html">computation</a> 
              mirrors the causal state-transitional <a class="thought" href="entries/structure_entry.html">structure</a> of the physical 
              <a class="thought" href="entries/system_entry.html">system</a>. This is what all definitions of <a class="thought" href="entries/implement_entry.html">implement</a>ation, in any <a class="thought" href="entries/computation_entry.html">computation</a>al 
              formalism, will have in common.</p>
<h2>2.1 Combinatorial-state automata</h2>
<p>Simple finite-state automata are unsatisfactory for many purposes, 
              due to the monadic <a class="thought" href="entries/nature_entry.html">nature</a> of their states. The states in most <a class="thought" href="entries/computation_entry.html">computation</a>al 
              formalisms have a combinatorial <a class="thought" href="entries/structure_entry.html">structure</a>: a <a class="thought" href="entries/cell_entry.html">cell</a> <a class="thought" href="entries/pattern_entry.html">pattern</a> in a cellular 
              automaton, a combination of tape-state and head-state in a Turing 
              <a class="thought" href="entries/machine_entry.html">machine</a>, variables and registers in a Pascal <a class="thought" href="entries/program_entry.html">program</a>, and so on. 
              All this can be accommodated within the framework of combinatorial-state 
              automata (CSAs), which differ from FSAs only in that an internal 
              state is specified not by a monadic label S, but by a vector <i>[S^1, 
              S^2, S^3, ...]</i>. The <a class="thought" href="entries/element_entry.html">element</a>s of this vector can be <a class="thought" href="entries/thought_entry.html">thought</a> of 
              as the <a class="thought" href="entries/component_entry.html">component</a>s of the overall state, such as the cells in a cellular 
              automaton or the tape-squares in a <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>. There are a finite 
              <a class="thought" href="entries/number_entry.html">number</a> of possible values <i>S_j^i</i> for each <a class="thought" href="entries/element_entry.html">element</a> <i>S^i</i>, 
              where <i>S_j^i</i> is the jth possible value for the ith <a class="thought" href="entries/element_entry.html">element</a>. 
              These values can be <a class="thought" href="entries/thought_entry.html">thought</a> of as "substates". Inputs 
              and outputs can have a similar sort of complex <a class="thought" href="entries/structure_entry.html">structure</a>: an input 
              vector is <i>[I^1,...,I^k]</i>, and so on. State-transition rules 
              are determined by specifying, for each <a class="thought" href="entries/element_entry.html">element</a> of the state-vector, 
              a function by which its new state depends on the old overall state-vector 
              and input-vector, and the same for each <a class="thought" href="entries/element_entry.html">element</a> of the output-vector.</p>
<p>Input and output vectors are always finite, but the internal state 
              vectors can be either finite or infinite. The finite case is simpler, 
              and is all that is required for any practical purposes. Even if 
              we are dealing with <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s, a <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a> with a tape 
              limited to <i>10^{200}</i> squares will certainly be all that is 
              required for simulation or emulation within <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> and 
              <a class="thought" href="entries/ai_entry.html">AI</a>. The infinite case can be spelled out in an <a class="thought" href="entries/analog_entry.html">analog</a>ous fashion, 
              however. The main complication is that restrictions have to be placed 
              on the vectors and dependency rules, so that these do not encode 
              an infinite amount of <a class="thought" href="entries/information_entry.html">information</a>. This is not too difficult, but 
              I will not go into details here.</p>
<p>The conditions under which a physical <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s a CSA are 
              <a class="thought" href="entries/analog_entry.html">analog</a>ous to those for an FSA. The main difference is that internal 
              states of the <a class="thought" href="entries/system_entry.html">system</a> need to be specified as vectors, where each 
              <a class="thought" href="entries/element_entry.html">element</a> of the vector corresponds to an independent <a class="thought" href="entries/element_entry.html">element</a> of the 
              physical <a class="thought" href="entries/system_entry.html">system</a>. A natural requirement for such a "vectorization" 
              is that each <a class="thought" href="entries/element_entry.html">element</a> correspond to a distinct physical region within 
              the <a class="thought" href="entries/system_entry.html">system</a>, although there may be other alternatives. The same goes 
              for the complex <a class="thought" href="entries/structure_entry.html">structure</a> of inputs and outputs. The <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s 
              a given CSA if there exists such a vectorization of states of the 
              <a class="thought" href="entries/system_entry.html">system</a>, and a mapping from <a class="thought" href="entries/element_entry.html">element</a>s of those vectors onto corresponding 
              <a class="thought" href="entries/element_entry.html">element</a>s of the vectors of the CSA, such that the state-transition 
              relations are isomorphic in the obvious way. The details can be 
              filled in straightforwardly, as follows:</p>
<p>A physical <a class="thought" href="entries/system_entry.html">system</a> P <a class="thought" href="entries/implement_entry.html">implement</a>s a CSA <i>M</i> if there is a vectorization 
              of internal states of <i>P</i> into <a class="thought" href="entries/component_entry.html">component</a>s <i>[s^1,s^2,...]</i>, 
              and a mapping f from the substates s^j into corresponding substates 
              S^j of M, along with similar vectorizations and mappings for inputs 
              and outputs, such that for every state-transition rule <i>([I^1,...,I^k],[S^1,S^2,...]) 
              -&gt; ([S'^1,S'^2,...],[O^1,...,O^l])</i> of <i>M</i>: if <i>P</i> 
              is in internal state <i>[s^1,s^2,...]</i> and receiving input <i>[i^1,...,i^n]</i> 
              which map to formal state and input <i>[S^1,S^2,...]</i> and <i>[I^1,...,I^k]</i> 
              respectively, this reliably causes it to enter an internal state 
              and produce an output that map to <i>[S'^1,S'^2,...]</i> and <i>[O^1,...,O^l]</i> 
              respectively.</p>
<p>Once again, further <a class="thought" href="entries/constraint_entry.html">constraint</a>s might be added to this definition 
              for various purposes, and there is much that can be said to flesh 
              out the definition's various parts; a detailed discussion of these 
              technicalities must await another forum (see Chalmers 1996a for 
              a start). This definition is not the last word in a theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation, 
              but it captures the theory's <a class="thought" href="entries/basic_entry.html">basic</a> form.</p>
<p>One might think that CSAs are not much of an advance on FSAs. Finite 
              CSAs, at least, are no more <a class="thought" href="entries/computation_entry.html">computation</a>ally powerful than FSAs; 
              there is a natural correspondence that associates every finite CSA 
              with an FSA with the same input/output behavior. Of course infinite 
              CSAs (such as <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s) are more powerful, but even leaving 
              that <a class="thought" href="entries/reason_entry.html">reason</a> aside, there are a <a class="thought" href="entries/number_entry.html">number</a> of <a class="thought" href="entries/reason_entry.html">reason</a>s why CSAs are a 
              more suitable formalism for our purposes than FSAs.</p>
<p>First, the <i><a class="thought" href="entries/implement_entry.html">implement</a>ation</i> conditions on a CSA are much more 
              constrained than those of the corresponding FSA. An <a class="thought" href="entries/implement_entry.html">implement</a>ation 
              of a CSA is required to consist in a complex causal interaction 
              among a <a class="thought" href="entries/number_entry.html">number</a> of separate parts; a CSA description can therefore 
              capture the causal organization of a <a class="thought" href="entries/system_entry.html">system</a> to a much finer grain. 
              Second, the <a class="thought" href="entries/structure_entry.html">structure</a> in CSA states can be of great <i>explanatory</i>
<a class="thought" href="entries/utility_entry.html">utility</a>. A description of a physical <a class="thought" href="entries/system_entry.html">system</a> as a CSA will often 
              be much more illuminating than a description as the corresponding 
              FSA.<sup>2</sup> Third, CSAs reflect in a much more direct way the 
              formal organization of such familiar <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/object_entry.html">object</a>s as Turing 
              <a class="thought" href="entries/machine_entry.html">machine</a>s, <a class="thought" href="entries/cellular_automata_entry.html">cellular automata</a>, and the like. Finally, the CSA framework 
              allows a unified account of the <a class="thought" href="entries/implement_entry.html">implement</a>ation conditions for both 
              finite and infinite <a class="thought" href="entries/machine_entry.html">machine</a>s.</p>
<p>This definition can straightforwardly be applied to yield <a class="thought" href="entries/implement_entry.html">implement</a>ation 
              conditions for more specific <a class="thought" href="entries/computation_entry.html">computation</a>al formalisms. To develop 
              an account of the <a class="thought" href="entries/implement_entry.html">implement</a>ation-conditions for a <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>, 
              say, we need only redescribe the <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a> as a CSA. The overall 
              state of a <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a> can be seen as a giant vector, consisting 
              of (a) the internal state of the head, and (b) the state of each 
              square of the tape, where this state in turn is an ordered pair 
              of a <a class="thought" href="entries/symbol_entry.html">symbol</a> and a flag indicating whether the square is occupied 
              by the head (of course only one square can be so occupied; this 
              will be ensured by restrictions on initial state and on state-transition 
              rules). The state-transition rules between vectors can be derived 
              naturally from the quintuples specifying the behavior of the <a class="thought" href="entries/machine_entry.html">machine</a>-head. 
              As usually understood, <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s only take inputs at a single 
              <a class="thought" href="entries/time_entry.html">time</a>-step (the start), and do not produce any output separate from 
              the <a class="thought" href="entries/content_entry.html">content</a>s of the tape. These restrictions can be overridden in 
              natural ways, for example by adding separate input and output tapes, 
              but even with inputs and outputs limited in this way there is a 
              natural description as a CSA. Given this translation from the Turing 
              <a class="thought" href="entries/machine_entry.html">machine</a> formalism to the CSA formalism, we can say that a given 
              <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a> is <a class="thought" href="entries/implement_entry.html">implement</a>ed whenever the corresponding CSA is 
              <a class="thought" href="entries/implement_entry.html">implement</a>ed.</p>
<p>A similar story holds for <a class="thought" href="entries/computation_entry.html">computation</a>s in other formalisms. Some 
              formalisms, such as <a class="thought" href="entries/cellular_automata_entry.html">cellular automata</a>, are even more straightforward. 
              Others, such as Pascal <a class="thought" href="entries/program_entry.html">program</a>s, are more complex, but the overall 
              principles are the same. In each case there is some room for maneuver, 
              and perhaps some arbitrary decisions to make (does writing a <a class="thought" href="entries/symbol_entry.html">symbol</a> 
              and moving the head count as two state-transitions or one?) but 
              little rests on the decisions we make. We can also give accounts 
              of <a class="thought" href="entries/implement_entry.html">implement</a>ation for non-deterministic and probabilistic automata, 
              by making simple changes in the definition of a CSA and the corresponding 
              account of <a class="thought" href="entries/implement_entry.html">implement</a>ation. The theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation for combinatorial-state 
              automata provides a basis for the theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation in general.</p>
<h2>2.2 Questions answered</h2>
<p>The above account may look complex, but the essential idea is very 
              simple: the relation between an <a class="thought" href="entries/implement_entry.html">implement</a>ed <a class="thought" href="entries/computation_entry.html">computation</a> and an <a class="thought" href="entries/implement_entry.html">implement</a>ing 
              <a class="thought" href="entries/system_entry.html">system</a> is one of isomorphism between the formal <a class="thought" href="entries/structure_entry.html">structure</a> of the 
              former and the causal <a class="thought" href="entries/structure_entry.html">structure</a> of the latter. In this way, we can 
              see that as far as the theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation is concerned, a 
              <a class="thought" href="entries/computation_entry.html">computation</a> is simply an <i>abstract specification of causal organization</i>. 
              This is <a class="thought" href="entries/import_entry.html">import</a>ant for later purposes. In the meantime, we can now 
              answer various questions and <a class="thought" href="entries/object_entry.html">object</a>ions.</p>
<p><b>Does every <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a> some <a class="thought" href="entries/computation_entry.html">computation</a>?</b> Yes. For example, 
              every physical <a class="thought" href="entries/system_entry.html">system</a> will <a class="thought" href="entries/implement_entry.html">implement</a> the simple FSA with a single 
              internal state; most physical <a class="thought" href="entries/system_entry.html">system</a>s will <a class="thought" href="entries/implement_entry.html">implement</a> the 2-state 
              cyclic FSA, and so on. This is no problem, and certainly does not 
              render the account vacuous. That would only be the case if every 
              <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>ed <i>every</i> <a class="thought" href="entries/computation_entry.html">computation</a>, and that is not the 
              case.</p>
<p><b>Does every <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a> any given <a class="thought" href="entries/computation_entry.html">computation</a>?</b> No. The 
              conditions for <a class="thought" href="entries/implement_entry.html">implement</a>ing a given complex <a class="thought" href="entries/computation_entry.html">computation</a> - say, a 
              CSA whose state-vectors have 1000 <a class="thought" href="entries/element_entry.html">element</a>s, with 10 possibilities 
              for each <a class="thought" href="entries/element_entry.html">element</a> and complex state-transition relations - will generally 
              be sufficiently rigorous that extremely few physical <a class="thought" href="entries/system_entry.html">system</a>s will 
              meet them. What is required is not just a mapping from states of 
              the <a class="thought" href="entries/system_entry.html">system</a> onto states of the CSA, as Searle (1990) effectively 
              suggests. The added requirement that the mapped states must satisfy 
              reliable state-transition rules is what does all the work. In this 
              case, there will effectively be at least <i>10^{1000}</i> <a class="thought" href="entries/constraint_entry.html">constraint</a>s 
              on state-transitions (one for each possible state-vector, and more 
              if there are multiple possible inputs). Each <a class="thought" href="entries/constraint_entry.html">constraint</a> will specify 
              one out of at least <i>10^{1000}</i> possible consequents (one for 
              each possible resultant state-vector, and more if there are outputs). 
              The chance that an arbitrary <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of states will satisfy these <a class="thought" href="entries/constraint_entry.html">constraint</a>s 
              is something less than one in <i>(10^{1000})^{10^{1000}}</i> (actually 
              significantly less, because of the requirement that transitions 
              be reliable). There is no <a class="thought" href="entries/reason_entry.html">reason</a> to suppose that the causal <a class="thought" href="entries/structure_entry.html">structure</a> 
              of an arbitrary <a class="thought" href="entries/system_entry.html">system</a> (such as Searle's wall) will satisfy these 
              <a class="thought" href="entries/constraint_entry.html">constraint</a>s. It is true that while we lack <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the fundamental 
              constituents of <a class="thought" href="entries/matter_entry.html">matter</a>, it is impossible to <i>prove</i> that arbitrary 
              <a class="thought" href="entries/object_entry.html">object</a>s do not <a class="thought" href="entries/implement_entry.html">implement</a> every <a class="thought" href="entries/computation_entry.html">computation</a> (perhaps every <a class="thought" href="entries/proton_entry.html">proton</a> 
              has an infinitely rich internal <a class="thought" href="entries/structure_entry.html">structure</a>), but anybody who denies 
              this conclusion will need to come up with a remarkably strong argument.</p>
<p><b>Can a given <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a> more than one <a class="thought" href="entries/computation_entry.html">computation</a>? </b>Yes. 
              Any <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>ing some complex <a class="thought" href="entries/computation_entry.html">computation</a> will simultaneously 
              be <a class="thought" href="entries/implement_entry.html">implement</a>ing many simpler <a class="thought" href="entries/computation_entry.html">computation</a>s - not just 1-state and 
              2-state FSAs, but <a class="thought" href="entries/computation_entry.html">computation</a>s of some <a class="thought" href="entries/complexity_entry.html">complexity</a>. This is no flaw 
              in the current account; it is precisely what we should expect. The 
              <a class="thought" href="entries/system_entry.html">system</a> on my desk is currently <a class="thought" href="entries/implement_entry.html">implement</a>ing all kinds of <a class="thought" href="entries/computation_entry.html">computation</a>s, 
              from EMACS to a clock <a class="thought" href="entries/program_entry.html">program</a>, and various sub-<a class="thought" href="entries/computation_entry.html">computation</a>s of these. 
              In general, there is no canonical mapping from a physical <a class="thought" href="entries/object_entry.html">object</a> 
              to "the" <a class="thought" href="entries/computation_entry.html">computation</a> it is performing. We might say that 
              within every physical <a class="thought" href="entries/system_entry.html">system</a>, there are numerous <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/system_entry.html">system</a>s. 
              To this very limited extent, the notion of <a class="thought" href="entries/implement_entry.html">implement</a>ation is "interest-relative". 
              Once again, however, there is no threat of vacuity. The question 
              of whether a given <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s a given <a class="thought" href="entries/computation_entry.html">computation</a> is still 
              entirely <a class="thought" href="entries/object_entry.html">object</a>ive. What counts is that a given <a class="thought" href="entries/system_entry.html">system</a> does not 
              <a class="thought" href="entries/implement_entry.html">implement</a> <i>every</i> <a class="thought" href="entries/computation_entry.html">computation</a>, or to put the point differently, 
              that most given <a class="thought" href="entries/computation_entry.html">computation</a>s are only <a class="thought" href="entries/implement_entry.html">implement</a>ed by a very limited 
              class of physical <a class="thought" href="entries/system_entry.html">system</a>s. This is what is required for a substantial 
              foundation for <a class="thought" href="entries/ai_entry.html">AI</a> and <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>, and it is what the account 
              I have given provides.</p>
<p><b>If even digestion is a <a class="thought" href="entries/computation_entry.html">computation</a>, isn't this vacuous?</b> 
              This <a class="thought" href="entries/object_entry.html">object</a>ion expresses the feeling that if every process, including 
              such things as digestion and oxidation, <a class="thought" href="entries/implement_entry.html">implement</a>s some <a class="thought" href="entries/computation_entry.html">computation</a>, 
              then there seems to be nothing special about cognition any more, 
              as <a class="thought" href="entries/computation_entry.html">computation</a> is so pervasive. This <a class="thought" href="entries/object_entry.html">object</a>ion rests on a misunderstanding. 
              It is true that any given <i><a class="thought" href="entries/instance_entry.html">instance</a></i> of digestion will <a class="thought" href="entries/implement_entry.html">implement</a> 
              some <a class="thought" href="entries/computation_entry.html">computation</a>, as any physical <a class="thought" href="entries/system_entry.html">system</a> does, but the <a class="thought" href="entries/system_entry.html">system</a>'s 
              <a class="thought" href="entries/implement_entry.html">implement</a>ing this <a class="thought" href="entries/computation_entry.html">computation</a> is in general irrelevant to its being 
              an <a class="thought" href="entries/instance_entry.html">instance</a> of digestion. To see this, we can note that the same 
              <a class="thought" href="entries/computation_entry.html">computation</a> could have been <a class="thought" href="entries/implement_entry.html">implement</a>ed by various other physical 
              <a class="thought" href="entries/system_entry.html">system</a>s (such as my SPARC) without its being an <a class="thought" href="entries/instance_entry.html">instance</a> of digestion. 
              Therefore the fact that the <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s the <a class="thought" href="entries/computation_entry.html">computation</a> is 
              not responsible for the <a class="thought" href="entries/existence_entry.html">existence</a> of digestion in the <a class="thought" href="entries/system_entry.html">system</a>.</p>
<p>With cognition, by contrast, the claim is that it is <i>in virtue</i> 
              of <a class="thought" href="entries/implement_entry.html">implement</a>ing some <a class="thought" href="entries/computation_entry.html">computation</a> that a <a class="thought" href="entries/system_entry.html">system</a> is cognitive. That 
              is, there is a certain class of <a class="thought" href="entries/computation_entry.html">computation</a>s such that <i>any</i>
<a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>ing that <a class="thought" href="entries/computation_entry.html">computation</a> is cognitive. We might go further 
              and argue that every cognitive <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s some <a class="thought" href="entries/computation_entry.html">computation</a> 
              such that any <a class="thought" href="entries/implement_entry.html">implement</a>ation of the <a class="thought" href="entries/computation_entry.html">computation</a> would also be cognitive, 
              and would share numerous specific mental properties with the original 
              <a class="thought" href="entries/system_entry.html">system</a>. These claims are controversial, of course, and I will be 
              arguing for them in the next section. But note that it is precisely 
              this relation between <a class="thought" href="entries/computation_entry.html">computation</a> and cognition that gives bite 
              to the <a class="thought" href="entries/computation_entry.html">computation</a>al analysis of cognition. If this relation or 
              something like it did not hold, the <a class="thought" href="entries/computation_entry.html">computation</a>al status of cognition 
              would be <a class="thought" href="entries/analog_entry.html">analog</a>ous to that of digestion.</p>
<p><b>What about Putnam's argument?</b> Putnam (1988) has suggested 
              that on a definition like this, almost any physical <a class="thought" href="entries/system_entry.html">system</a> can be 
              seen to <a class="thought" href="entries/implement_entry.html">implement</a> every finite-state automaton. He argues for this 
              conclusion by demonstrating that there will almost always be a mapping 
              from physical states of a <a class="thought" href="entries/system_entry.html">system</a> to internal states of an FSA, such 
              that over a given <a class="thought" href="entries/time_entry.html">time</a>-period (from 12:00 to 12:10 today, say) the 
              transitions between states are just as the <a class="thought" href="entries/machine_entry.html">machine</a> table say they 
              should be. If the <a class="thought" href="entries/machine_entry.html">machine</a> table requires that state <i>A</i> be 
              followed by state <i>B</i>, then every <a class="thought" href="entries/instance_entry.html">instance</a> of state <i>A</i> 
              is followed by state B in this <a class="thought" href="entries/time_entry.html">time</a> period. Such a mapping will 
              be possible for an inputless FSA under the assumption that physical 
              states do not repeat. We simply map the initial physical state of 
              the <a class="thought" href="entries/system_entry.html">system</a> onto an initial formal state of the <a class="thought" href="entries/computation_entry.html">computation</a>, and 
              map successive states of the <a class="thought" href="entries/system_entry.html">system</a> onto successive states of the 
              <a class="thought" href="entries/computation_entry.html">computation</a>.</p>
<p>However, to suppose that this <a class="thought" href="entries/system_entry.html">system</a> <a class="thought" href="entries/implement_entry.html">implement</a>s the FSA in question 
              is to misconstrue the state-transition conditionals in the definition 
              of <a class="thought" href="entries/implement_entry.html">implement</a>ation. What is required is not simply that state <i>A</i> 
              be followed by state B on all <a class="thought" href="entries/instance_entry.html">instance</a>s in which it happens to come 
              up in a given <a class="thought" href="entries/time_entry.html">time</a>-period. There must be a reliable, counterfactual-supporting 
              connection between the states. Given a formal state-transition <i>A 
              -&gt; B</i>, it must be the case that <i>if</i> the <a class="thought" href="entries/system_entry.html">system</a> were 
              to be in state <i>A</i>, it would transit to state <i>B</i>. Further, 
              such a conditional must be satisfied for <i>every</i> transition 
              in the <a class="thought" href="entries/machine_entry.html">machine</a> table, not just for those whose antecedent states 
              happen to come up in a given <a class="thought" href="entries/time_entry.html">time</a> period. It is easy to see that 
              Putnam's <a class="thought" href="entries/system_entry.html">system</a> does not satisfy this much stronger requirement. 
              In effect, Putnam has required only that certain weak material conditionals 
              be satisfied, rather than conditionals with modal force. For this 
              <a class="thought" href="entries/reason_entry.html">reason</a>, his purported <a class="thought" href="entries/implement_entry.html">implement</a>ations are not <a class="thought" href="entries/implement_entry.html">implement</a>ations at 
              all.</p>
<p>(Two notes. First, Putnam responds briefly to the charge that his 
              <a class="thought" href="entries/system_entry.html">system</a> fails to support counterfactuals, but considers a different 
              class of counterfactuals - those of the form "if the <a class="thought" href="entries/system_entry.html">system</a> 
              had not been in state <i>A</i>, it would not have transited to state 
              <i>B</i>". It is not these counterfactuals that are relevant 
              here. Second, it turns out that Putnam's argument for the widespread 
              realization of inputless FSAs can be patched up in a certain way; 
              this just goes to show that inputless FSAs are an inappropriate 
              formalism for <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>, due to their complete lack of combinatorial 
              <a class="thought" href="entries/structure_entry.html">structure</a>. Putnam gives a related argument for the widespread realization 
              of FSAs with input and output, but this argument is strongly vulnerable 
              to an <a class="thought" href="entries/object_entry.html">object</a>ion like the one above, and cannot be patched up in 
              an <a class="thought" href="entries/analog_entry.html">analog</a>ous way. CSAs are even less vulnerable to this sort of 
              argument. I discuss all this at much greater length in Chalmers 
              1996a.)</p>
<p><b>What about <a class="thought" href="entries/semantics_entry.html">semantics</a>?</b> It will be noted that nothing in my 
              account of <a class="thought" href="entries/computation_entry.html">computation</a> and <a class="thought" href="entries/implement_entry.html">implement</a>ation invokes any semantic considerations, 
              such as the representational <a class="thought" href="entries/content_entry.html">content</a> of internal states. This is 
              precisely as it should be: <a class="thought" href="entries/computation_entry.html">computation</a>s are specified syntactically, 
              not semantically. Although it may very well be the case that any 
              <a class="thought" href="entries/implement_entry.html">implement</a>ations of a given <a class="thought" href="entries/computation_entry.html">computation</a> share some kind of semantic 
              <a class="thought" href="entries/content_entry.html">content</a>, this should be a <i>consequence</i> of an account of <a class="thought" href="entries/computation_entry.html">computation</a> 
              and <a class="thought" href="entries/implement_entry.html">implement</a>ation, rather than built into the definition. If we 
              build semantic considerations into the conditions for <a class="thought" href="entries/implement_entry.html">implement</a>ation, 
              any role that <a class="thought" href="entries/computation_entry.html">computation</a> can play in providing a foundation for 
              <a class="thought" href="entries/ai_entry.html">AI</a> and <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> will be endangered, as the notion of semantic 
              <a class="thought" href="entries/content_entry.html">content</a> is so ill-understood that it desperately needs a foundation 
              itself.</p>
<p>The original account of <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s by Turing (1936) certainly 
              had no semantic <a class="thought" href="entries/constraint_entry.html">constraint</a>s built in. A <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a> is defined 
              purely in terms of the mechanisms involved, that is, in terms of 
              syntactic <a class="thought" href="entries/pattern_entry.html">pattern</a>s and the way they are transformed. To <a class="thought" href="entries/implement_entry.html">implement</a> 
              a <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>, we need only ensure that this formal <a class="thought" href="entries/structure_entry.html">structure</a> 
              is reflected in the causal <a class="thought" href="entries/structure_entry.html">structure</a> of the <a class="thought" href="entries/implement_entry.html">implement</a>ation. Some 
              <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s will certainly support a <a class="thought" href="entries/system_entry.html">system</a>atic semantic interpretation, 
              in which case their <a class="thought" href="entries/implement_entry.html">implement</a>ations will also, but this plays no 
              part in the definition of what it is to be or to <a class="thought" href="entries/implement_entry.html">implement</a> a Turing 
              <a class="thought" href="entries/machine_entry.html">machine</a>. This is made particularly clear if we note that there are 
              some <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s, such as <a class="thought" href="entries/machine_entry.html">machine</a>s defined by random sets of 
              state-transition quintuples, that support no non-trivial semantic 
              interpretation. We need an account of what it is to <a class="thought" href="entries/implement_entry.html">implement</a> these 
              <a class="thought" href="entries/machine_entry.html">machine</a>s, and such an account will then generalize to <a class="thought" href="entries/machine_entry.html">machine</a>s that 
              support a semantic interpretation. Certainly, when <a class="thought" href="entries/computer_entry.html">computer</a> designers 
              ensure that their <a class="thought" href="entries/machine_entry.html">machine</a>s <a class="thought" href="entries/implement_entry.html">implement</a> the <a class="thought" href="entries/program_entry.html">program</a>s that they are 
              supposed to, they do this by ensuring that the mechanisms have the 
              right causal organization; they are not concerned with semantic 
              <a class="thought" href="entries/content_entry.html">content</a>. In the words of Haugeland (1985), if you take care of the 
              <a class="thought" href="entries/syntax_entry.html">syntax</a>, the <a class="thought" href="entries/semantics_entry.html">semantics</a> will take care of itself.</p>
<p>I have said that the notion of <a class="thought" href="entries/computation_entry.html">computation</a> should not be dependent 
              on that of semantic <a class="thought" href="entries/content_entry.html">content</a>; neither do I think that the latter 
              notion should be dependent on the former. Rather, both <a class="thought" href="entries/computation_entry.html">computation</a> 
              and <a class="thought" href="entries/content_entry.html">content</a> should be dependent on the common notion of <i>causation</i>. 
              We have seen the first dependence in the account of <a class="thought" href="entries/computation_entry.html">computation</a> 
              above. The notion of <a class="thought" href="entries/content_entry.html">content</a> has also been frequently analyzed in 
              terms of causation (see e.g. Dretske 1981 and Fodor 1987). This 
              common pillar in the analyses of both <a class="thought" href="entries/computation_entry.html">computation</a> and <a class="thought" href="entries/content_entry.html">content</a> allows 
              that the two notions will not sway independently, while at the same 
              <a class="thought" href="entries/time_entry.html">time</a> ensuring that neither is dependent on the other for its analysis.</p>
<p><b>What about <a class="thought" href="entries/computer_entry.html">computer</a>s?</b> Although Searle (1990) talks about 
              what it takes for something to be a "digital <a class="thought" href="entries/computer_entry.html">computer</a>", 
              I have talked only about <a class="thought" href="entries/computation_entry.html">computation</a>s and eschewed reference to 
              <a class="thought" href="entries/computer_entry.html">computer</a>s. This is deliberate, as it seems to me that <a class="thought" href="entries/computation_entry.html">computation</a> 
              is the more fundamental notion, and certainly the one that is <a class="thought" href="entries/import_entry.html">import</a>ant 
              for <a class="thought" href="entries/ai_entry.html">AI</a> and <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>. <a class="thought" href="entries/ai_entry.html">AI</a> and <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> certainly 
              do not require that cognitive <a class="thought" href="entries/system_entry.html">system</a>s be <a class="thought" href="entries/computer_entry.html">computer</a>s, unless we stipulate 
              that all it takes to be a <a class="thought" href="entries/computer_entry.html">computer</a> is to <a class="thought" href="entries/implement_entry.html">implement</a> some <a class="thought" href="entries/computation_entry.html">computation</a>, 
              in which case the definition is vacuous.</p>
<p>What does it take for something to be a <a class="thought" href="entries/computer_entry.html">computer</a>? Presumably, a 
              <a class="thought" href="entries/computer_entry.html">computer</a> cannot merely <a class="thought" href="entries/implement_entry.html">implement</a> a single <a class="thought" href="entries/computation_entry.html">computation</a>. It must be 
              capable of <a class="thought" href="entries/implement_entry.html">implement</a>ing many <a class="thought" href="entries/computation_entry.html">computation</a>s -- that is, it must be 
              <i><a class="thought" href="entries/program_entry.html">program</a>mable</i>. In the extreme case, a <a class="thought" href="entries/computer_entry.html">computer</a> will be universal, 
              capable of being <a class="thought" href="entries/program_entry.html">program</a>med to compute any recursively enumerable 
              function. Perhaps universality is not required of a <a class="thought" href="entries/computer_entry.html">computer</a>, but 
              <a class="thought" href="entries/program_entry.html">program</a>mability certainly is. To bring <a class="thought" href="entries/computer_entry.html">computer</a>s within the scope 
              of the theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation above, we could require that a <a class="thought" href="entries/computer_entry.html">computer</a> 
              be a CSA with certain <a class="thought" href="entries/parameter_entry.html">parameter</a>s, such that depending on how these 
              <a class="thought" href="entries/parameter_entry.html">parameter</a>s are <a class="thought" href="entries/single_electron_transfer_entry.html">set</a>, a <a class="thought" href="entries/number_entry.html">number</a> of different CSAs can be <a class="thought" href="entries/implement_entry.html">implement</a>ed. 
              A universal <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a> could be seen in this <a class="thought" href="entries/light_entry.html">light</a>, for <a class="thought" href="entries/instance_entry.html">instance</a>, 
              where the <a class="thought" href="entries/parameter_entry.html">parameter</a>s correspond to the "program" <a class="thought" href="entries/symbol_entry.html">symbol</a>s 
              on the tape. In any case, such a theory of <a class="thought" href="entries/computer_entry.html">computer</a>s is not required 
              for the study of cognition.</p>
<p>Is the <a class="thought" href="entries/brain_entry.html">brain</a> a <a class="thought" href="entries/computer_entry.html">computer</a> in this sense? Arguably. For a start, the 
              <a class="thought" href="entries/brain_entry.html">brain</a> can be "programmed" to <a class="thought" href="entries/implement_entry.html">implement</a> various <a class="thought" href="entries/computation_entry.html">computation</a>s 
              by the laborious means of conscious serial rule-following; but this 
              is a fairly incidental ability. On a different level, it might be 
              argued that <a class="thought" href="entries/learning_entry.html">learning</a> provides a certain kind of <a class="thought" href="entries/program_entry.html">program</a>mability 
              and <a class="thought" href="entries/parameter_entry.html">parameter</a>-setting, but this is a sufficiently indirect kind 
              of <a class="thought" href="entries/parameter_entry.html">parameter</a>-setting that it might be argued that it does not qualify. 
              In any case, the question is quite unimportant for our purposes. 
              What counts is that the <a class="thought" href="entries/brain_entry.html">brain</a> <a class="thought" href="entries/implement_entry.html">implement</a>s various complex <a class="thought" href="entries/computation_entry.html">computation</a>s, 
              not that it is a <a class="thought" href="entries/computer_entry.html">computer</a>.</p>
<h2>3 <a class="thought" href="entries/computation_entry.html">Computation</a> and cognition</h2>
<p>The above is only half the story. We now need to exploit the above 
              account of <a class="thought" href="entries/computation_entry.html">computation</a> and <a class="thought" href="entries/implement_entry.html">implement</a>ation to outline the relation 
              between <a class="thought" href="entries/computation_entry.html">computation</a> and cognition, and to justify the foundational 
              role of <a class="thought" href="entries/computation_entry.html">computation</a> in <a class="thought" href="entries/ai_entry.html">AI</a> and <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>.</p>
<p>Justification of the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency has usually 
              been tenuous. Perhaps the most common move has been an appeal to 
              the <a class="thought" href="entries/turing_test_entry.html">Turing test</a>, noting that every <a class="thought" href="entries/implement_entry.html">implement</a>ation of a given <a class="thought" href="entries/computation_entry.html">computation</a> 
              will have a certain kind of behavior, and claiming that the right 
              kind of behavior is sufficient for mentality. The <a class="thought" href="entries/turing_test_entry.html">Turing test</a> is 
              a weak foundation, however, and one to which <a class="thought" href="entries/ai_entry.html">AI</a> need not appeal. 
              It may be that any behavioral description can be <a class="thought" href="entries/implement_entry.html">implement</a>ed by 
              <a class="thought" href="entries/system_entry.html">system</a>s lacking mentality altogether (such as the giant lookup tables 
              of Block 1981). Even if behavior suffices for <i><a class="thought" href="entries/mind_entry.html">mind</a></i>, the demise 
              of logical behaviorism has made it very implausible that it suffices 
              for specific mental properties: two mentally distinct <a class="thought" href="entries/system_entry.html">system</a>s can 
              have the same behavioral dispositions. A <a class="thought" href="entries/computation_entry.html">computation</a>al basis for 
              cognition will require a tighter link than this, then.</p>
<p>Instead, the central property of <a class="thought" href="entries/computation_entry.html">computation</a> on which I will focus 
              is one that we have already noted: the fact that a <a class="thought" href="entries/computation_entry.html">computation</a> provides 
              an abstract specification of the causal organization of a <a class="thought" href="entries/system_entry.html">system</a>. 
              Causal organization is the nexus between <a class="thought" href="entries/computation_entry.html">computation</a> and cognition. 
              If cognitive <a class="thought" href="entries/system_entry.html">system</a>s have their mental properties in virtue of their 
              causal organization, and if that causal organization can be specified 
              <a class="thought" href="entries/computation_entry.html">computation</a>ally, then the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency is 
              established. Similarly, if it is the causal organization of a <a class="thought" href="entries/system_entry.html">system</a> 
              that is primarily relevant in the explanation of behavior, then 
              the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation will be established. By 
              the account above, we will always be able to provide a <a class="thought" href="entries/computation_entry.html">computation</a>al 
              specification of the relevant causal organization, and therefore 
              of the properties on which cognition rests.</p>
<h2>3.1 Organizational invariance</h2>
<p>To spell out this story in more detail, I will introduce the notion 
              of the <i>causal topology</i> of a <a class="thought" href="entries/system_entry.html">system</a>. The causal topology represents 
              the abstract causal organization of the <a class="thought" href="entries/system_entry.html">system</a>: that is, the <a class="thought" href="entries/pattern_entry.html">pattern</a> 
              of interaction among parts of the <a class="thought" href="entries/system_entry.html">system</a>, abstracted away from the 
              make-up of <a class="thought" href="entries/individual_entry.html">individual</a> parts and from the way the causal connections 
              are <a class="thought" href="entries/implement_entry.html">implement</a>ed. Causal topology can be <a class="thought" href="entries/thought_entry.html">thought</a> of as a dynamic 
              topology <a class="thought" href="entries/analog_entry.html">analog</a>ous to the static topology of a graph or a <a class="thought" href="entries/network_entry.html">network</a>. 
              Any <a class="thought" href="entries/system_entry.html">system</a> will have causal topology at a <a class="thought" href="entries/number_entry.html">number</a> of different levels. 
              For the cognitive <a class="thought" href="entries/system_entry.html">system</a>s with which we will be concerned, the relevant 
              level of causal topology will be a level fine enough to determine 
              the causation of behavior. For the <a class="thought" href="entries/brain_entry.html">brain</a>, this is probably the neural 
              level or higher, depending on just how the <a class="thought" href="entries/brain_entry.html">brain</a>'s cognitive mechanisms 
              function. (The notion of causal topology is necessarily informal 
              for now; I will discuss its formalization below.)</p>
<p>Call a property <i>P</i> an <i>organizational</i> <i>invariant</i> 
              if it is invariant with respect to causal topology: that is, if 
              any change to the <a class="thought" href="entries/system_entry.html">system</a> that preserves the causal topology preserves 
              <i>P</i>. The sort of changes in question include: (a) moving the 
              <a class="thought" href="entries/system_entry.html">system</a> in space; (b) stretching, distorting, expanding and contracting 
              the <a class="thought" href="entries/system_entry.html">system</a>; (<a class="thought" href="entries/c_entry.html">c</a>) replacing sufficiently small parts of the <a class="thought" href="entries/system_entry.html">system</a> 
              with parts that perform the same local function (e.g. replacing 
              a <a class="thought" href="entries/neuron_entry.html">neuron</a> with a <a class="thought" href="entries/silicon_entry.html">silicon</a> <a class="thought" href="entries/chip_entry.html">chip</a> with the same I/O properties); (d) 
              replacing the causal links between parts of a <a class="thought" href="entries/system_entry.html">system</a> with other 
              links that preserve the same <a class="thought" href="entries/pattern_entry.html">pattern</a> of dependencies (e.g., we might 
              replace a mechanical link in a telephone exchange with an electrical 
              link); and (e) any other changes that do not alter the <a class="thought" href="entries/pattern_entry.html">pattern</a> of 
              causal interaction among parts of the <a class="thought" href="entries/system_entry.html">system</a>.</p>
<p>Most properties are not organizational invariants. The property 
              of flying is not, for <a class="thought" href="entries/instance_entry.html">instance</a>: we can move an airplane to the ground 
              while preserving its causal topology, and it will no longer be flying. 
              Digestion is not: if we gradually replace the parts involved in 
              digestion with pieces of metal, while preserving causal <a class="thought" href="entries/pattern_entry.html">pattern</a>s, 
              after a while it will no longer be an <a class="thought" href="entries/instance_entry.html">instance</a> of digestion: no 
              food groups will be broken down, no <a class="thought" href="entries/energy_entry.html">energy</a> will be extracted, and 
              so on. The property of being tube of toothpaste is not an organizational 
              invariant: if we deform the tube into a sphere, or replace the toothpaste 
              by peanut butter while preserving causal topology, we no longer 
              have a tube of toothpaste.</p>
<p>In general, most properties depend essentially on certain features 
              that are not features of causal topology. Flying depends on height, 
              digestion depends on a particular physiochemical makeup, tubes of 
              toothpaste depend on shape and physiochemical makeup, and so on. 
              Change the features in question enough and the property in question 
              will change, even though causal topology might be preserved throughout.</p>
<h2>3.2 The organizational invariance of mental properties</h2>
<p>The central claim of this section is that most mental properties 
              are organizational invariants. It does not <a class="thought" href="entries/matter_entry.html">matter</a> how we stretch, 
              move about, or replace small parts of a cognitive <a class="thought" href="entries/system_entry.html">system</a>: as long 
              as we preserve its causal topology, we will preserve its mental 
              properties.</p>
<p>An exception has to be made for properties that are partly supervenient 
              on states of the environment. Such properties include <a class="thought" href="entries/knowledge_entry.html">knowledge</a> 
              (if we move a <a class="thought" href="entries/system_entry.html">system</a> that knows that <i>P</i> into an environment 
              where <i>P</i> is not true, then it will no longer know that <i>P</i>), 
              and belief, on some construals where the <a class="thought" href="entries/content_entry.html">content</a> of a belief depends 
              on environmental <a class="thought" href="entries/context_entry.html">context</a>. However, mental properties that depend 
              only on internal (<a class="thought" href="entries/brain_entry.html">brain</a>) state will be organizational invariants. 
              This is not to say that causal topology is irrelevant to <a class="thought" href="entries/knowledge_entry.html">knowledge</a> 
              and belief. It will still capture the <i>internal</i> contribution 
              to those properties - that is, causal topology will contribute as 
              much as the <a class="thought" href="entries/brain_entry.html">brain</a> contributes. It is just that the environment will 
              also play a role.</p>
<p>The central claim can be justified by dividing mental properties 
              into two varieties: psychological properties - those that are characterized 
              by their causal role, such as belief, <a class="thought" href="entries/learning_entry.html">learning</a>, and <a class="thought" href="entries/perception_entry.html">perception</a> - 
              and phenomenal properties, or those that are characterized by way 
              in which they are consciously <a class="thought" href="entries/experience_entry.html">experience</a>d. Psychological properties 
              are concerned with the sort of thing the <a class="thought" href="entries/mind_entry.html">mind</a> <i>does</i>, and phenomenal 
              properties are concerned with the way it <i>feels</i>. (Some will 
              hold that properties such as belief should be assimilated to the 
              second rather than the first class; I do not think that this is 
              correct, but nothing will depend on that here.)</p>
<p>Psychological properties, as has been argued by Armstrong (1968) 
              and Lewis (1972) among others, are effectively defined by their 
              role within an overall causal <a class="thought" href="entries/system_entry.html">system</a>: it is the <a class="thought" href="entries/pattern_entry.html">pattern</a> of interaction 
              between different states that is definitive of a <a class="thought" href="entries/system_entry.html">system</a>'s psychological 
              properties. <a class="thought" href="entries/system_entry.html">System</a>s with the same causal topology will share these 
              <a class="thought" href="entries/pattern_entry.html">pattern</a>s of causal interactions among states, and therefore, by 
              the analysis of Lewis (1972), will share their psychological properties 
              (as long as their relation to the environment is appropriate).</p>
<p>Phenomenal properties are more problematic. It seems unlikely that 
              these can be <i>defined</i> by their causal roles (although many, 
              including Lewis and Armstrong, think they might be). To be a conscious 
              <a class="thought" href="entries/experience_entry.html">experience</a> is not to perform some role, but to have a particular 
              feel. These properties are characterized by <i>what it is like</i> 
              to have them, in Nagel's (1974) phrase. Phenomenal properties are 
              still quite mysterious and ill-understood.</p>
<p>Nevertheless, I believe that they can be seen to be organizational 
              invariants, as I have argued elsewhere. The argument for this, very 
              briefly, is a <i>reductio</i>. Assume conscious <a class="thought" href="entries/experience_entry.html">experience</a> is not 
              organizationally invariant. Then there exist <a class="thought" href="entries/system_entry.html">system</a>s with the same 
              causal topology but different conscious <a class="thought" href="entries/experience_entry.html">experience</a>s. Let us say 
              this is because the <a class="thought" href="entries/system_entry.html">system</a>s are made of different materials, such 
              as <a class="thought" href="entries/neuron_entry.html">neuron</a>s and <a class="thought" href="entries/silicon_entry.html">silicon</a>; a similar argument can be given for other 
              sorts of differences. As the two <a class="thought" href="entries/system_entry.html">system</a>s have the same causal topology, 
              we can (in principle) transform the first <a class="thought" href="entries/system_entry.html">system</a> into the second 
              by making only gradual changes, such as by replacing <a class="thought" href="entries/neuron_entry.html">neuron</a>s one 
              at a <a class="thought" href="entries/time_entry.html">time</a> with I/O equivalent <a class="thought" href="entries/silicon_entry.html">silicon</a> chips, where the overall <a class="thought" href="entries/pattern_entry.html">pattern</a> 
              of interaction remains the same throughout. Along the spectrum of 
              intermediate <a class="thought" href="entries/system_entry.html">system</a>s, there must be two <a class="thought" href="entries/system_entry.html">system</a>s between which we 
              replace less than ten percent of the <a class="thought" href="entries/system_entry.html">system</a>, but whose conscious 
              <a class="thought" href="entries/experience_entry.html">experience</a>s differ. Consider these two <a class="thought" href="entries/system_entry.html">system</a>s, N and S, which are 
              identical except in that some <a class="thought" href="entries/circuit_entry.html">circuit</a> in one is neural and in the 
              other is <a class="thought" href="entries/silicon_entry.html">silicon</a>.</p>
<p>The key step in the <a class="thought" href="entries/thought_entry.html">thought</a>-<a class="thought" href="entries/experiment_entry.html">experiment</a> is to take the relevant 
              neural <a class="thought" href="entries/circuit_entry.html">circuit</a> in <i>N</i>, and to install alongside it a causally 
              isomorphic <a class="thought" href="entries/silicon_entry.html">silicon</a> back-up <a class="thought" href="entries/circuit_entry.html">circuit</a>, with a <a class="thought" href="entries/switch_entry.html">switch</a> between the two 
              <a class="thought" href="entries/circuit_entry.html">circuit</a>s. What happens when we flip the <a class="thought" href="entries/switch_entry.html">switch</a>? By <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a>, the 
              <a class="thought" href="entries/system_entry.html">system</a>'s conscious <a class="thought" href="entries/experience_entry.html">experience</a>s will change: say, for purposes of 
              illustration, from a bright red <a class="thought" href="entries/experience_entry.html">experience</a> to a bright blue <a class="thought" href="entries/experience_entry.html">experience</a> 
              (or to a faded red <a class="thought" href="entries/experience_entry.html">experience</a>, or whatever). This follows from the 
              fact that the <a class="thought" href="entries/system_entry.html">system</a> after the change is a version of <i>S</i>, 
              whereas before the change it is just <i>N</i>.</p>
<p>But given the assumptions, there is no way for the <a class="thought" href="entries/system_entry.html">system</a> to <i>notice</i> 
              these changes. Its causal topology stays constant, so that all of 
              its functional states and behavioral dispositions stay fixed. If 
              noticing is defined functionally (as it should be), then there is 
              no room for any noticing to take place, and if it is not, any noticing 
              here would seem to be a thin <a class="thought" href="entries/event_entry.html">event</a> indeed. There is certainly no 
              room for a <a class="thought" href="entries/thought_entry.html">thought</a> "Hmm! Something strange just happened!" 
              unless it is floating free in some Cartesian realm.<sup>3</sup> 
              Even if there were such a <a class="thought" href="entries/thought_entry.html">thought</a>, it would be utterly impotent; 
              it could lead to no change of processing within the <a class="thought" href="entries/system_entry.html">system</a>, which 
              could not even mention it. (If the substitution were to yield some 
              change in processing, then the <a class="thought" href="entries/system_entry.html">system</a>s would not have the same causal 
              topology after all. Recall that the argument has the form of a <i>reductio</i>.) 
              We might even flip the <a class="thought" href="entries/switch_entry.html">switch</a> a <a class="thought" href="entries/number_entry.html">number</a> of times, so that red and 
              blue <a class="thought" href="entries/experience_entry.html">experience</a>s "dance" before the <a class="thought" href="entries/system_entry.html">system</a>'s inner eye; 
              it will never notice. This, I take it, is a <i>reductio ad absurdum</i> 
              of the original <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a>: if one's <a class="thought" href="entries/experience_entry.html">experience</a>s change, one can 
              potentially notice in a way that makes some causal difference. Therefore 
              the original assumption is false, and phenomenal properties are 
              organizational invariants. This needs to be worked out in more detail, 
              of course. I give the details of this "Dancing Qualia" 
              argument along with a related "Fading Qualia" argument 
              in (Chalmers 1995).</p>
<p>If all this works, it establishes that most mental properties are 
              organizational invariants: any two <a class="thought" href="entries/system_entry.html">system</a>s that share their fine-grained 
              causal topology will share their mental properties, modulo the contribution 
              of the environment.</p>
<h2>3.3 Justifying the theses</h2>
<p>To establish the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency, all we need 
              to do now is establish that organizational invariants are fixed 
              by some <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/structure_entry.html">structure</a>. This is quite straightforward.</p>
<p>An organizationally invariant property depends only on some <a class="thought" href="entries/pattern_entry.html">pattern</a> 
              of causal interaction between parts of the <a class="thought" href="entries/system_entry.html">system</a>. Given such a 
              <a class="thought" href="entries/pattern_entry.html">pattern</a>, we can straightforwardly abstract it into a CSA description: 
              the parts of the <a class="thought" href="entries/system_entry.html">system</a> will correspond to <a class="thought" href="entries/element_entry.html">element</a>s of the CSA state-vector, 
              and the <a class="thought" href="entries/pattern_entry.html">pattern</a>s of interaction will be expressed in the state-transition 
              rules. This will work straightforwardly as long as each part has 
              only a finite <a class="thought" href="entries/number_entry.html">number</a> of states that are relevant to the causal dependencies 
              between parts, which is likely to be the case in any <a class="thought" href="entries/biological_entry.html">biological</a>
<a class="thought" href="entries/system_entry.html">system</a> whose functions cannot realistically depend on infinite precision. 
              (I discuss the issue of <a class="thought" href="entries/analog_entry.html">analog</a> quantities in more detail below.) 
              Any <a class="thought" href="entries/system_entry.html">system</a> that <a class="thought" href="entries/implement_entry.html">implement</a>s this CSA will share the causal topology 
              of the original <a class="thought" href="entries/system_entry.html">system</a>. In fact, it turns out that the CSA formalism 
              provides a perfect formalization of the notion of causal topology. 
              A CSA description specifies a division of a <a class="thought" href="entries/system_entry.html">system</a> into parts, a 
              <a class="thought" href="entries/space_entry.html">space</a> of states for each part, and a <a class="thought" href="entries/pattern_entry.html">pattern</a> of interaction between 
              these states. This is precisely what is constitutive of causal topology.</p>
<p>If what has gone before is correct, this establishes the thesis 
              of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency, and therefore the view that Searle 
              has called "strong <a class="thought" href="entries/ai_entry.html">artificial intelligence</a>": that there 
              exists some <a class="thought" href="entries/computation_entry.html">computation</a> such that any <a class="thought" href="entries/implement_entry.html">implement</a>ation of the <a class="thought" href="entries/computation_entry.html">computation</a> 
              possesses mentality. The fine-grained causal topology of a <a class="thought" href="entries/brain_entry.html">brain</a> 
              can be specified as a CSA. Any <a class="thought" href="entries/implement_entry.html">implement</a>ation of that CSA will share 
              that causal topology, and therefore will share organizationally 
              invariant mental properties that arise from the <a class="thought" href="entries/brain_entry.html">brain</a>.</p>
<p>The thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation can be justified in a similar 
              way. As mental properties are organizational invariants, the physical 
              properties on which they depend are properties of causal organization. 
              Insofar as mental properties are to be explained in terms of the 
              physical at all, they can be explained in terms of the causal organization 
              of the <a class="thought" href="entries/system_entry.html">system</a>.<sup>4</sup> We can invoke further properties (<a class="thought" href="entries/implement_entry.html">implement</a>ational 
              details) if we like, but there is a clear <a class="thought" href="entries/sense_entry.html">sense</a> in which they are 
              not vital to the explanation. The neural or <a class="thought" href="entries/electronic_entry.html">electronic</a> composition 
              of an <a class="thought" href="entries/element_entry.html">element</a> is irrelevant for many purposes; to be more precise, 
              composition is relevant only insofar as it determines the <a class="thought" href="entries/element_entry.html">element</a>'s 
              causal role within the <a class="thought" href="entries/system_entry.html">system</a>. An <a class="thought" href="entries/element_entry.html">element</a> with different physical 
              composition but the same causal role would do just as well. This 
              is not to make the implausible claim that neural properties, say, 
              are entirely irrelevant to explanation. Often the best way to investigate 
              a <a class="thought" href="entries/system_entry.html">system</a>'s causal organization is to investigate its neural properties. 
              The claim is simply that insofar as neural properties are explanatorily 
              relevant, it is in virtue of the role they play in determining a 
              <a class="thought" href="entries/system_entry.html">system</a>s causal organization.</p>
<p>In the explanation of behavior, too, causal organization takes 
              center stage. A <a class="thought" href="entries/system_entry.html">system</a>'s behavior is determined by its underlying 
              causal organization, and we have seen that the <a class="thought" href="entries/computation_entry.html">computation</a>al framework 
              provides an ideal <a class="thought" href="entries/language_entry.html">language</a> in which this organization can be specified. 
              Given a <a class="thought" href="entries/pattern_entry.html">pattern</a> of causal interaction between substates of a <a class="thought" href="entries/system_entry.html">system</a>, 
              for <a class="thought" href="entries/instance_entry.html">instance</a>, there will be a CSA description that captures that 
              <a class="thought" href="entries/pattern_entry.html">pattern</a>. <a class="thought" href="entries/computation_entry.html">Computation</a>al descriptions of this kind provide a general 
              framework for the explanation of behavior. <br>
              For some explanatory purposes, we will invoke properties that are 
              not organizational invariants. If we are interested in the <a class="thought" href="entries/biological_entry.html">biological</a> 
              basis of cognition, we will invoke neural properties. To explain 
              situated cognition, we may invoke properties of the environment. 
              This is fine; the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation is not an 
              <i>exclusive</i> thesis. Still, usually we are interested in neural 
              properties insofar as they determine causal organization, we are 
              interested in properties of the environment insofar as they affect 
              the <a class="thought" href="entries/pattern_entry.html">pattern</a> of processing in a <a class="thought" href="entries/system_entry.html">system</a>, and so on. <a class="thought" href="entries/computation_entry.html">Computation</a> provides 
              a general explanatory framework that these other considerations 
              can supplement.</p>
<h2>3.4 Some <a class="thought" href="entries/object_entry.html">object</a>ions</h2>
<p>A <a class="thought" href="entries/computation_entry.html">computation</a>al basis for cognition can be challenged in two ways. 
              The first sort of challenge argues that <a class="thought" href="entries/computation_entry.html">computation</a> cannot <i>do</i> 
              what cognition does: that a <a class="thought" href="entries/computation_entry.html">computation</a>al simulation might not even 
              reproduce <a class="thought" href="entries/human_entry.html">human</a> behavioral capacities, for <a class="thought" href="entries/instance_entry.html">instance</a>, perhaps because 
              the causal <a class="thought" href="entries/structure_entry.html">structure</a> in <a class="thought" href="entries/human_entry.html">human</a> cognition goes beyond what a <a class="thought" href="entries/computation_entry.html">computation</a>al 
              description can provide. The second concedes that <a class="thought" href="entries/computation_entry.html">computation</a> might 
              capture the capacities, but argues that more is required for true 
              mentality. I will consider four <a class="thought" href="entries/object_entry.html">object</a>ions of the second variety, 
              and then three of the first. Answers to most of these <a class="thought" href="entries/object_entry.html">object</a>ions 
              fall directly out of the framework developed above.</p>
<p><b>But a <a class="thought" href="entries/computation_entry.html">computation</a>al model is just a simulation!</b> According 
              to this <a class="thought" href="entries/object_entry.html">object</a>ion, due to Searle (1980), Harnad (1989), and many 
              others, we do not expect a <a class="thought" href="entries/computer_entry.html">computer</a> model of a hurricane to be a 
              real hurricane, so why should a <a class="thought" href="entries/computer_entry.html">computer</a> model of <a class="thought" href="entries/mind_entry.html">mind</a> be a real 
              mind? But this is to miss the <a class="thought" href="entries/import_entry.html">import</a>ant point about organizational 
              invariance. A <a class="thought" href="entries/computation_entry.html">computation</a>al simulation is not a mere formal <a class="thought" href="entries/abstraction_entry.html">abstraction</a>, 
              but has rich internal <a class="thought" href="entries/dynamics_entry.html">dynamics</a> of its own. If appropriately designed 
              it will share the causal topology of the <a class="thought" href="entries/system_entry.html">system</a> that is being modeled, 
              so that the <a class="thought" href="entries/system_entry.html">system</a>'s organizationally invariant properties will 
              be not merely simulated but <i>replicated</i>.</p>
<p>The question about whether a <a class="thought" href="entries/computation_entry.html">computation</a>al model simulates or replicates 
              a given property comes down to the question of whether or not the 
              property is an organizational invariant. The property of being a 
              hurricane is obviously not an organizational invariant, for <a class="thought" href="entries/instance_entry.html">instance</a>, 
              as it is essential to the very notion of hurricanehood that wind 
              and air be involved. The same goes for properties such as digestion 
              and temperature, for which specific physical <a class="thought" href="entries/element_entry.html">element</a>s play a defining 
              role. There is no such obvious <a class="thought" href="entries/object_entry.html">object</a>ion to the organizational invariance 
              of cognition, so the cases are disanalogous, and indeed, I have 
              argued above that for mental properties, organizational invariance 
              actually holds. It follows that a model that is <a class="thought" href="entries/computation_entry.html">computation</a>ally 
              equivalent to a <a class="thought" href="entries/mind_entry.html">mind</a> will itself be a <a class="thought" href="entries/mind_entry.html">mind</a>.</p>
<p><b><a class="thought" href="entries/syntax_entry.html">Syntax</a> and <a class="thought" href="entries/semantics_entry.html">semantics</a></b>. Searle (1984) has argued along the 
              following lines: (1) A <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a> is syntactic; (2) <a class="thought" href="entries/syntax_entry.html">Syntax</a> 
              is not sufficient for <a class="thought" href="entries/semantics_entry.html">semantics</a>; (3) Minds have <a class="thought" href="entries/semantics_entry.html">semantics</a>; therefore 
              (4) <a class="thought" href="entries/implement_entry.html">Implement</a>ing a <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a> is insufficient for a <a class="thought" href="entries/mind_entry.html">mind</a>. 
              Leaving aside worries about the second premise, we can note that 
              this argument equivocates between <a class="thought" href="entries/program_entry.html">program</a>s and <a class="thought" href="entries/implement_entry.html">implement</a>ations of 
              those <a class="thought" href="entries/program_entry.html">program</a>s. While <a class="thought" href="entries/program_entry.html">program</a>s themselves are syntactic <a class="thought" href="entries/object_entry.html">object</a>s, 
              <a class="thought" href="entries/implement_entry.html">implement</a>ations are not: they are real physical <a class="thought" href="entries/system_entry.html">system</a>s with complex 
              causal organization, with real physical causation going on inside. 
              In an <a class="thought" href="entries/electronic_entry.html">electronic</a> <a class="thought" href="entries/computer_entry.html">computer</a>, for <a class="thought" href="entries/instance_entry.html">instance</a>, <a class="thought" href="entries/circuit_entry.html">circuit</a>s and voltages push 
              each other around in a manner <a class="thought" href="entries/analog_entry.html">analog</a>ous to that in which <a class="thought" href="entries/neuron_entry.html">neuron</a>s 
              and activations push each other around. It is precisely in virtue 
              of this causation that <a class="thought" href="entries/implement_entry.html">implement</a>ations may have cognitive and therefore 
              semantic properties.</p>
<p>It is the notion of <a class="thought" href="entries/implement_entry.html">implement</a>ation that does all the work here. 
              A <a class="thought" href="entries/program_entry.html">program</a> and its physical <a class="thought" href="entries/implement_entry.html">implement</a>ation should not be regarded 
              as equivalent - they lie on entirely different levels, and have 
              entirely different properties. It is the <a class="thought" href="entries/program_entry.html">program</a> that is syntactic; 
              it is the <a class="thought" href="entries/implement_entry.html">implement</a>ation that has semantic <a class="thought" href="entries/content_entry.html">content</a>. Of course, there 
              is still a substantial question about how an <a class="thought" href="entries/implement_entry.html">implement</a>ation comes 
              to possess semantic <a class="thought" href="entries/content_entry.html">content</a>, just as there is a substantial question 
              about how a <i><a class="thought" href="entries/brain_entry.html">brain</a></i> comes to possess semantic <a class="thought" href="entries/content_entry.html">content</a>. But 
              once we focus on the <a class="thought" href="entries/implement_entry.html">implement</a>ation, rather than the <a class="thought" href="entries/program_entry.html">program</a>, we 
              are at least in the right ballpark. We are talking about a physical 
              <a class="thought" href="entries/system_entry.html">system</a> with causal heft, rather than a shadowy syntactic <a class="thought" href="entries/object_entry.html">object</a>. 
              If we accept, as is extremely plausible, that brains have semantic 
              properties in virtue of their causal organization and causal relations, 
              then the same will go for <a class="thought" href="entries/implement_entry.html">implement</a>ations. <a class="thought" href="entries/syntax_entry.html">Syntax</a> may not be sufficient 
              for <a class="thought" href="entries/semantics_entry.html">semantics</a>, but the right kind of causation is.</p>
<p><b>The <a class="thought" href="entries/chinese_room_entry.html">Chinese room</a></b>. There is not room here to deal with Searle's 
              famous <a class="thought" href="entries/chinese_room_entry.html">Chinese room</a> argument in detail. I note, however, that the 
              account I have given supports the "Systems reply", according 
              to which the entire <a class="thought" href="entries/system_entry.html">system</a> understands Chinese even if the homunculus 
              doing the simulating does not. Say the overall <a class="thought" href="entries/system_entry.html">system</a> is simulating 
              a <a class="thought" href="entries/brain_entry.html">brain</a>, <a class="thought" href="entries/neuron_entry.html">neuron</a>-by-<a class="thought" href="entries/neuron_entry.html">neuron</a>. Then like any <a class="thought" href="entries/implement_entry.html">implement</a>ation, it will 
              share <a class="thought" href="entries/import_entry.html">import</a>ant causal organization with the <a class="thought" href="entries/brain_entry.html">brain</a>. In particular, 
              if there is a <a class="thought" href="entries/symbol_entry.html">symbol</a> for every <a class="thought" href="entries/neuron_entry.html">neuron</a>, then the <a class="thought" href="entries/pattern_entry.html">pattern</a>s of interaction 
              between slips of paper bearing those <a class="thought" href="entries/symbol_entry.html">symbol</a>s will mirror <a class="thought" href="entries/pattern_entry.html">pattern</a>s 
              of interaction between <a class="thought" href="entries/neuron_entry.html">neuron</a>s in the <a class="thought" href="entries/brain_entry.html">brain</a>, and so on. This organization 
              is <a class="thought" href="entries/implement_entry.html">implement</a>ed in a baroque way, but we should not let the baroqueness 
              blind us to the fact that the causal organization -- <i>real</i>, 
              physical causal organization -- is there. (The same goes for a simulation 
              of cognition at level above the neural, in which the shared causal 
              organization will lie at a coarser level.)</p>
<p>It is precisely in virtue of this causal organization that the 
              <a class="thought" href="entries/system_entry.html">system</a> possesses its mental properties. We can rerun a version of 
              the "dancing qualia" argument to see this. In principle, 
              we can move from the <a class="thought" href="entries/brain_entry.html">brain</a> to the <a class="thought" href="entries/chinese_room_entry.html">Chinese room</a> simulation in small 
              steps, replacing <a class="thought" href="entries/neuron_entry.html">neuron</a>s at each step by little demons doing the 
              same causal work, and then gradually cutting down labor by replacing 
              two neighboring demons by one who does the same work. Eventually 
              we arrive at a <a class="thought" href="entries/system_entry.html">system</a> where a single demon is responsible for maintaining 
              the causal organization, without requiring any real <a class="thought" href="entries/neuron_entry.html">neuron</a>s at all. 
              This organization might be maintained between marks on paper, or 
              it might even be present inside the demon's own head, if the calculations 
              are memorized. The arguments about organizational invariance all 
              hold here -- for the same <a class="thought" href="entries/reason_entry.html">reason</a>s as before, it is implausible to 
              suppose that the <a class="thought" href="entries/system_entry.html">system</a>'s <a class="thought" href="entries/experience_entry.html">experience</a>s will change or disappear.</p>
<p>Performing the <a class="thought" href="entries/thought_entry.html">thought</a>-<a class="thought" href="entries/experiment_entry.html">experiment</a> this way makes it clear that 
              we should not expect the <a class="thought" href="entries/experience_entry.html">experience</a>s to be had by the <i>demon</i>. 
              The demon is simply a kind of causal facilitator, ensuring that 
              states bear the appropriate causal relations to each other. The 
              conscious <a class="thought" href="entries/experience_entry.html">experience</a>s will be had by the <a class="thought" href="entries/system_entry.html">system</a> as a whole. Even 
              if that <a class="thought" href="entries/system_entry.html">system</a> is <a class="thought" href="entries/implement_entry.html">implement</a>ed inside the demon by virtue of the 
              demon's memorization, the <a class="thought" href="entries/system_entry.html">system</a> should not be confused with demon 
              itself. We should not suppose that the demon will share the <a class="thought" href="entries/implement_entry.html">implement</a>ed 
              <a class="thought" href="entries/system_entry.html">system</a>'s <a class="thought" href="entries/experience_entry.html">experience</a>s, any more than it will share the <a class="thought" href="entries/experience_entry.html">experience</a>s 
              of an ant that crawls inside its skull: both are cases of two <a class="thought" href="entries/computation_entry.html">computation</a>al 
              <a class="thought" href="entries/system_entry.html">system</a>s being <a class="thought" href="entries/implement_entry.html">implement</a>ed within a single physical <a class="thought" href="entries/space_entry.html">space</a>. Mental 
              properties arising from distinct <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/system_entry.html">system</a>s will be quite 
              distinct, and there is no <a class="thought" href="entries/reason_entry.html">reason</a> to suppose that they overlap.</p>
<p><b>What about the environment?</b> Some mental properties, such 
              as <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and even belief, depend on the environment being a 
              certain way. <a class="thought" href="entries/computation_entry.html">Computation</a>al organization, as I have outlined it, 
              cannot determine the environmental contribution, and therefore cannot 
              fully guarantee this sort of mental property. But this is no problem. 
              All we need <a class="thought" href="entries/computation_entry.html">computation</a>al organization to give us is the <i>internal</i> 
              contribution to mental properties: that is, the same contribution 
              that the <a class="thought" href="entries/brain_entry.html">brain</a> makes (for <a class="thought" href="entries/instance_entry.html">instance</a>, <a class="thought" href="entries/computation_entry.html">computation</a>al organization will 
              determine the so-called "narrow <a class="thought" href="entries/content_entry.html">content</a>" of a belief, 
              if this exists; see Fodor 1987). The full panoply of mental properties 
              might only be determined by <a class="thought" href="entries/computation_entry.html">computation</a>-plus-environment, just as 
              it is determined by <a class="thought" href="entries/brain_entry.html">brain</a>-plus-environment. These considerations 
              do not count against the prospects of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a>, and 
              they affect the aspirations of <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> no 
              more than they affect the aspirations of <a class="thought" href="entries/neuroscience_entry.html">neuroscience</a>.</p>
<p><b>Is cognition computable?</b> In the preceding discussion I have 
              taken for granted that <a class="thought" href="entries/computation_entry.html">computation</a> can at least <i>simulate</i>
<a class="thought" href="entries/human_entry.html">human</a> cognitive <a class="thought" href="entries/capacity_entry.html">capacity</a>, and have been concerned to argue that 
              this counts as honest-to-goodness mentality. The former point has 
              often been granted by opponents of <a class="thought" href="entries/ai_entry.html">AI</a>, (e.g. Searle 1980) who have 
              directed the fire at the latter, but it is not uncontroversial.</p>
<p>This is to some extent an empirical issue, but the relevant evidence 
              is solidly on the side of computability. We have every <a class="thought" href="entries/reason_entry.html">reason</a> to 
              believe that the low-level laws of <a class="thought" href="entries/physics_entry.html">physics</a> are computable. If so, 
              then low-level neurophysiological processes can be <a class="thought" href="entries/computation_entry.html">computation</a>ally 
              simulated; it follows that the function of the whole <a class="thought" href="entries/brain_entry.html">brain</a> is computable 
              too, as the <a class="thought" href="entries/brain_entry.html">brain</a> consists in a <a class="thought" href="entries/network_entry.html">network</a> of neurophysiological parts. 
              Some have disputed the premise: for example, Penrose (1989) has 
              speculated that the effects of quantum <a class="thought" href="entries/gravity_entry.html">gravity</a> are noncomputable, 
              and that these effects may play a role in cognitive functioning. 
              He offers no arguments to back up this speculation, however, and 
              there is no evidence of such noncomputability in current physical 
              theory (see Pour-El and Richards (1989) for a discussion). Failing 
              such a radical development as the discovery that the fundamental 
              laws of <a class="thought" href="entries/nature_entry.html">nature</a> are uncomputable, we have every <a class="thought" href="entries/reason_entry.html">reason</a> to believe 
              that <a class="thought" href="entries/human_entry.html">human</a> cognition can be <a class="thought" href="entries/computation_entry.html">computation</a>ally modeled. </p>
<p><b>What about G&#246;del's theorem?</b> G&#246;del's theorem states 
              that for any consistent formal <a class="thought" href="entries/system_entry.html">system</a>, there are statements of arithmetic 
              that are unprovable within the <a class="thought" href="entries/system_entry.html">system</a>. This has led some (Lucas 
              1963; Penrose 1989) to conclude that humans have abilities that 
              cannot be duplicated by any <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/system_entry.html">system</a>. For example, our 
              ability to "see" the truth of the G&#246;del sentence 
              of a formal <a class="thought" href="entries/system_entry.html">system</a> is argued to be non-<a class="thought" href="entries/algorithm_entry.html">algorithm</a>ic. I will not deal 
              with this <a class="thought" href="entries/object_entry.html">object</a>ion in detail here, as the answer to it is not a 
              direct application of the current framework. I will simply note 
              that the assumption that we can see the truth of arbitrary G&#246;del 
              sentences requires that we have the ability to determine the consistency 
              or inconsistency of any given formal <a class="thought" href="entries/system_entry.html">system</a>, and there is no <a class="thought" href="entries/reason_entry.html">reason</a> 
              to believe that we have this ability in general. (For more on this 
              point, see Putnam 1960, Bowie 1982 and the commentaries on Penrose 
              1990.)</p>
<p><b>Discreteness and continuity. </b>An <a class="thought" href="entries/import_entry.html">import</a>ant <a class="thought" href="entries/object_entry.html">object</a>ion notes 
              that the CSA formalism only captures <i>discrete</i> causal organization, 
              and argues that some cognitive properties may depend on continuous 
              aspects of that organization, such as <a class="thought" href="entries/analog_entry.html">analog</a> values or chaotic dependencies.</p>
<p>A <a class="thought" href="entries/number_entry.html">number</a> of responses to this are possible. The first is to note 
              that the current framework can fairly easily be extended to deal 
              with <a class="thought" href="entries/computation_entry.html">computation</a> over continuous quantities such as real <a class="thought" href="entries/number_entry.html">number</a>s. 
              All that is required is that the various substates of a CSA be represented 
              by a real <a class="thought" href="entries/parameter_entry.html">parameter</a> rather than a discrete <a class="thought" href="entries/parameter_entry.html">parameter</a>, where appropriate 
              restrictions are placed on allowable state-transitions (for <a class="thought" href="entries/instance_entry.html">instance</a>, 
              we can require that <a class="thought" href="entries/parameter_entry.html">parameter</a>s are transformed polynomially, where 
              the requisite transformation can be conditional on sign). See Blum, 
              Shub and Smale (1989) for a careful working-out of some of the relevant 
              theory of computability. A theory of <a class="thought" href="entries/implement_entry.html">implement</a>ation can be given 
              along in a fashion similar to the account I have given above, where 
              continuous quantities in the formalism are required to correspond 
              to continuous physical <a class="thought" href="entries/parameter_entry.html">parameter</a>s with an appropriate correspondence 
              in state-transitions.</p>
<p>This formalism is still discrete in time: <a class="thought" href="entries/evolution_entry.html">evolution</a> of the continuous 
              states proceeds in discrete temporal steps. It might be argued that 
              cognitive organization is in fact continuous in <a class="thought" href="entries/time_entry.html">time</a>, and that a 
              relevant formalism should capture this. In this case, the specification 
              of discrete state-transitions between states can be replaced by 
              differential equations specifying how continuous quantities change 
              in continuous <a class="thought" href="entries/time_entry.html">time</a>, giving a thoroughly continuous <a class="thought" href="entries/computation_entry.html">computation</a>al 
              framework. MacLennan (1990) describes a framework along these lines. 
              Whether such a framework truly qualifies as <i><a class="thought" href="entries/computation_entry.html">computation</a>al</i> 
              is largely a terminological <a class="thought" href="entries/matter_entry.html">matter</a>, but there it is arguable that 
              the framework is significantly similar in kind to the traditional 
              approach; all that has changed is that discrete states and steps 
              have been "smoothed out".</p>
<p>We need not go this far, however. There are good <a class="thought" href="entries/reason_entry.html">reason</a>s to suppose 
              that whether or not cognition in the <a class="thought" href="entries/brain_entry.html">brain</a> is continuous, a discrete 
              framework can capture everything <a class="thought" href="entries/import_entry.html">import</a>ant that is going on. To 
              see this, we can note that a discrete <a class="thought" href="entries/abstraction_entry.html">abstraction</a> can describe and 
              simulate a continuous process to any required degree of accuracy. 
              It might be <a class="thought" href="entries/object_entry.html">object</a>ed that chaotic processes can amplify microscopic 
              differences to significant levels. Even so, it is implausible that 
              the correct functioning of mental processes <i>depends</i> on the 
              precise value of the tenth decimal place of <a class="thought" href="entries/analog_entry.html">analog</a> quantities. The 
              presence of background <a class="thought" href="entries/noise_entry.html">noise</a> and randomness in <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/system_entry.html">system</a>s 
              implies that such precision would inevitably be "washed out" 
              in practice. It follows that although a discrete simulation may 
              not yield precisely the behavior that a given cognitive <a class="thought" href="entries/system_entry.html">system</a> produces 
              on a given occasion, it will yield plausible behavior that the <a class="thought" href="entries/system_entry.html">system</a>
<i>might</i> have produced had background <a class="thought" href="entries/noise_entry.html">noise</a> been a little different. 
              This is all that a proponent of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> need claim.</p>
<p>Indeed, the presence of <a class="thought" href="entries/noise_entry.html">noise</a> in physical <a class="thought" href="entries/system_entry.html">system</a>s suggests that 
              any given continuous <a class="thought" href="entries/computation_entry.html">computation</a> of the above kinds can never be 
              reliably <a class="thought" href="entries/implement_entry.html">implement</a>ed in practice, but only approximately <a class="thought" href="entries/implement_entry.html">implement</a>ed. 
              For the purposes of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> we will do just as well 
              with discrete <a class="thought" href="entries/system_entry.html">system</a>s, which can also give us approximate <a class="thought" href="entries/implement_entry.html">implement</a>ations 
              of continuous <a class="thought" href="entries/computation_entry.html">computation</a>s.</p>
<p>It follows that these considerations do not count against the theses 
              of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency or of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation. To 
              see the first, note that a discrete simulation can replicate everything 
              <i>essential</i> to cognitive functioning, for the <a class="thought" href="entries/reason_entry.html">reason</a>s above, 
              even though it may not duplicate every last detail of a given episode 
              of cognition. To see the second, note that for similar <a class="thought" href="entries/reason_entry.html">reason</a>s the 
              precise values of <a class="thought" href="entries/analog_entry.html">analog</a> quantities cannot be relevant to the explanation 
              of our cognitive <i>capacities</i>, and that a discrete description 
              can do the job.</p>
<p>This is not to exclude continuous formalisms from cognitive explanation. 
              The thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation is not an exclusive thesis. 
              It may be that continuous formalisms will provide a simpler and 
              more natural framework for the explanation of many dynamic processes, 
              as we find in the theory of <a class="thought" href="entries/neural_network_entry.html">neural network</a>s. Perhaps the most <a class="thought" href="entries/reason_entry.html">reason</a>able 
              version of the <a class="thought" href="entries/computation_entry.html">computation</a>alist view accepts the thesis of (discrete) 
              <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency, but supplements the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al 
              explanation with the proviso that continuous <a class="thought" href="entries/computation_entry.html">computation</a> may sometimes 
              provide a more natural explanatory framework (a discrete explanation 
              could do the same job, but more clumsily). In any case, continuous 
              <a class="thought" href="entries/computation_entry.html">computation</a> does not give us anything fundamentally new.</p>
<h2>4 Other kinds of <a class="thought" href="entries/computation_entry.html">computation</a>alism</h2>
<p><a class="thought" href="entries/ai_entry.html">Artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> are 
              committed to a kind of <a class="thought" href="entries/computation_entry.html">computation</a>alism about the <a class="thought" href="entries/mind_entry.html">mind</a>, a <a class="thought" href="entries/computation_entry.html">computation</a>alism 
              defined by the theses of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency and <a class="thought" href="entries/computation_entry.html">computation</a>al 
              explanation. In this paper I have tried to justify this <a class="thought" href="entries/computation_entry.html">computation</a>alism, 
              by spelling out the role of <a class="thought" href="entries/computation_entry.html">computation</a> as a tool for describing 
              and duplicating causal organization. I think that this kind of <a class="thought" href="entries/computation_entry.html">computation</a>alism 
              is all that <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al cognitive 
              <a class="thought" href="entries/science_entry.html">science</a> are committed to, and indeed is all that they need. This 
              sort of <a class="thought" href="entries/computation_entry.html">computation</a>alism provides a <i>general</i> framework precisely 
              because it makes so few claims about the <i>kind</i> of <a class="thought" href="entries/computation_entry.html">computation</a> 
              that is central to the explanation and replication of cognition. 
              No <a class="thought" href="entries/matter_entry.html">matter</a> what the causal organization of cognitive processes turns 
              out to be, there is good <a class="thought" href="entries/reason_entry.html">reason</a> to believe that it can be captured 
              within a <a class="thought" href="entries/computation_entry.html">computation</a>al framework.</p>
<p>The fields have often been taken to be committed to stronger claims, 
              sometimes by proponents and more often by opponents. For example, 
              Edelman (1989) criticizes the <a class="thought" href="entries/computation_entry.html">computation</a>al approach to the study 
              of the <a class="thought" href="entries/mind_entry.html">mind</a> on the grounds that: An analysis of the <a class="thought" href="entries/evolution_entry.html">evolution</a>, development, 
              and <a class="thought" href="entries/structure_entry.html">structure</a> of brains makes it highly unlikely that they could 
              be <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s. This is so because of the enormous <a class="thought" href="entries/individual_entry.html">individual</a> 
              variation in <a class="thought" href="entries/structure_entry.html">structure</a> that brains possess at a variety of organizational 
              levels. [...] [Also,] an analysis of both ecological and environmental 
              variation, and of the categorization procedures of <a class="thought" href="entries/animal_entry.html">animal</a>s and humans, 
              makes it highly unlikely that the world (physical and social) can 
              function as a tape for a <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>. (Edelman 1989, p. 30.)</p>
<p>But <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> 
              are not committed to the claim that the <a class="thought" href="entries/brain_entry.html">brain</a> is literally a Turing 
              <a class="thought" href="entries/machine_entry.html">machine</a> with a moving head and a tape, and even less to the claim 
              that that tape is the environment. The claim is simply that some 
              <a class="thought" href="entries/computation_entry.html">computation</a>al framework can <i>explain</i> and <i>replicate</i>
<a class="thought" href="entries/human_entry.html">human</a> cognitive processes. It may turn out that the relevant <a class="thought" href="entries/computation_entry.html">computation</a>al 
              description of these processes is very fine-grained, reflecting 
              extremely complex causal <a class="thought" href="entries/dynamics_entry.html">dynamics</a> among <a class="thought" href="entries/neuron_entry.html">neuron</a>s, and it may well 
              turn out that there is significant variation in causal organization 
              between <a class="thought" href="entries/individual_entry.html">individual</a>s. There is nothing here that is incompatible 
              with a <a class="thought" href="entries/computation_entry.html">computation</a>al approach to <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>.</p>
<p>In a similar way, a <a class="thought" href="entries/computation_entry.html">computation</a>alist need not claim that the <a class="thought" href="entries/brain_entry.html">brain</a> 
              is a von Neumann <a class="thought" href="entries/machine_entry.html">machine</a>, or has some other specific <a class="thought" href="entries/architecture_entry.html">architecture</a>. 
              Like <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s, von Neumann <a class="thought" href="entries/machine_entry.html">machine</a>s are just one kind of 
              <a class="thought" href="entries/architecture_entry.html">architecture</a>, particularly well-suited to <a class="thought" href="entries/program_entry.html">program</a>mability, but the 
              claim that the <a class="thought" href="entries/brain_entry.html">brain</a> <a class="thought" href="entries/implement_entry.html">implement</a>s such an <a class="thought" href="entries/architecture_entry.html">architecture</a> is far ahead 
              of any empirical evidence and is most likely false. The commitments 
              of <a class="thought" href="entries/computation_entry.html">computation</a>alism are more general.</p>
<p><a class="thought" href="entries/computation_entry.html">Computation</a>alism is occasionally associated with the view that 
              cognition is rule-following, but again this is a strong empirical 
              <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a> that is inessential to the foundations of the fields. 
              It is entirely possible that the only "rules" found in 
              a <a class="thought" href="entries/computation_entry.html">computation</a>al description of <a class="thought" href="entries/thought_entry.html">thought</a> will be at a very low level, 
              specifying the causal <a class="thought" href="entries/dynamics_entry.html">dynamics</a> of <a class="thought" href="entries/neuron_entry.html">neuron</a>s, for <a class="thought" href="entries/instance_entry.html">instance</a>, or perhaps 
              the <a class="thought" href="entries/dynamics_entry.html">dynamics</a> of some level between the neural and the cognitive. 
              Even if there are no rules to be found at the cognitive level, a 
              <a class="thought" href="entries/computation_entry.html">computation</a>al approach to the <a class="thought" href="entries/mind_entry.html">mind</a> can still succeed. Another claim 
              to which a <a class="thought" href="entries/computation_entry.html">computation</a>alist need not be committed are "the 
              <a class="thought" href="entries/brain_entry.html">brain</a> is a <a class="thought" href="entries/computer_entry.html">computer</a>"; as we have seen, it is not <a class="thought" href="entries/computer_entry.html">computer</a>s 
              that are central but <a class="thought" href="entries/computation_entry.html">computation</a>s).</p>
<p>The most ubiquitous "strong" form of <a class="thought" href="entries/computation_entry.html">computation</a>alism 
              has been what we may call <i><a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>alism</i>: the 
              view that cognition is <a class="thought" href="entries/computation_entry.html">computation</a> over representation (Newell and 
              Simon 1976; Fodor and Pylyshyn 1988). To a first approximation, 
              we can cash out this view as the claim that the <i><a class="thought" href="entries/computation_entry.html">computation</a>al</i> 
              primitives in a <a class="thought" href="entries/computation_entry.html">computation</a>al description of cognition are also 
              <i>representational</i> primitives. That is to say, the <a class="thought" href="entries/basic_entry.html">basic</a> syntactic 
              entities between which state-transitions are defined are themselves 
              bearers of semantic <a class="thought" href="entries/content_entry.html">content</a>, and are therefore <i><a class="thought" href="entries/symbol_entry.html">symbol</a>s</i>.</p>
<p><a class="thought" href="entries/symbol_entry.html">Symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>alism has been a popular and fruitful approach 
              to the <a class="thought" href="entries/mind_entry.html">mind</a>, but it does not exhaust the resources of <a class="thought" href="entries/computation_entry.html">computation</a>. 
              Not all <a class="thought" href="entries/computation_entry.html">computation</a>s are <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>s. We have seen that 
              there are some <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s that lack semantic <a class="thought" href="entries/content_entry.html">content</a> altogether, 
              for <a class="thought" href="entries/instance_entry.html">instance</a>. Perhaps <a class="thought" href="entries/system_entry.html">system</a>s that carry semantic <a class="thought" href="entries/content_entry.html">content</a> are more 
              plausible models of cognition, but even in these <a class="thought" href="entries/system_entry.html">system</a>s there is 
              no <a class="thought" href="entries/reason_entry.html">reason</a> why the <a class="thought" href="entries/content_entry.html">content</a> must be carried by the <a class="thought" href="entries/system_entry.html">system</a>s' <a class="thought" href="entries/computation_entry.html">computation</a>al 
              primitives. In connectionist <a class="thought" href="entries/system_entry.html">system</a>s, for example, the <a class="thought" href="entries/basic_entry.html">basic</a> bearers 
              of semantic <a class="thought" href="entries/content_entry.html">content</a> are <i>distributed</i> representations, <a class="thought" href="entries/pattern_entry.html">pattern</a>s 
              of activity over many units, whereas the <a class="thought" href="entries/computation_entry.html">computation</a>al primitives 
              are simple units that may themselves lack semantic <a class="thought" href="entries/content_entry.html">content</a>. To use 
              Smolensky's term (Smolensky 1988), these <a class="thought" href="entries/system_entry.html">system</a>s perform <i>subsymbolic</i>
<a class="thought" href="entries/computation_entry.html">computation</a>: the level of <a class="thought" href="entries/computation_entry.html">computation</a> falls below the level of representation.<sup>5</sup> 
              But the <a class="thought" href="entries/system_entry.html">system</a>s are <a class="thought" href="entries/computation_entry.html">computation</a>al nevertheless.</p>
<p>Note that the distinction between <a class="thought" href="entries/symbol_entry.html">symbol</a>ic and subsymbolic <a class="thought" href="entries/computation_entry.html">computation</a> 
              does not coincide with the distinction between different <a class="thought" href="entries/computation_entry.html">computation</a>al 
              formalisms, such as <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s and <a class="thought" href="entries/neural_network_entry.html">neural network</a>s. Rather, 
              the distinction divides the class of <a class="thought" href="entries/computation_entry.html">computation</a>s within each of 
              these formalisms. Some <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s perform <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>, 
              and some perform subsymbolic <a class="thought" href="entries/computation_entry.html">computation</a>; the same goes for neural 
              <a class="thought" href="entries/network_entry.html">network</a>s. (Of course it is sometimes said that all <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s 
              perform "symbol manipulation", but this holds only if 
              the ambiguous term "symbol" is used in a purely syntactic 
              <a class="thought" href="entries/sense_entry.html">sense</a>, rather than in the semantic <a class="thought" href="entries/sense_entry.html">sense</a> I am using here.)</p>
<p>Both proponents and opponents of a <a class="thought" href="entries/computation_entry.html">computation</a>al approach have 
              often implicitly identified <a class="thought" href="entries/computation_entry.html">computation</a> with <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>. 
              A critique called <i>What <a class="thought" href="entries/computer_entry.html">Computer</a>s Can't Do</i> (Dreyfus 1972), 
              for <a class="thought" href="entries/instance_entry.html">instance</a>, turns out to be largely directed at <a class="thought" href="entries/system_entry.html">system</a>s that perform 
              <a class="thought" href="entries/computation_entry.html">computation</a> over explicit representation. Other sorts of <a class="thought" href="entries/computation_entry.html">computation</a> 
              are left untouched, and indeed <a class="thought" href="entries/system_entry.html">system</a>s performing subsymbolic <a class="thought" href="entries/computation_entry.html">computation</a> 
              seem well suited for some of Dreyfus's problem areas. The broader 
              ambitions of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> are therefore left intact.</p>
<p>On the other side of the fence, Fodor (1992) uses the name "Computational 
              Theory of Mind" for a version of <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>alism, 
              and suggests that Turing's main contribution to <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> 
              is the idea that syntactic state-transitions between <a class="thought" href="entries/symbol_entry.html">symbol</a>s can 
              be made to respect their semantic <a class="thought" href="entries/content_entry.html">content</a>. This strikes me as false. 
              Turing was concerned very little with the semantic <a class="thought" href="entries/content_entry.html">content</a> of internal 
              states, and the concentration on <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a> came later. 
              Rather, Turing's key contribution was the formalization of the notion 
              of <i>mechanism</i>, along with the associated <i>universality</i> 
              of the formalization. It is this universality that gives us good 
              <a class="thought" href="entries/reason_entry.html">reason</a> to suppose that <a class="thought" href="entries/computation_entry.html">computation</a> can do almost anything that any 
              mechanism can do, thus accounting for the centrality of <a class="thought" href="entries/computation_entry.html">computation</a> 
              in the study of cognition.</p>
<p>Indeed, a focus on <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a> sacrifices the universality 
              that is at the heart of Turing's contribution. Universality applies 
              to entire classes of automata, such as <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s, where these 
              classes are defined syntactically. The requirement that an automaton 
              performs <a class="thought" href="entries/computation_entry.html">computation</a> over representation is a strong further <a class="thought" href="entries/constraint_entry.html">constraint</a>, 
              a semantic <a class="thought" href="entries/constraint_entry.html">constraint</a> that plays no part in the <a class="thought" href="entries/basic_entry.html">basic</a> theory of 
              <a class="thought" href="entries/computation_entry.html">computation</a>. There is no <a class="thought" href="entries/reason_entry.html">reason</a> to suppose that the much narrower 
              class of <a class="thought" href="entries/turing_machine_entry.html">Turing machine</a>s that perform <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a> is universal. 
              If we wish to appeal to universality in a defense of <a class="thought" href="entries/computation_entry.html">computation</a>alism, 
              we must cast the net more widely than this.<sup>6</sup></p>
<p>The various strong forms of <a class="thought" href="entries/computation_entry.html">computation</a>alism outlined here are 
              bold empirical hypotheses with varying degrees of plausibility. 
              I suspect that they are all false, but in any case their truth and 
              falsity is not the issue here. Because they are such strong empirical 
              hypotheses, they are in no position to serve as a <i>foundation</i> 
              for <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>. 
              If the fields were committed to these hypotheses, their status would 
              be much more questionable than it currently is. <a class="thought" href="entries/ai_entry.html">Artificial intelligence</a> 
              and <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> can survive the discovery that 
              the <a class="thought" href="entries/brain_entry.html">brain</a> is not a von Neumann <a class="thought" href="entries/machine_entry.html">machine</a>, or that cognition is not 
              rule-following, or that the <a class="thought" href="entries/brain_entry.html">brain</a> does not engage in <a class="thought" href="entries/computation_entry.html">computation</a> 
              over representation, precisely because these are not among the fields' 
              foundational commitments. <a class="thought" href="entries/computation_entry.html">Computation</a> is much more general than 
              this, and consequently much more robust.<sup>7</sup></p>
<h2>5 Conclusion: Toward a minimal <a class="thought" href="entries/computation_entry.html">computation</a>alism</h2>
<p>The view that I have advocated can be called <i>minimal <a class="thought" href="entries/computation_entry.html">computation</a>alism</i>. 
              It is defined by the twin theses of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency and 
              <a class="thought" href="entries/computation_entry.html">computation</a>al explanation, where <a class="thought" href="entries/computation_entry.html">computation</a> is taken in the broad 
              <a class="thought" href="entries/sense_entry.html">sense</a> that dates back to Turing. I have argued that these theses 
              are compelling precisely because <a class="thought" href="entries/computation_entry.html">computation</a> provides a general 
              framework for describing and determining <a class="thought" href="entries/pattern_entry.html">pattern</a>s of causal organization, 
              and because mentality is rooted in such <a class="thought" href="entries/pattern_entry.html">pattern</a>s. The thesis of 
              <a class="thought" href="entries/computation_entry.html">computation</a>al explanation holds because <a class="thought" href="entries/computation_entry.html">computation</a> provides a perfect 
              <a class="thought" href="entries/language_entry.html">language</a> in which to specify the causal organization of cognitive 
              processes; and the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al sufficiency holds because 
              in all <a class="thought" href="entries/implement_entry.html">implement</a>ations of the appropriate <a class="thought" href="entries/computation_entry.html">computation</a>s, the causal 
              <a class="thought" href="entries/structure_entry.html">structure</a> of mentality is replicated.</p>
<p>Unlike the stronger forms of <a class="thought" href="entries/computation_entry.html">computation</a>alism, minimal <a class="thought" href="entries/computation_entry.html">computation</a>alism 
              is not a bold empirical <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a>. To be sure, there are some ways 
              that empirical <a class="thought" href="entries/science_entry.html">science</a> might prove it to be false: if it turns out 
              that the fundamental laws of <a class="thought" href="entries/physics_entry.html">physics</a> are noncomputable and if this 
              noncomputability reflects itself in cognitive functioning, for <a class="thought" href="entries/instance_entry.html">instance</a>, 
              or if it turns out that our cognitive capacities depend essentially 
              on infinite precision in certain <a class="thought" href="entries/analog_entry.html">analog</a> quantities, or indeed if 
              it turns out that cognition is mediated by some non-physical substance 
              whose workings are not computable. But these developments seem unlikely; 
              and failing developments like these, <a class="thought" href="entries/computation_entry.html">computation</a> provides a general 
              framework in which we can express the causal organization of cognition, 
              whatever that organization turns out to be.</p>
<p>Minimal <a class="thought" href="entries/computation_entry.html">computation</a>alism is compatible with such diverse <a class="thought" href="entries/program_entry.html">program</a>s 
              as <a class="thought" href="entries/connectionism_entry.html">connectionism</a>, logicism, and approaches focusing on dynamic <a class="thought" href="entries/system_entry.html">system</a>s, 
              <a class="thought" href="entries/evolution_entry.html">evolution</a>, and <a class="thought" href="entries/artificial_life_entry.html">artificial life</a>. It is occasionally said that <a class="thought" href="entries/program_entry.html">program</a>s 
              such as <a class="thought" href="entries/connectionism_entry.html">connectionism</a> are "noncomputational", but it seems 
              more <a class="thought" href="entries/reason_entry.html">reason</a>able to say that the success of such <a class="thought" href="entries/program_entry.html">program</a>s would vindicate 
              Turing's <a class="thought" href="entries/dream_entry.html">dream</a> of a <a class="thought" href="entries/computation_entry.html">computation</a>al <a class="thought" href="entries/intelligence_entry.html">intelligence</a>, rather than destroying 
              it.</p>
<p><a class="thought" href="entries/computation_entry.html">Computation</a> is such a valuable tool precisely because almost any 
              theory of cognitive mechanisms can be expressed in <a class="thought" href="entries/computation_entry.html">computation</a>al 
              terms, even though the relevant <a class="thought" href="entries/computation_entry.html">computation</a>al formalisms may vary. 
              All such theories are theories of causal organization, and <a class="thought" href="entries/computation_entry.html">computation</a> 
              is sufficiently flexible that it can capture almost any kind of 
              organization, whether the causal relations hold between high-level 
              representations or among low-level neural processes. Even such <a class="thought" href="entries/program_entry.html">program</a>s 
              as the Gibsonian theory of <a class="thought" href="entries/perception_entry.html">perception</a> are ultimately compatible 
              with minimal <a class="thought" href="entries/computation_entry.html">computation</a>alism. If <a class="thought" href="entries/perception_entry.html">perception</a> turns out to work as 
              the Gibsonians imagine, it will still be mediated by causal mechanisms, 
              and the mechanisms will be expressible in an appropriate <a class="thought" href="entries/computation_entry.html">computation</a>al 
              form. That <a class="thought" href="entries/expression_entry.html">expression</a> may look very unlike a traditional <a class="thought" href="entries/computation_entry.html">computation</a>al 
              theory of <a class="thought" href="entries/perception_entry.html">perception</a>, but it will be <a class="thought" href="entries/computation_entry.html">computation</a>al nevertheless.</p>
<p>In this <a class="thought" href="entries/light_entry.html">light</a>, we see that <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> and <a class="thought" href="entries/computation_entry.html">computation</a>al 
              <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> do not rest on shaky empirical hypotheses. Instead, 
              they are consequences of some very plausible principles about the 
              causal basis of cognition, and they are compatible with an extremely 
              wide range of empirical discoveries about the functioning of the 
              <a class="thought" href="entries/mind_entry.html">mind</a>. It is precisely because of this flexibility that <a class="thought" href="entries/computation_entry.html">computation</a> 
              serves as a <i>foundation</i> for the fields in question, by providing 
              a common framework within which many different theories can be expressed, 
              and by providing a tool with which the theories' causal mechanisms 
              can be instantiated. No <a class="thought" href="entries/matter_entry.html">matter</a> how <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a> <a class="thought" href="entries/progress_entry.html">progress</a>es 
              in the coming years, there is good <a class="thought" href="entries/reason_entry.html">reason</a> to believe that <a class="thought" href="entries/computation_entry.html">computation</a> 
              will be at center stage.</p>
<p><b>References</b><br>
              Armstrong, D.M. 1968. <i>A Materialist Theory of the <a class="thought" href="entries/mind_entry.html">Mind</a></i>. Routledge 
              and Kegan Paul. <br>
              Block, N. 1981. Psychologism and behaviorism. <i>Philosophical Review</i> 
              90:5-43. <br>
              Blum, L., Shub, M., and Smale, S. 1989. On a theory of <a class="thought" href="entries/computation_entry.html">computation</a> 
              and <a class="thought" href="entries/complexity_entry.html">complexity</a> over the real <a class="thought" href="entries/number_entry.html">number</a>s: NP-completeness, recursive 
              functions, and universal <a class="thought" href="entries/machine_entry.html">machine</a>s. <i>Bulletin (New Series) of the 
              American Mathematical <a class="thought" href="entries/society_entry.html">Society</a></i> 21(1):1-46. <br>
              Bowie, G. 1982. Lucas' <a class="thought" href="entries/number_entry.html">number</a> is finally up. <i>Journal of Philosophical 
              <a class="thought" href="entries/logic_entry.html">Logic</a></i> 11:279-85. <br>
              Chalmers, D.J. (1995). Absent qualia, fading qualia, dancing qualia. 
              In (T. Metzinger, ed) <i>Conscious <a class="thought" href="entries/experience_entry.html">Experience</a></i>. Ferdinand Schoningh. 
              <br>
              Chalmers, D.J. (1996a). Does a rock <a class="thought" href="entries/implement_entry.html">implement</a> every finite-state 
              automaton? I. <br>
              Chalmers, D.J. (1996b). <i>The Conscious Mind: In <a class="thought" href="entries/search_entry.html">Search</a> of a Fundamental 
              Theory</i>. Oxford University Press. Press. <br>
              Dietrich, E.S. 1990. <a class="thought" href="entries/computation_entry.html">Computation</a>alism. <i>Social <a class="thought" href="entries/epistemology_entry.html">Epistemology</a></i>. 
              <br>
              Dretske, F. 1981. <i><a class="thought" href="entries/knowledge_entry.html">Knowledge</a> and the Flow of <a class="thought" href="entries/information_entry.html">Information</a></i>. 
              MIT Press. <br>
              Dreyfus, H. 1972. <i>What <a class="thought" href="entries/computer_entry.html">Computer</a>s Can't Do</i>. Harper and Row. 
              <br>
              Edelman, G.M. 1989. <i>The Remembered Present: A <a class="thought" href="entries/biological_entry.html">Biological</a> Theory 
              of <a class="thought" href="entries/consciousness_entry.html">Consciousness</a></i>. <a class="thought" href="entries/basic_entry.html">Basic</a> Books. <br>
              Fodor, J.A. 1975. <i>The <a class="thought" href="entries/language_entry.html">Language</a> of <a class="thought" href="entries/thought_entry.html">Thought</a></i>. Harvard University 
              Press. <br>
              Fodor, J.A. 1987. <i>Psychosemantics: The Problem of Meaning in 
              the <a class="thought" href="entries/philosophy_entry.html">Philosophy</a> of <a class="thought" href="entries/mind_entry.html">Mind</a></i>. MIT Press. <br>
              Fodor, J.A. and Pylyshyn, Z.W. 1988. <a class="thought" href="entries/connectionism_entry.html">Connectionism</a> and cognitive 
              <a class="thought" href="entries/architecture_entry.html">architecture</a>. <i>Cognition</i> 28:3-71. <br>
              Fodor, J.A. 1992. The big idea: Can there be a <a class="thought" href="entries/science_entry.html">science</a> of mind? 
              <i>Times Literary Supplement</i> 4567:5-7 (July 3, 1992). <br>
              Gibson, J. 1979. <i>The Ecological Approach to Visual <a class="thought" href="entries/perception_entry.html">Perception</a></i>. 
              Houghton Mifflin. <br>
              Harnad, S. 1989. Minds, <a class="thought" href="entries/machine_entry.html">machine</a>s and Searle. <i>Journal of <a class="thought" href="entries/experiment_entry.html">Experiment</a>al 
              and Theoretical <a class="thought" href="entries/ai_entry.html">Artificial Intelligence</a></i> 1:5-25. <br>
              Haugeland, J. 1985. <i><a class="thought" href="entries/ai_entry.html">Artificial intelligence</a>: The Very Idea</i>. 
              MIT Press. <br>
              Lewis, D. 1972. Psychophysical and theoretical identifications. 
              <i>Australasian Journal of <a class="thought" href="entries/philosophy_entry.html">Philosophy</a></i> 50:249-58. <br>
              Lucas, J.R. 1963. Minds, <a class="thought" href="entries/machine_entry.html">machine</a>s, and G&#246;del. <i><a class="thought" href="entries/philosophy_entry.html">Philosophy</a></i> 
              36:112-127. <br>
              MacLennan, B. 1990. Field <a class="thought" href="entries/computation_entry.html">computation</a>: A theoretical framework for 
              massively parallel <a class="thought" href="entries/analog_entry.html">analog</a> <a class="thought" href="entries/computation_entry.html">computation</a>, Parts I - IV. Technical Report 
              CS-90-100. <a class="thought" href="entries/computer_science_entry.html">Computer Science</a> Department, University of Tennessee. 
              <br>
              Nagel, T. 1974. What is it like to be a bat? <i>Philosophical Review</i> 
              4:435-50. <br>
              Newell, A. and Simon, H.A. 1981. <a class="thought" href="entries/computer_science_entry.html">Computer science</a> as empirical inquiry: 
              <a class="thought" href="entries/symbol_entry.html">Symbol</a>s and <a class="thought" href="entries/search_entry.html">search</a>. <i><a class="thought" href="entries/communication_entry.html">Communication</a>s of the Association for Computing 
              <a class="thought" href="entries/machine_entry.html">Machine</a>ry</i> 19:113-26. <br>
              Penrose, R. 1989. <i>The Emperor's New Mind: Concerning <a class="thought" href="entries/computer_entry.html">computer</a>s, 
              minds, and the laws of <a class="thought" href="entries/physics_entry.html">physics</a></i>. Oxford University Press. <br>
              Penrose, R. 1990. Precis of The Emperor's New <a class="thought" href="entries/mind_entry.html">Mind</a>. <i>Behavioral 
              and <a class="thought" href="entries/brain_entry.html">Brain</a> <a class="thought" href="entries/science_entry.html">Science</a>s</i> 13:643-655. <br>
              Pour-El, M.B., and Richards, J.I. 1989. <i>Computability in Analysis 
              and <a class="thought" href="entries/physics_entry.html">Physics</a></i>. Springer-Verlag. <br>
              Putnam, H. 1960. Minds and <a class="thought" href="entries/machine_entry.html">machine</a>s. In (S. Hook, ed.) <i>Dimensions 
              of <a class="thought" href="entries/mind_entry.html">Mind</a></i>. New York University Press. <br>
              Putnam, H. 1967. The <a class="thought" href="entries/nature_entry.html">nature</a> of mental states. In (W.H. Capitan and 
              D.D. Merrill, eds.) <i><a class="thought" href="entries/art_entry.html">Art</a>, <a class="thought" href="entries/mind_entry.html">Mind</a>, and <a class="thought" href="entries/religion_entry.html">Religion</a></i>. University of 
              Pittsburgh Press. <br>
              Putnam, H. 1988. <i>Representation and <a class="thought" href="entries/reality_entry.html">Reality</a></i>. MIT Press. <br>
              Pylyshyn, Z.W. 1984. <i><a class="thought" href="entries/computation_entry.html">Computation</a> and Cognition: Toward a Foundation 
              for <a class="thought" href="entries/cognitive_science_entry.html">Cognitive Science</a></i>. MIT Press. <br>
              Searle, J.R. 1980. Minds, brains and <a class="thought" href="entries/program_entry.html">program</a>s. <i>Behavioral and 
              <a class="thought" href="entries/brain_entry.html">Brain</a> <a class="thought" href="entries/science_entry.html">Science</a>s</i> 3:417-57. <br>
              Searle, J.R. 1984. <i>Minds, brains, and <a class="thought" href="entries/science_entry.html">science</a></i>. Harvard University 
              Press. <br>
              Searle, J.R. 1990. Is the <a class="thought" href="entries/brain_entry.html">brain</a> a <a class="thought" href="entries/digital_entry.html">digital</a> <a class="thought" href="entries/computer_entry.html">computer</a>? <i>Proceedings 
              and Addresses of the American Philosophical Association</i> 64:21-37. 
              <br>
              Searle, J.R. 1991. <i>The Rediscovery of the <a class="thought" href="entries/mind_entry.html">Mind</a></i>. MIT Press. 
              <br>
              Smolensky, P. 1988. On the proper treatment of <a class="thought" href="entries/connectionism_entry.html">connectionism</a>. <i>Behavioral 
              and <a class="thought" href="entries/brain_entry.html">Brain</a> <a class="thought" href="entries/science_entry.html">Science</a>s</i> 11:1-23. <br>
              Turing, A.M. 1936. On computable <a class="thought" href="entries/number_entry.html">number</a>s, with an application to 
              the Entscheidungsproblem. <i>Proceedings of the London Mathematical 
              <a class="thought" href="entries/society_entry.html">Society</a>, Series 2</i> 42: 230-65.</p>
<p>Notes<br>
<sup>1</sup>. I take it that something like this is the "standard" 
              definition of <a class="thought" href="entries/implement_entry.html">implement</a>ation of a finite-state automaton; see, for 
              example, the definition of the description of a <a class="thought" href="entries/system_entry.html">system</a> by a probabilistic 
              automaton in Putnam (1967). It is surprising, however, how little 
              <a class="thought" href="entries/space_entry.html">space</a> has been devoted to accounts of <a class="thought" href="entries/implement_entry.html">implement</a>ation in the literature 
              in theoretical <a class="thought" href="entries/computer_science_entry.html">computer science</a>, <a class="thought" href="entries/philosophy_entry.html">philosophy</a> of <a class="thought" href="entries/psychology_entry.html">psychology</a>, and cognitive 
              <a class="thought" href="entries/science_entry.html">science</a>, considering how central the notion of <a class="thought" href="entries/computation_entry.html">computation</a> is to 
              these fields. It is remarkable that there could be a controversy 
              about what it takes for a physical <a class="thought" href="entries/system_entry.html">system</a> to <a class="thought" href="entries/implement_entry.html">implement</a> a <a class="thought" href="entries/computation_entry.html">computation</a> 
              (e.g. Searle 1990, 1991) at this late date. </p>
<p><sup>2</sup>. See Pylyshyn 1984, p. 71, for a related point. </p>
<p><sup>3</sup>. In analyzing a related <a class="thought" href="entries/thought_entry.html">thought</a>-<a class="thought" href="entries/experiment_entry.html">experiment</a>, Searle 
              (1991) suggests that a subject who has undergone <a class="thought" href="entries/silicon_entry.html">silicon</a> replacement 
              might react as follows: "You want to cry out, 'I can't see 
              anything. I'm going totally blind'. But you hear your voice saying 
              in a way that is completely out of your control, 'I see a red <a class="thought" href="entries/object_entry.html">object</a> 
              in front of me'" (pp. 66-67). But given that the <a class="thought" href="entries/system_entry.html">system</a>'s causal 
              topology remains constant, it is very unclear where there is room 
              for such "wanting" to take place, if it is not in some 
              Cartesian realm. Searle suggests some other things that might happen, 
              such as a <a class="thought" href="entries/reduction_entry.html">reduction</a> to total paralysis, but these suggestions require 
              a change in causal topology and are therefore not relevant to the 
              issue of organizational invariance. </p>
<p><sup>4</sup>. I am skeptical about whether phenomenal properties 
              can be explained in wholly physical terms. As I argue in Chalmers 
              (forthcoming <a class="thought" href="entries/c_entry.html">c</a>), given any account of the physical or <a class="thought" href="entries/computation_entry.html">computation</a>al 
              processes underlying mentality, the question of why these processes 
              should give rise to conscious <a class="thought" href="entries/experience_entry.html">experience</a> does not seem to be explainable 
              within physical or <a class="thought" href="entries/computation_entry.html">computation</a>al theory alone. Nevertheless, it 
              remains the case that phenomenal properties <i>depend</i> on physical 
              properties, and if what I have said earlier is correct, the physical 
              properties that they depend on are organizational properties. Further, 
              the explanatory gap with respect to conscious <a class="thought" href="entries/experience_entry.html">experience</a> is compatible 
              with the <a class="thought" href="entries/computation_entry.html">computation</a>al explanation of cognitive processes and of 
              behavior, which is what the thesis of <a class="thought" href="entries/computation_entry.html">computation</a>al explanation 
              requires. </p>
<p><sup>5</sup>. Of course there is a <a class="thought" href="entries/sense_entry.html">sense</a> in which it can be said 
              that connectionist models perform "computation over representation", 
              in that connectionist processing involves the transformation of 
              representations, but this <a class="thought" href="entries/sense_entry.html">sense</a> is to weak to cut the distinction 
              between <a class="thought" href="entries/symbol_entry.html">symbol</a>ic and subsymbolic <a class="thought" href="entries/computation_entry.html">computation</a> at its joints. Perhaps 
              the most interesting foundational distinction between <a class="thought" href="entries/symbol_entry.html">symbol</a>ic and 
              connectionist <a class="thought" href="entries/system_entry.html">system</a>s is that in the former but not in the latter, 
              the <a class="thought" href="entries/computation_entry.html">computation</a>al (syntactic) primitives are also the representational 
              (semantic) primitives. </p>
<p><sup>6</sup>. It is common for proponents of <a class="thought" href="entries/symbol_entry.html">symbol</a>ic <a class="thought" href="entries/computation_entry.html">computation</a>alism 
              to hold, usually as an unargued premise, that what makes a <a class="thought" href="entries/computation_entry.html">computation</a> 
              a <a class="thought" href="entries/computation_entry.html">computation</a> is the fact that it involves representations with 
              semantic <a class="thought" href="entries/content_entry.html">content</a>. The books by Fodor (1975) and Pylyshyn (1984), 
              for <a class="thought" href="entries/instance_entry.html">instance</a>, are both premised on the assumption that there is 
              no <a class="thought" href="entries/computation_entry.html">computation</a> without representation. Of course this is to some 
              extent a terminological issue, but as I have stressed in 2.2 and 
              here, this assumption has no basis in <a class="thought" href="entries/computation_entry.html">computation</a>al theory and unduly 
              restricts the role that <a class="thought" href="entries/computation_entry.html">computation</a> plays in the foundations of 
              <a class="thought" href="entries/cognitive_science_entry.html">cognitive science</a>. </p>
<p><sup>7</sup>. Some other claims with which <a class="thought" href="entries/computation_entry.html">computation</a>alism is 
              sometimes associated include "the <a class="thought" href="entries/brain_entry.html">brain</a> is a <a class="thought" href="entries/computer_entry.html">computer</a>", 
              "the <a class="thought" href="entries/mind_entry.html">mind</a> is to the <a class="thought" href="entries/brain_entry.html">brain</a> as <a class="thought" href="entries/software_entry.html">software</a> is to <a class="thought" href="entries/hardware_entry.html">hardware</a>", 
              and "cognition is <a class="thought" href="entries/computation_entry.html">computation</a>". The first of these is 
              not required, for the <a class="thought" href="entries/reason_entry.html">reason</a>s given in 2.2: it is not <a class="thought" href="entries/computer_entry.html">computer</a>s 
              that are central to cognitive theory but <a class="thought" href="entries/computation_entry.html">computation</a>s. The second 
              claim is an imperfect <a class="thought" href="entries/expression_entry.html">expression</a> of the <a class="thought" href="entries/computation_entry.html">computation</a>alist position 
              for similar <a class="thought" href="entries/reason_entry.html">reason</a>s: certainly the <a class="thought" href="entries/mind_entry.html">mind</a> does not seem to be something 
              separable that the <a class="thought" href="entries/brain_entry.html">brain</a> can load and run, as a <a class="thought" href="entries/computer_entry.html">computer</a>'s <a class="thought" href="entries/hardware_entry.html">hardware</a> 
              can load and run <a class="thought" href="entries/software_entry.html">software</a>. Even the third does not seem to me to 
              be central to <a class="thought" href="entries/computation_entry.html">computation</a>alism: perhaps there is a <a class="thought" href="entries/sense_entry.html">sense</a> in which 
              it is true, but what is more <a class="thought" href="entries/import_entry.html">import</a>ant is that <a class="thought" href="entries/computation_entry.html">computation</a> suffices 
              for and explains cognition. See Dietrich (1990) for some related 
              distinctions between <a class="thought" href="entries/computation_entry.html">computation</a>alism, "computerism", 
              and "cognitivism". </p>
<p><a class="thought" href="entries/copyright_entry.html">Copyright</a> &#169; 1994 by David Chalmers. Used with permission.</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D7419" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id7420"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>A computation behind the mind<br><span class="mindxheader"><i>posted on 06/11/2002 5:33 AM by tomaz@techemail.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id7420" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D7420" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>A mandatory reading, if you ask me. Instead of  time wasting with some 'common sense sceptics'* - just try to understand this.
<br>
<br>
- Thomas
<br>
<br>
-----------
<br>
* What do you have of those "how could an evolution ever lead to such marvelous ..." sayers. The "how could a computation ever lead to such marvelous ..." sayers - are their brothers, sisters and close cousins.
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id44980"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Causal computations<br><span class="mindxheader"><i>posted on 08/08/2005 5:27 AM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=2102">rpoz99</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id44980" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D44980" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>For how combinatorial-state automata can be defined and operate my own papers on causal computations can serve as an example: <a href="http://web.archive.org/web/20071011224129/http://sun1000.pwr.wroc.pl/~rpoz" target="_blank">http://sun1000.pwr.wroc.pl/~rpoz</a>
<br>
<br>
---
<br>
<br>
Roman
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id45013"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Causal computations<br><span class="mindxheader"><i>posted on 08/08/2005 6:48 PM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=471">eldras</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id45013" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D45013" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Brilliant work, thanks! Refreshing to read your stuff.
<br>
<br>
I heard David lecturing but had to leave the lecture as it was too basic to believe. I was really disppointed.
<br>
<br>
People think in boxes.
<br>
<br>
<br>
Of course there are different paradigms for perspective and what matters is how we deduce and coax out the technology to end suffering.
<br>
<br>
Whether you have a striuctiral or causal approach or a matrox appoach or a hierachical, there is motion and how that motion effects your own goals is the issue as you build something.
<br>
<br>
I think you must be influenced by Turing. Have you read his metamorphosis paper?
<br>
<br>
Neural networks are just one way of organising a whole lot of calculating systems.
<br>
<br>
<br>
Cheers
<br>
<br>
Eldras</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id45207"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Causal computations<br><span class="mindxheader"><i>posted on 08/10/2005 5:42 PM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=2102">rpoz99</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id45207" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D45207" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I have studied Turing work trying to understand what is so powerful in his machine, why it is claimed that it is more powerful than other automata since all operate in the world of limited resources? Then I have studied neural networks and it seems that they can perform the same processing but in a completely different way. The new paradigm says that there can be different ways of keeping computations causal, not only structural way, but at present we know very little about them. I have shown just one approach and neural networks are just one example of implementation. I think more effective will be holographic associative memories or something like this.
<br>
<br>
The causal, unstructural approach offers a simple explanation of what semantics is (unfortunately, no paper on this). Meaning emerges when one syntactic process is used to describe other - it is relative depending on what syntax and alphabet we use to describe the same causal processes. 
<br>
<br>
---
<br>
<br>
Bye, Roman</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id46072"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="60"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="619"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Causal computations<br><span class="mindxheader"><i>posted on 08/24/2005 11:08 PM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=471">eldras</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id46072" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D46072" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>There are many approaches Roman.
<br>
<br>
Nor are their finite limits for them
<br>
<br>
eg there is infinite regree, outward expension (inflation theory)
<br>
building new universes, dimension theory, then mucking about with the universal wavelength.
<br>
<br>
that's before we begin on symbolic representation
<br>
<br>
<br>
<br>
Cheers
<br>
<br>
Eldras</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id86232"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: A Computational Foundation for the Study of Cognition<br><span class="mindxheader"><i>posted on 09/16/2007 8:49 AM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id86232" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D86232" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>If being looked at as philosophy, this attempt is too metaphysical, but might be useful.
<br>
<br>
From the practical AI point of view, its weaknesses and fussiness become critically self-destructive.
<br>
<br>
The consciousness is an abstraction of the processing behind certain level of operational abilities, in general of perception-reaction-action abilities.
<br>
<br>
The internal workings of consciousness are, in general, deducible from that.
<br>
<br>
:)
<br>
<br>
es
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id86235"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: A Computational Foundation for the Study of Cognition<br><span class="mindxheader"><i>posted on 09/16/2007 11:01 AM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=2395">doojie</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id86235" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D86235" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p> We seem to be approaching some critical understandings. One thing that has bothered me recently is the nature of consciousness as "What it is to be like" something.
<br>
<br>
 The assumption being that if we model it colsely enough, then we essentially have THE TRUE THING.
<br>
<br>
 But the attempt at modeling, by whatever model we use, from computation to ancient religion, is a still a model.
<br>
<br>
 Even the search for the true religion reflects this essential flaw. If we can perfect the processes of organization to the degree that we can simulate it to a near perfect condition, then we have religious truth.
<br>
<br>
 Problem is, that attempt at the perfect simulation of truth results in a speciation that represents a different simulation.
<br>
<br>
 Is there an important difference between simulation and reality or between simulation and truth? Clearly there is something in our minds that think this to be true because of the eternal speciation of mental representations.
<br>
<br>
 So, if consciousness is explained as "what it is to be like" something, then consciousness itself will be the process causing speciation, since it cannot differentiate between simulation and reality. The "deception" lies at the level of consciousness or mind.
<br>
<br>
 It seems to me that we're exploring the truth of the Church-Turing thesis. Is there an equivalence between computers and the brain?
<br>
<br>
 The algorithms deciding functions at the neural level suggests that the brain can be modeled precisely at some point, but would such precise modeling lead to the kind of simulation that results in our own mind?
<br>
<br>
 This seems to lead back to Minsky's conclusion that there is a "cloaking device" at work by which we cannot backtrack to the mathematically precise firings of our neurons. What we desire to do, we cannot do, since doing so would require a fundamental change in those neural firings.
<br>
<br>
 What we call consciousness is itself a simulation, incomplete by nature and allowing for choices among alternatives, some of which are successful and some of which are not, being an extension of an evolutionary process.
<br>
<br>
 I still see a problem with this, extending from Godel's and Chaitin's conclusions. Based on mathematical models, we are confrontd with Godel's incompleteness, and further with the realzation that in any axiomatic system, there exists an infinity of undecideable propositions.
<br>
<br>
 This suggests that no simulation can ever model reality truthfully, which means that the best laid plans of mice and men will go awry.
<br>
<br>
 And if consciousness itself is a simulation, it will never provide a truthful model of reality, so that organization of knowledge is always subject to change.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id86241"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: A Computational Foundation for the Study of Cognition<br><span class="mindxheader"><i>posted on 09/16/2007 11:59 AM by <a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D7419%23id86241" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20071011224129/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D86241" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>There is an infinite number of ways, that consciousness can be implemented with/by/over.
<br>
<br>
Because of that, a search for a machinery/algorithm definition is off the mark.
<br>
<br>
The only definite thing about consciousness, is that it must support perception-cognition-action on the appropriately defined level.
<br>
<br>
es
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20071011224129im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>