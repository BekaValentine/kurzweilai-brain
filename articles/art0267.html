<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>When Will HAL Understand What We Are Saying? Computer Speech Recognition and Understanding</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/meme/memelist.html?m=3">How to Build a Brain</a> &gt; 
When Will HAL Understand What We Are Saying? Computer Speech Recognition and Understanding
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20100613223450/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0267.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0267.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/articles/art0267.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">When Will HAL Understand What We Are Saying? Computer Speech Recognition and Understanding</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td><td><span class="Authors"> <a class="Authors" href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0005.html" target="_top">    Raymond   Kurzweil </a><br>
</span></td>
</table>
<br>
<div class="TeaserText">This chapter from "<a class="thought" href="entries/hal_entry.html">HAL</a>'s Legacy: 2001's <a class="thought" href="entries/computer_entry.html">Computer</a> as <a class="thought" href="entries/dream_entry.html">Dream</a> and <a class="thought" href="entries/reality_entry.html">Reality</a>" addresses the accomplishments--and challenges--of <a class="thought" href="entries/automated_speech_rec_entry.html">automatic speech recognition</a>. What kind of <a class="thought" href="entries/paradigm_shift_entry.html">paradigm shift</a> in computing will give <a class="thought" href="entries/hal_entry.html">HAL</a> the ability to understand human <a class="thought" href="entries/context_entry.html">context</a>, and therefore truly speak?</div>
<br>
<br>
<p>Originally published 1996 in <a href="http://web.archive.org/web/20100613223450/http://mitpress.mit.edu/e-books/Hal/" target="_new">Hal's Legacy: 2001's Computer as Dream and Reality</a>. Published on KurzweilAI.net August 6, 2001.</p>
<p>
<i>Let's talk about how to wreck a nice beach.</i>
</p>
<p>Well, actually, if I were presenting this chapter verbally, you would have little difficulty understanding the preceding sentence as <i>Let's talk about how to recognize speech</i>. Of course, I wouldn't have enunciated the <i>g</i> in <i>recognize</i>, but then we routinely leave out and otherwise slur at least a quarter of the sounds that are "supposed" to be there a <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> speech scientists call coarticulation.</p>
<p>On the other hand, had this been an article on a rowdy headbangers' beach convention (a topic we assume <a class="thought" href="entries/hal_entry.html">HAL</a> knew little about), the interpretation at the beginning of the chapter would have been reasonable.</p>
<p>On yet another hand, if you were a researcher in speech recognition and heard me read the first sentence of this chapter, you would immediately pick up the beach--wrecking interpretation, because this sentence is a famous example of acoustic ambiguity and is frequently cited by speech researchers.</p>
<p>The point is that we understand speech in <a class="thought" href="entries/context_entry.html">context</a>. Spoken <a class="thought" href="entries/language_entry.html">language</a> is filled with ambiguities. Only our understanding of the situation, subject <a class="thought" href="entries/matter_entry.html">matter</a>, and person (or <a class="thought" href="entries/entity_entry.html">entity</a>) speaking--as well as our familiarity with the speaker--lets us infer what words are actually spoken.</p>
<p>Perhaps the most basic ambiguity in spoken <a class="thought" href="entries/language_entry.html">language</a> is the <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> of <i>homonyms</i>, words that sound absolutely identical but are actually different words with different meanings. When Frank asks, "Listen, <a class="thought" href="entries/hal_entry.html">HAL</a>, there's never been any <a class="thought" href="entries/instance_entry.html">instance</a> at all of a <a class="thought" href="entries/computer_entry.html">computer</a> error occurring in a 9000 series, has there?", <a class="thought" href="entries/hal_entry.html">HAL</a> has little difficulty interpreting the last word as <i>there</i> and not <i>their</i>. <a class="thought" href="entries/context_entry.html">Context</a> is the only source of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> that can resolve such ambiguities. <a class="thought" href="entries/hal_entry.html">HAL</a> understands that the word their is an adjective and would have to be followed by the noun it modifies. Because it is the last word in the sentence, there is the only reasonable interpretation. Today's speech--recognition systems would also have little difficulty with this word and would resolve it the same way <a class="thought" href="entries/hal_entry.html">HAL</a> does.</p>
<p>A more difficult task in interpreting Frank's statement is the word all. Is all a place ; such as <a class="thought" href="entries/ibm_entry.html">IBM</a> headquarters--where a <a class="thought" href="entries/computer_entry.html">computer</a> error may take place, as in "there's never been any <a class="thought" href="entries/instance_entry.html">instance</a> of a <a class="thought" href="entries/computer_entry.html">computer</a> error at <a class="thought" href="entries/ibm_entry.html">IBM</a> ... " <a class="thought" href="entries/hal_entry.html">HAL</a> resolves this ambiguity the same way viewers of the movie do. We know that all is not the name of a place or organization where an error may take place. This leaves us with <i>at all</i> as an <a class="thought" href="entries/expression_entry.html">expression</a> of emphasis reinforcing the meaning of never as the only likely interpretation.</p>
<p>In fact, we try to understand what is being said before the words are even spoken, through a process called <i>hypothesis</i><i> and test</i>. Next time you order coffee in a restaurant and a waiter asks how you want it, try saying "I'd like some dream and sugar please." It would take a rather attentive person to hear that you are talking about sweet dreams and not white coffee.</p>
<p>When we listen to other people talking--and people frequently do not really listen, a fault that <a class="thought" href="entries/hal_entry.html">HAL</a> does not seem to share with the rest of us--we constantly anticipate what they are going to say ... next. Consider Dave's reply to <a class="thought" href="entries/hal_entry.html">HAL</a>'s questions about the crew <a class="thought" href="entries/psychology_entry.html">psychology</a> report:</p>
<p>Dave: Well, I don't know. That's rather a difficult question to ...</p>
<p></p>
<p>When Dave finally says the answer, <a class="thought" href="entries/hal_entry.html">HAL</a> tests his <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a> by matching the word he heard against the word he had hypothesized Dave would say. In watching the movie, we all do the same thing. Any reasonable match would tend to confirm our expectation The test involves an acoustic matching process, but the <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a> has nothing to do with sound at all--nor even with <a class="thought" href="entries/language_entry.html">language</a>--but rather relates to <a class="thought" href="entries/knowledge_entry.html">knowledge</a> on a multiplicity of levels. As many of the chapters in this book point out, <a class="thought" href="entries/knowledge_entry.html">knowledge</a> goes far beyond mere facts and data. For <i>information</i> to become <i>knowledge</i>, it must incorporate the relationships between ideas. And for <a class="thought" href="entries/knowledge_entry.html">knowledge</a> to be useful, the links describing how concepts interact must be easily accessed, updated, and manipulated. <a class="thought" href="entries/human_entry.html">Human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is remarkable in its ability to perform all these tasks. Ironically, it is also remarkably weak at reliably storing the <a class="thought" href="entries/information_entry.html">information</a> on which <a class="thought" href="entries/knowledge_entry.html">knowledge</a> is based. The natural strengths of today's computers are roughly the opposite. They have, therefore, become powerful allies of the human intellect because of their ability to reliably store and rapidly retrieve vast quantities of <a class="thought" href="entries/information_entry.html">information</a>. Conversely, they have been slow to master true <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. Modeling the <a class="thought" href="entries/knowledge_entry.html">knowledge</a> needed to understand the highly ambiguous and variable <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> of human speech has been a primary key to making <a class="thought" href="entries/progress_entry.html">progress</a> in the field of <a class="thought" href="entries/automated_speech_rec_entry.html">automatic speech recognition</a> (<a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>). </p><h1>Lesson 1: <a class="thought" href="entries/knowledge_entry.html">Knowledge</a> Is a Many--layered Thing </h1><p>Thus lesson <a class="thought" href="entries/number_entry.html">number</a> one for constructing a <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/system_entry.html">system</a> that can understand human speech is to build ;in <a class="thought" href="entries/knowledge_entry.html">knowledge</a> at many levels: the <a class="thought" href="entries/structure_entry.html">structure</a> of speech sounds, the way speech is produced by our vocal apparatus, the patterns of speech sounds that comprise dialects and languages, the complex (and not fully understood) rules of word usage, and the--greatest difficulty--general <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the subject <a class="thought" href="entries/matter_entry.html">matter</a> being spoken about.</p>
<p>Each level of analysis provides useful constraints that can limit our <a class="thought" href="entries/search_entry.html">search</a> for the right answer. For example, the basic building blocks of speech called <i>phonemes</i> cannot appear in just any order. Indeed, many sequences are impossible to articulate (try saying <i>ptkee</i>). More important, only certain <a class="thought" href="entries/phoneme_entry.html">phoneme</a> sequences correspond to a word or word fragment in the <a class="thought" href="entries/language_entry.html">language</a>. Although the set of phonemes used is similar (although not identical) from one <a class="thought" href="entries/language_entry.html">language</a> to another, contextual factors differ dramatically. English, for example, has over ten thousand possible syllables, whereas Japanese has only a hundred and twenty.</p>
<p>On a higher level, the <a class="thought" href="entries/syntax_entry.html">syntax</a> and <a class="thought" href="entries/semantics_entry.html">semantics</a> of the <a class="thought" href="entries/language_entry.html">language</a> put constraints on possible word orders. Resolving homonym ambiguities can require multiple levels of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. One type of <a class="thought" href="entries/technology_entry.html">technology</a> frequently used in speech recognition and understanding systems is a sentence <a class="thought" href="entries/parser_entry.html">parser</a>, which builds sentence diagrams like those we learned in elementary school. One of the first such systems, developed in 1963 by Susumu Kuno of Harvard (around the time Kubrick and Clarke began work on <i>2001</i>), revealed the depth of ambiguity in English. Kuno asked his computerized <a class="thought" href="entries/parser_entry.html">parser</a> what the sentence "<a class="thought" href="entries/time_entry.html">Time</a> flies like an arrow" means. In what has become a famous response, the <a class="thought" href="entries/computer_entry.html">computer</a> replied that it was not quite sure. It might mean</p>
<p>1. That time passes as quickly as an arrow passes.</p>
<p>2. Or maybe it is a command telling us to time the flies the same way that an arrow times flies; that is, <a class="thought" href="entries/time_entry.html">Time</a> flies like an arrow would.</p>
<p>3. Or it could be a command telling us to time only those flies that are similar to arrows; that is, <a class="thought" href="entries/time_entry.html">Time</a> flies that are like an arrow.</p>
<p>4. Or perhaps it means that a type of flies known as time flies have a fondness for arrows: <a class="thought" href="entries/time_entry.html">Time</a>--flies like (i.e., appreciate) an arrow."</p>
<p>It became clear from this and other syntactical ambiguities that understanding <a class="thought" href="entries/language_entry.html">language</a>, spoken or written, requires both <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the relationships between words and of the concepts underlying words. It is impossible to understand the sentence about time (or even to understand that the sentence is indeed talking about time and not flies) without mastery of the <a class="thought" href="entries/knowledge_entry.html">knowledge</a> structures that represent what we know about time, flies, arrows, and how these concepts relate to one another</p>
<p>A <a class="thought" href="entries/system_entry.html">system</a> armed with this type of <a class="thought" href="entries/information_entry.html">information</a> would know that flies are not similar to arrows and would thus knock out the third interpretation. Often there is more than one way to resolve <a class="thought" href="entries/language_entry.html">language</a> ambiguities. The third interpretation could be syntactically resolved by noting that <i>like</i> in the sense of <i>similar to</i> ordinarily requires <a class="thought" href="entries/number_entry.html">number</a> agreement between the two objects compared. Such a <a class="thought" href="entries/system_entry.html">system</a> would also note that, as there are no such things as <i>time flies</i>, the fourth interpretation too is wrong. The <a class="thought" href="entries/system_entry.html">system</a> would also need such tidbits of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> as the fact that flies have never shown a fondness for arrows, and that arrows cannot and do not <i>time</i> anything--much less flies--to select the first interpretation as the only plausible one. The ambiguity of <a class="thought" href="entries/language_entry.html">language</a>, however, is far greater than this example suggests. In a <a class="thought" href="entries/language_entry.html">language</a>--parsing project at the MIT Speech Lab, Ken Church found a sentence with over two million syntactically correct interpretations.</p>
<p>Often the tidbits of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> we need have to do with the specific situation of speakers and listeners. If I walk into my business associate's office and say "rook to king one," I am likely to get a response along the lines of "excuse me?" Even if my words were understood, their meaning would still be unclear; my associate would probably interpret them as a sarcastic remark implying that I think he regards himself as a king. In the <a class="thought" href="entries/context_entry.html">context</a> of a chess game, however, not only is the meaning clear, but the words are easy to recognize. Indeed, our contemporary speech--recognition systems do a very good job when the <a class="thought" href="entries/domain_entry.html">domain</a> of discourse is restricted to something as narrow as a chess game. So, <a class="thought" href="entries/hal_entry.html">HAL</a>, too, has little trouble understanding when Frank says "rook to king one" during one of their chess matches. </p><h1>Lesson 2: The Unpredictability of <a class="thought" href="entries/human_entry.html">Human</a> Speech </h1><p>A second lesson for building our <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/system_entry.html">system</a> is that it must be capable of understanding the variability of human speech. We can, of course, build in pictures of human speech called spectrograms, which plot the intensity of different frequencies (or pitches in human perceptual terms) as they change over time. What is interesting, and--for those of us developing speech- recognition machines--daunting is that spectrograms of two people saying the same word can look dramatically different. Even the same person pronouncing the same word at different times can produce quite different spectrograms.</p>
<p>Look at the two spectrogram pictures of Dave and Frank saying the word <i>HAL</i>. It would be difficult to know that they are saying the same word from the pictures alone. Yet the spectrograms present all the <a class="thought" href="entries/salient_entry.html">salient</a> <a class="thought" href="entries/information_entry.html">information</a> in the speech signals.</p>
<p>Yet, there must be something about these different sound pictures that is the same; otherwise we humans and <a class="thought" href="entries/hal_entry.html">HAL</a>, as a human-level <a class="thought" href="entries/machine_entry.html">machine</a>, would be unable to identify them as two examples of the same spoken word. Thus, one key to building <a class="thought" href="entries/automated_speech_rec_entry.html">automatic speech recognition</a> (<a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>) machines is the <a class="thought" href="entries/search_entry.html">search</a> for these <i>invariant features</i>. We note for example that vowel sounds (e.g., the <i>a</i> sound in <i>HAL</i>, which may be denoted as <i>&#230;</i> (or /<i>a</i>/) involve certain resonant frequencies called <i>formants</i> that are sustained over some tens of milliseconds. We tend to find these formants in a certain mathematical relationship whenever /<i>a</i>/ is spoken. The same is true of the other sustained vowels. (Although the relationship is not a simple one, we observe that the relationship of the frequency of the second formant to the first formant for a particular vowel falls within a certain range, with some overlap between the ranges for different vowels.) Speech recognition systems frequently include a <a class="thought" href="entries/search_entry.html">search</a> function for finding these relationships, sometimes called features.</p>
<p>By studying spectrograms, we also note that certain changes do not convey any <a class="thought" href="entries/information_entry.html">information</a>; that is, there are types of changes we should filter out and ignore. An obvious one is loudness. When Dave shouts <a class="thought" href="entries/hal_entry.html">HAL</a>'s name in the pod in space, <a class="thought" href="entries/hal_entry.html">HAL</a> realizes it is still his name. <a class="thought" href="entries/hal_entry.html">HAL</a> infers some meaning from Dave's volume, but it is relatively unimportant for identifying the words being spoken. We apply, vtherefore, a process called <i>normalization</i>, in which we make all words the same loudness so as to eliminate this noninformative source of variability.</p>
<p>A more complex example is the <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a> of nonlinear time compression. When we speak, we change our speed according to <a class="thought" href="entries/context_entry.html">context</a> and other factors. If we speak one word more quickly, we do not increase the rate evenly throughout the entire word. The duration of certain portions of the word, such as plosive consonants (e.g., /<i>p</i>/, /<i>b</i>/, /<i>t</i>/), remains fairly constant, while other portions, such as vowels, undergo most of the change. In matching a spoken word to a stored example (a <i>template</i>), we need to align corresponding acoustic events or the match will never succeed. A mathematical technique called <i>dynamic programming</i> solves this temporal alignment. </p><h1>Lesson 3: Speech Is Like a Song </h1><p>A third lesson is also apparent from studying speech spectrograms. It is that the perceptual cues needed to identify speech sounds and assemble words are found in the frequency <a class="thought" href="entries/domain_entry.html">domain</a>, and not in the original time-varying signal. To make sense of it, we need to convert the original waveform into its frequency components. The human vocal tract is similar to a musical <a class="thought" href="entries/instrument_entry.html">instrument</a> (indeed it <i>is</i> a musical <a class="thought" href="entries/instrument_entry.html">instrument</a>). The vocal cords vibrate, creating a characteristic pitched sound; the length and tautness of the cords determine pitch in the same way that the length and tautness of a violin or piano string does. We can control the tautness of our vocal cords--as we do when singing--and alter the overtones produced by our vibrating cords by moving our tongue, teeth, and lips--which change the shape of the vocal tract. The vocal tract is a chamber that acts like a pipe in a pipe organ, the harmonic resonances of which emphasize certain overtones and diminish others. Finally, we control a small piece of tissue called the alveolar flap (or soft palate), which opens and closes the nasal cavity. When the alveolar flap is open, the nasal cavity adds an additional resonant chamber; it's a lot like opening another organ pipe. (Viewers of <i>My Fair Lady</i> will recall that the anatomy of speech recognition is also an important topic for specializts in phonetics.)</p>
<p>In addition to the pitched sound produced by the vocal cords, we can produce a noiselike sound by the rush of air through the speech cavity. This sound does not have specific overtones but is a complex spectrum of many frequencies mixed together. Like the musical tones produced by the vocal cords, the spectra of these noise sounds are shaped by the changing resonances of the moving vocal tract.</p>
<p>This vocal apparatus allows us to create the varied sounds that comprise human speech. Although many animals communicate with others of their <a class="thought" href="entries/species_entry.html">species</a> through sound, we humans are unique in our ability to shape that sound into <a class="thought" href="entries/language_entry.html">language</a>. We produce vowel sounds (e.g., /<i>a</i>/, /<i>i</i>/) by shaping the overtones from the vibrating vocal cords into distinct frequency bands, the formants. Sibilant sounds (/<i>s</i>/, /<i>z</i>/) result from the rush of air through particular configurations of tongue and teeth. Plosive consonants (/<i>p</i>/, /<i>b</i>/, /<i>t</i>/) are transitory sounds created by the percussive movement of lips, tongue, and mouth cavity. Nasal sounds (/<i>n</i>/, /<i>m</i>/) are created by invoking the resonances of the nasal cavity. The distribution of sounds vary from one <a class="thought" href="entries/language_entry.html">language</a> to another.</p>
<p>Each of the several dozen basic sounds, the phonemes, requires an intricate movement involving precise coordination of the vocal cords, alveolar flap, tongue, lips, and teeth. We typically speak about three words per second. So with an average of six phonemes per word, we make about eighteen intricate phonetic gestures per second, a task comparable in <a class="thought" href="entries/complexity_entry.html">complexity</a> to a performance by a concert pianist. We do this without <a class="thought" href="entries/thinking_entry.html">thinking</a> about it, of course. Our thoughts remain on the conceptual (that is, the highest) level of the <a class="thought" href="entries/language_entry.html">language</a> and <a class="thought" href="entries/knowledge_entry.html">knowledge</a> <a class="thought" href="entries/hierarchy_entry.html">hierarchy</a>. In our first two years of life, however, we <a class="thought" href="entries/thought_entry.html">thought</a> a lot about how to make speech sounds--and how to string them together meaningfully. This process is an example of our sequential (i.e., logical, rational) conscious mind training our parallel preconscious <a class="thought" href="entries/pattern_entry.html">pattern</a>-processing mental faculties.</p>
<p>The mechanisms described above for creating speech sounds--vocal cord vibrations, the noise of rushing air, articulatory gestures of the mouth, teeth and tongue, the shaping of the vocal and nasal cavities--produce different rates of vibration. Physicists measure these rates of vibration as frequencies; we perceive them as pitches. Though we normally think of speech as a single time-varying sound, it is actually a composite of many different sounds, each with its own frequency. Using this insight, most <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> researchers starting in the late 1960s began by breaking up the speech waveform into a <a class="thought" href="entries/number_entry.html">number</a> of frequency bands. A typical <a class="thought" href="entries/commercial_entry.html">commercial</a> or <a class="thought" href="entries/research_entry.html">research</a> <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> will produce between a few and several dozen frequency bands. The front end of the human auditory <a class="thought" href="entries/system_entry.html">system</a> does exactly the same thing: each nerve ending in the cochlea (inner ear) responds to different frequencies and emits a pulsed <a class="thought" href="entries/digital_entry.html">digital</a> signal when activated by an appropriate pitch. The cochlea differentiates several thousand overlapping bands of frequency, which gives the human auditory <a class="thought" href="entries/system_entry.html">system</a> its extremely high degree of sensitivity to frequency. Experiments have shown that increasing the <a class="thought" href="entries/number_entry.html">number</a> of overlapping frequency bands of an <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/system_entry.html">system</a> (thus making it more like the human auditory <a class="thought" href="entries/system_entry.html">system</a>) increases the ability of that <a class="thought" href="entries/system_entry.html">system</a> to recognize human speech. </p><h1>Lesson 4: Learn While You Listen </h1><p>A fourth lesson emphasizes the importance of <a class="thought" href="entries/learning_entry.html">learning</a>. At each stage of processing, a <a class="thought" href="entries/system_entry.html">system</a> must adapt to the <a class="thought" href="entries/individual_entry.html">individual</a> characteristics of the talker. <a class="thought" href="entries/learning_entry.html">Learning</a> to do this has to take place at several levels: those of the frequency and time relationships characterizing each <a class="thought" href="entries/phoneme_entry.html">phoneme</a>, the dialect (pronunciation) patterns of each word, and the syntactic patterns of possible phrases and sentences. At the highest cognitive level, a person or <a class="thought" href="entries/machine_entry.html">machine</a> understanding speech learns a great deal about what a particular talker tends to talk about and how that talker phrases his or her thoughts.</p>
<p>
<a class="thought" href="entries/hal_entry.html">HAL</a> learns a great deal about his human crew mates by listening to the sound of their voices, what they talk about, and how they put sentences together. He also watches what their mouths do when they articulate certain phrases (chapter 11). <a class="thought" href="entries/hal_entry.html">HAL</a> gathers so much <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about them that he can understand them even when some of the <a class="thought" href="entries/information_entry.html">information</a> is obscured--for example, when he has to rely solely on his visual observation of Dave and Frank's lips. </p><h1>Lesson 5: Hungry for <a class="thought" href="entries/mips_entry.html">MIPS</a> and Megabytes </h1><p>The fifth lesson is that speech recognition is a process hungry for MIPs (<a class="thought" href="entries/mips_entry.html">millions of instructions per second</a>) and megabytes (millions of bytes of storage); which is to say that we can obtain more accurate performance by using faster computers with larger memories. Certain algorithms or methods are only available in computers that operate at high levels of performance. Brute force--that is, huge <a class="thought" href="entries/memory_entry.html">memory</a>--is necessary but clearly not sufficient without solving the difficult algorithmic and <a class="thought" href="entries/knowledge_entry.html">knowledge</a>-capture issues mentioned above.</p>
<p>We now know that 1997, when <a class="thought" href="entries/hal_entry.html">HAL</a> reportedly became intelligent, is too soon. We won't have the quantity of computing, in terms of speed and <a class="thought" href="entries/memory_entry.html">memory</a> needed, to build a <a class="thought" href="entries/hal_entry.html">HAL</a>. And we won't be there in 2001 either.</p>
<p>Let's keep these lessons in mind as we examine the roots and <a class="thought" href="entries/future_entry.html">future</a> prospects of building machines that can duplicate <a class="thought" href="entries/hal_entry.html">HAL</a>'s ability to understand speech. </p><h1>The Importance of Speech Recognition </h1><p>Before examining the sweep of <a class="thought" href="entries/progress_entry.html">progress</a> in this field, it is worthwhile to underscore the importance of the auditory sense, particularly our ability to understand spoken <a class="thought" href="entries/language_entry.html">language</a>, and why this is a critical faculty for <a class="thought" href="entries/hal_entry.html">HAL</a>. Most of <a class="thought" href="entries/hal_entry.html">HAL</a>'s interaction with the crew is verbal. It is primarily through his recognition and understanding of speech that he communicates. <a class="thought" href="entries/hal_entry.html">HAL</a>'s visual perceptual skills, which are far more difficult to create, are relatively less important for carrying out his mission, even though the pivotal scene, in which <a class="thought" href="entries/hal_entry.html">HAL</a> understands Dave and Frank's conspiratorial <a class="thought" href="entries/conversation_entry.html">conversation</a>, without being able to hear them, relies on <a class="thought" href="entries/hal_entry.html">HAL</a>'s visual sense. Of course, his apparently self-taught lipreading is based on his speech-recognition ability and would have been impossible if <a class="thought" href="entries/hal_entry.html">HAL</a> had not been able to understand spoken <a class="thought" href="entries/language_entry.html">language</a>.</p>
<p>To demonstrate the importance of the auditory sense, try watching the television news with the sound turned off. Then try it again with the sound on, but without looking at the picture. Next, try a similar <a class="thought" href="entries/experiment_entry.html">experiment</a> with a videotape of the movie <i>2001</i>. You will probably find it easier to follow the stories with your ears alone than with your eyes alone, even though our eyes transmit much more <a class="thought" href="entries/information_entry.html">information</a> to our brains than our ears do--about fifty billion bits per second from both eyes versus approximately a million bits per second from two ears. The result is surprising. There is a saying that a picture is worth a thousand words; yet the above exercise illustrates the superior power of spoken <a class="thought" href="entries/language_entry.html">language</a> to convey our thoughts. Part of that power lies in the close link between verbal <a class="thought" href="entries/language_entry.html">language</a> and conscious <a class="thought" href="entries/thinking_entry.html">thinking</a>. Until recently, a popular theory held that <a class="thought" href="entries/thinking_entry.html">thinking</a> was subvocalized speech. (J. B. Watson, the founder of behaviorism, attached great attention to the small movements of the tongue and larynx made while we think.) Although we now recognize that thoughts incorporate both <a class="thought" href="entries/language_entry.html">language</a> and visual images, the crucial importance of the auditory sense in the acquisition of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>--which we need in order to recognize speech in the first place--is widely accepted.</p>
<p>Yet many people consider blindness a more serious handicap than deafness. A careful consideration of the issues shows this to be a misconception. With modern mobility techniques, blind persons with appropriate training have little difficulty going from place to place. The blind employees of my first company (<a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> <a class="thought" href="entries/computer_entry.html">Computer</a> Products, Inc., which developed the <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> <a class="thought" href="entries/reading_machine_entry.html">Reading Machine</a> for the Blind) traveled around the world routinely. Reading machines can vprovide <a class="thought" href="entries/access_entry.html">access</a> to the world of print, and visually impaired people <a class="thought" href="entries/experience_entry.html">experience</a> few barriers to communicating with others in groups or <a class="thought" href="entries/individual_entry.html">individual</a> encounters. For the deaf, however, the barrier to understanding what other people are saying is fundamental.</p>
<p>We learn to understand and produce spoken <a class="thought" href="entries/language_entry.html">language</a> during our first year of life, years before we can understand or create written <a class="thought" href="entries/language_entry.html">language</a>. <a class="thought" href="entries/hal_entry.html">HAL</a> apparently spent years <a class="thought" href="entries/learning_entry.html">learning</a> human speech by listening to his teacher, whom he identifies as Mr. Langley, at the <a class="thought" href="entries/hal_entry.html">HAL</a> lab in Urbana, Illinois. Studies with humans have shown that groups of people can solve problems with dramatically greater speed if they can communicate verbally rather than being restricted to other methods. <a class="thought" href="entries/hal_entry.html">HAL</a> and his human colleagues amply demonstrate this finding. Thus, intelligent machines that understand verbal <a class="thought" href="entries/language_entry.html">language</a> make possible an optimal modality of <a class="thought" href="entries/communication_entry.html">communication</a>. In recent years, a major goal of <a class="thought" href="entries/ai_entry.html">artificial intelligence</a> <a class="thought" href="entries/research_entry.html">research</a> has been making our interactions with computers more natural and intuitive. <a class="thought" href="entries/hal_entry.html">HAL</a>'s primarily verbal <a class="thought" href="entries/communication_entry.html">communication</a> with crew members is a clear example of an intuitive user <a class="thought" href="entries/interface_entry.html">interface</a>. </p><h1>The Roots of <a class="thought" href="entries/automated_speech_rec_entry.html">Automatic Speech Recognition</a> (<a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>) </h1><p>Keeping in mind our five lessons about creating speech-recognition systems, it is interesting to examine historical attempts to endow machines with the ability to understand human speech. The effort goes back to Alexander Graham Bell, and the roots of the story go even farther back, to Bell's grandfather Alexander Bell, a widely known lecturer and speech teacher. <i>His</i> son, Alexander Melville Bell, created a phonetic <a class="thought" href="entries/system_entry.html">system</a> for teaching the deaf to speak called <i>visible speech</i>. At the age of twenty-four, Alexander Graham Bell began teaching his father's <a class="thought" href="entries/system_entry.html">system</a> of visible speech to instructors of the deaf in Boston. He fell in love with and subsequently married one of his students, Mabel Hubbard. She had been deaf since the age of four as a result of scarlet fever. The marriage served to deepen his commitment to applying his inventiveness to overcoming the handicaps of deafness.</p>
<p>He built a <a class="thought" href="entries/device_entry.html">device</a> he called a <i>phonautograph</i> to make visual patterns from sound. Attaching a thin stylus to an eardrum he obtained from a medical school, he traced the patterns produced by speaking through the eardrum on a smoked glass screen. His wife, however, was unable to understand speech by looking at these patterns. The <a class="thought" href="entries/device_entry.html">device</a> could convert speech sounds into pictures, but the pictures were highly variable and showed no similarity in patterns, even when the same person spoke the same word.</p>
<p>In 1874, Bell demonstrated that the different frequency harmonics from an electrical signal could be separated. His <i>harmonic telegraph</i> could send multiple telegraphic messages over the same wire by using different frequency tones. The next year, the twenty-eight-year-old Bell had a profound insight. He hypothesized that although the <a class="thought" href="entries/information_entry.html">information</a> needed to understand speech sounds could not be seen by simply displaying the speech signal directly, it could be recognized if you first broke the signal into different frequency bands. Bell's intuitive discovery of our third lesson also turns out to be a key to finding the invariant features needed for the second lesson.</p>
<p>Bell felt sure he had all the pieces needed to <a class="thought" href="entries/implement_entry.html">implement</a> this insight and give his wife the ability to understand human speech. He had already developed a moving drum and solenoid (a metal core wrapped with wire) that could transform a human voice into a time-varying current of electricity. All he needed to do, he <a class="thought" href="entries/thought_entry.html">thought</a>, was to break up this electrical signal into different frequency bands, as he had done previously with the harmonic telegraph, then render each of these harmonics visually--by using multiple phonautographs. In June of 1875, while attempting to prepare this <a class="thought" href="entries/experiment_entry.html">experiment</a>, he accidentally connected the wire from the input solenoid back to another similar <a class="thought" href="entries/device_entry.html">device</a>. Now most processes are not reversible. Try unsmashing a teacup or speaking into a <a class="thought" href="entries/reading_machine_entry.html">reading machine</a> for the blind, which converts print into speech; it will not convert the speech back into print. But, unexpectedly, Bell's erstwhile microphone began to speak! Thus was the telephone discovered, or we should say, invented.</p>
<p>The <a class="thought" href="entries/device_entry.html">device</a> ultimately broke down the <a class="thought" href="entries/communication_entry.html">communication</a> barrier of distance for the human race. Ironically, Bell's great <a class="thought" href="entries/invention_entry.html">invention</a> also deepened the isolation of the deaf. The two methods of <a class="thought" href="entries/communication_entry.html">communication</a> available to the deaf--sign <a class="thought" href="entries/language_entry.html">language</a> and lipreading--are not possible over the telephone.</p>
<p>He continued to <a class="thought" href="entries/experiment_entry.html">experiment</a> with a frequency-based phonautograph, but without a <a class="thought" href="entries/computer_entry.html">computer</a> to analyze the rapidly time-varying harmonic bands, the <a class="thought" href="entries/information_entry.html">information</a> remained a bewildering array to a sighted deaf person. We now know that we can visually examine frequency-based pictures of speech (i.e., spectrograms) and understand the <a class="thought" href="entries/communication_entry.html">communication</a> from the visual <a class="thought" href="entries/information_entry.html">information</a> alone; but the process is extremely difficult and slow. An MIT graduate course, Speech Spectrogram Reading, teaches precisely this skill. The purpose of the course is to give students insight into the spectral cues of <a class="thought" href="entries/salient_entry.html">salient</a> speech events. For many years, the course's professor, Dr. Victor Zue, was the only person who could understand speech from spectrograms with any proficiency; several people have reportedly now mastered this skill. Computers, on the other hand, can readily handle spectral <a class="thought" href="entries/information_entry.html">information</a>, and we can build a cruqde but usable speech-recognition <a class="thought" href="entries/system_entry.html">system</a> using this type of acoustic <a class="thought" href="entries/information_entry.html">information</a> alone. So Bell was on the right track--about a century too soon.</p>
<p>Ironically, another pioneer, <a class="thought" href="entries/babbage_entry.html">Charles Babbage</a>, had attempted to create that other prerequisite to <a class="thought" href="entries/automated_speech_rec_entry.html">automatic speech recognition</a>--the programmable <a class="thought" href="entries/computer_entry.html">computer</a>--about forty years earlier. Babbage built his <a class="thought" href="entries/computer_entry.html">computer</a>, the <i>analytical engine</i>, entirely of mechanical parts; yet it was a true <a class="thought" href="entries/computer_entry.html">computer</a>, with a <a class="thought" href="entries/stored_program_entry.html">stored program</a>, a <a class="thought" href="entries/central_processing_unit_entry.html">central processing unit</a>, and <a class="thought" href="entries/memory_entry.html">memory</a> store. Despite Babbage's exhaustive efforts, nineteenth-century machining <a class="thought" href="entries/technology_entry.html">technology</a> could not build the <a class="thought" href="entries/machine_entry.html">machine</a>. Like Bell, Babbage was about a century ahead of his time, and the <a class="thought" href="entries/analytical_engine_entry.html">analytical engine</a> never ran.</p>
<p>Not until the 1940s, when fueled by the exigencies of war, were the first computers actually built: the Z-3 by Konrad Zuse in Nazi Germany, the Mark I by U.S. Navy Commander Howard Aiken, and the <a class="thought" href="entries/robinson_entry.html">Robinson</a> and <a class="thought" href="entries/colossus_entry.html">Colossus</a> computers by <a class="thought" href="entries/turing_entry.html">Alan Turing</a> and his English colleagues. Turing's Bletchley group broke the German <a class="thought" href="entries/enigma_code_entry.html">Enigma code</a> and are credited with enabling the Royal Air Force to win the Battle of Britain and so withstand the Nazi war <a class="thought" href="entries/machine_entry.html">machine</a>.</p>
<p>For Bell, whose <a class="thought" href="entries/invention_entry.html">invention</a> of the telephone created the <a class="thought" href="entries/telecommunications_entry.html">telecommunications</a> revolution, the original goal of easing the isolation of the deaf remained elusive. His insights into separating the speech signal into different frequency components and rendering those components as visible traces were not successfully implemented until Potter, Kopp, and Green designed the spectrogram and Dreyfus-Graf developed the steno-sonograph in the late 1940s. These devices generated interest in the possibility of automatically recognizing speech because they made the invariant features of speech visible for all to see.</p>
<p>The first serious speech recognizer was developed in 1952 by Davis, Biddulph, and Balashek of <a class="thought" href="entries/bell_labs_entry.html">Bell Labs</a>. Using a simple frequency splitter, it generated plots of the first two formants, which it identified by matching them against prestored patterns in an <a class="thought" href="entries/analog_entry.html">analog</a> <a class="thought" href="entries/memory_entry.html">memory</a>. With training, it was reported, the <a class="thought" href="entries/machine_entry.html">machine</a> achieved 97 percent accuracy on the spoken forms of ten digits.</p>
<p>By the 1950s, researchers began to follow lesson 5 and to use computers for <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a>, which allowed for <a class="thought" href="entries/linear_entry.html">linear</a> time normalization, a <a class="thought" href="entries/concept_entry.html">concept</a> introduced by Denes and Mathews in 1960. The 1960s saw several successful experiments with discrete word recognition in real time using <a class="thought" href="entries/digital_entry.html">digital</a> computers; words were spoken in isolation with brief silent pauses between them. Some notable success was also achieved with relatively large vocabularies, although with constrained syntaxes. In 1969, two such systems--the Vicens <a class="thought" href="entries/system_entry.html">system</a>, which accepted a five-hundred-word vocabulary, and the Medress <a class="thought" href="entries/system_entry.html">system</a> with its one-hundred-word vocabulary were described in Ph.D. dissertations.</p>
<p>That same year, John Pierce wrote a celebrated, caustic letter objecting to the repetitious implementation of small-vocabulary discrete word devices. He argued for attacking more ambitious goals by harnessing different levels of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, including <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of speech, <a class="thought" href="entries/language_entry.html">language</a>, and task. He argued against real-time devices, anticipating (correctly) that processing speeds would improve dramatically in the near <a class="thought" href="entries/future_entry.html">future</a>. Partly in response to the concerns articulated by Pierce, the U.S. Defense Advanced <a class="thought" href="entries/research_entry.html">Research</a> Projects Agency began serious funding of <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a><a class="thought" href="entries/research_entry.html">research</a> with the ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> (Speech Understanding <a class="thought" href="entries/research_entry.html">Research</a>) project, which began in 1971. As Allen Newell of Carnegie Mellon University observes in his 1975 paper, there were three ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> dogmas. First, all sources of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, from <a class="thought" href="entries/acoustics_entry.html">acoustics</a> to <a class="thought" href="entries/semantics_entry.html">semantics</a>, should be part of any <a class="thought" href="entries/research_entry.html">research</a> <a class="thought" href="entries/system_entry.html">system</a>. Second, <a class="thought" href="entries/context_entry.html">context</a> and a priori <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the <a class="thought" href="entries/language_entry.html">language</a> should supplement analysis of the sound itself. Third, the objective of <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> is, properly, speech understanding, not simply correct identification of words in a spoken message. Systems, therefore, should be evaluated in terms of their ability to respond correctly to spoken messages about such pragmatic problems as travel budget management. (For example, researchers might ask a <a class="thought" href="entries/system_entry.html">system</a>  "What is the plane fare to Ottawa?") Not surprisingly, this third dogma was the most controversial and remains so today; and different markets have been identified for speech-recognition and speech-understanding systems.</p>
<p>The objective goal of ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> was a recognition <a class="thought" href="entries/system_entry.html">system</a> with 90 percent sentence accuracy for continuous-speech sentences, using thousand-word vocabularies, not in real time. Of four principal ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> projects, the only one to meet the stated goal was Carnegie Mellon University's Harpy <a class="thought" href="entries/system_entry.html">system</a>, which achieved a 5 percent error rate on a 1,011-word vocabulary on continuous speech. One of the ways the CMU team achieved the goal was clever: they made the task easier by restricting word order; that is, by limiting spoken words to certain sequences in the sentence.</p>
<p>The five-year ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> project was thoroughly analyzed and debated for at least a decade after its completion. Its legacy was to establish firmly the five lessons I have described. By then it was clear that the best way to reduce the error rate was to build in as much <a class="thought" href="entries/knowledge_entry.html">knowledge</a> as possible about speech (lesson 1): how speech sounds are structured, how they are strung together, what determines sequences, the syntactic <a class="thought" href="entries/structure_entry.html">structure</a> of the <a class="thought" href="entries/language_entry.html">language</a> (English, in this case), and the <a class="thought" href="entries/semantics_entry.html">semantics</a> and pragmatics of the subject <a class="thought" href="entries/matter_entry.html">matter</a> and task--which for ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> were far simpler than what <a class="thought" href="entries/hal_entry.html">HAL</a> had to understand.</p>
<p>Great strides were made in normalizing the speech signal to filter out variability (lesson 2). F. Itakura, a Japanese scientist, and H. Sakoe and S. Chiba introduced <i>dynamic programming</i> to compute optimal nonlinear time alignments, a technique that quickly became the standard. Jim Baker and <a class="thought" href="entries/ibm_entry.html">IBM</a>'s Fred Jelinek introduced a statistical <a class="thought" href="entries/method_entry.html">method</a> called Markov Modeling; it provided a powerful mathematical tool for finding the invariant <a class="thought" href="entries/information_entry.html">information</a> in the speech signal.</p>
<p>Lesson 3, about breaking the speech signal into its frequency components, had already been established prior to the ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> projects, some of which developed systems that could adapt to aspects of the speaker's voice (lesson 4). Lesson 5 was anticipated by allowing ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> researchers to use as much <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/memory_entry.html">memory</a> as they could afford to buy and as much <a class="thought" href="entries/computer_entry.html">computer</a> time as they had the patience to wait for. An underlying, and accurate expectation was that <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> (see chapter 3 and below) would ultimately provide whatever computing <a class="thought" href="entries/platform_entry.html">platform</a> the algorithms required. </p><h1>The 1970s </h1><p>The 1970s were notable for other significant <a class="thought" href="entries/research_entry.html">research</a> efforts. In addition to introducing <a class="thought" href="entries/dynamic_programming_entry.html">dynamic programming</a>, Itakura developed an influential analysis of spectral-distance measures, a way to compute how similar two different sounds are. His <a class="thought" href="entries/system_entry.html">system</a> demonstrated an impressive 97.3 percent accuracy on two hundred Japanese words spoken over the telephone. <a class="thought" href="entries/bell_labs_entry.html">Bell Labs</a> also achieved significant success (a 97.1 percent accuracy) with speaker--independent systems--that is, systems that understand voices they have not heard before. <a class="thought" href="entries/ibm_entry.html">IBM</a> concentrated on the Markov modeling statistical technique and demonstrated systems that could recognize a large vocabulary.</p>
<p>By the end of the 1970s, numerous <a class="thought" href="entries/commercial_entry.html">commercial</a> speech-recognition products were available. They ranged from Heuristics' $259 H-2000 Speech Link, to $100,000 speaker-independent systems from Verbex and Nippon. Other companies, including Threshold, Scott, Centigram, and Interstate, offered systems with sixteen-channel filter banks at prices between $2,000 and $15,000. Such products could recognize small vocabularies spoken with pauses between words. </p><h1>The 1980s </h1><p>The 1980s saw the <a class="thought" href="entries/commercial_entry.html">commercial</a> field of <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> split into two fairly distinct market segments. One group--which included Verbex, Voice Processing Corporation, and several others--pursued reliable speaker-independent recognition of small vocabularies for telephone transaction processing. The other group, which included <a class="thought" href="entries/ibm_entry.html">IBM</a> and two new companies--Jim and Janet Baker's Dragon Systems, and my <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> Applied <a class="thought" href="entries/intelligence_entry.html">Intelligence</a>--pursued large-vocabulary <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> for creating written documents by voice.</p>
<p>Important work on large-vocabulary continuous speech (i.e., speech with no pauses between words) was also conducted at Carnegie Mellon University by Kai-Fu Lee, who subsequently left the university to head <a class="thought" href="entries/apple_entry.html">Apple</a>'s speech-recognition efforts.</p>
<p>By 1991, revenues for the <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> industry were in low eight figures and were increasing substantially every year. A buyer could choose any one (but not two) characteristics from the following menu: large vocabulary, <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a>, or continuous speech. <a class="thought" href="entries/hal_entry.html">HAL</a>, of course, could do all three. </p><h1>The State of the <a class="thought" href="entries/art_entry.html">Art</a> </h1><p>So where are we today? We now, finally, have inexpensive personal computers that can support high-performance <a class="thought" href="entries/automated_speech_rec_entry.html">ASR</a> <a class="thought" href="entries/software_entry.html">software</a>. Buyers can now choose any two (but not all three) capabilities from the menu listed above. For example, my company's <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> VOICE for <a class="thought" href="entries/windows_entry.html">Windows</a> can recognize a sixty-thousand-word vocabulary spoken discretely (i.e., with brief pauses between each word). Another experimental <a class="thought" href="entries/system_entry.html">system</a> can handle a thousand-word, command-and-control vocabulary with continuous speech (i.e., no pauses). Both systems provide <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a>; that is, they can recognize words spoken by your voice even if they've never heard it before. Systems in this product category are also made by Dragon Systems and <a class="thought" href="entries/ibm_entry.html">IBM</a>.</p><h1>Playing <a class="thought" href="entries/hal_entry.html">HAL</a> </h1><p>To demonstrate today's state of the art in <a class="thought" href="entries/computer_entry.html">computer</a> speech recognition, we fed in some of the sound track of 2001 into the <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> VOICE for <a class="thought" href="entries/windows_entry.html">Windows</a> version 2.0 (KV/Win 2.0). KV/Win 2.0 is capable of understanding the speech of a person it has not heard speak before and can recognize a vocabulary of up to sixty thousand words (forty thousand in its initial vocabulary with the ability to add another twenty thousand). The primary limitation of today's <a class="thought" href="entries/technology_entry.html">technology</a> is that it can only handle discrete speech--that is, words or brief phrases (such as <i>thank you</i>) spoken with brief pauses in between. I played the following dialog to KV/Win 2.0 with a view to <a class="thought" href="entries/learning_entry.html">learning</a> whether it could understand Dave as <a class="thought" href="entries/hal_entry.html">HAL</a> does in the movie:</p>
<p>
<i>HAL</i>: Good evening, Dave.</p>
<p>
<i>Dave</i>: How you doing, <a class="thought" href="entries/hal_entry.html">HAL</a>?</p>
<p>
<i>HAL</i>: Everything is running smoothly; and you?</p>
<p>
<i>Dave</i>: Oh, not too bad.</p>
<p>
<i>HAL</i>: Have you been doing some more work?</p>
<p>
<i>Dave</i>: Just a few sketches.</p>
<p>
<i>HAL</i>: May I see them?</p>
<p>
<i>Dave</i>: Sure.</p>
<p>
<i>HAL</i>: That's a very nice rendering, Dave. I think you've improved a great deal. Can you hold it a bit closer?</p>
<p>
<i>Dave</i>: Sure.</p>
<p>
<i>HAL</i>: That's Dr. Hunter, isn't it?</p>
<p>
<i>Dave</i>: Hm hmm.</p>
<p>
<i>HAL</i>: By the way, do you mind if I ask you a personal question?</p>
<p>
<i>Dave</i>: No, not at all.</p>
<p>I trained the <a class="thought" href="entries/system_entry.html">system</a> on the phrases "Oh, not too bad" and "No, not at all," but did not train it on Dave's voice. When I did the <a class="thought" href="entries/experiment_entry.html">experiment</a>, KV/Win 2.0 had never heard Dave's voice, and it had to pick out each word or phrase from among forty thousand possibilities. I had the <a class="thought" href="entries/system_entry.html">system</a> listen to Dave saying the following discrete words and phrases from the above dialog:</p>
<p>Dave: Oh, not too bad.</p>
<p>Dave: Sure.</p>
<p>Dave: Sure.</p>
<p>Dave: No, not at all.</p>
<p>KV/Win 2.0 was able to successfully recognize the above utterances even though it had not been previously exposed to Dave's voice. For good measure, I also had KV/Win 2.0 listen to Dave in the critical scene in which <a class="thought" href="entries/hal_entry.html">HAL</a> is betraying him. In this scene, Dave says the word <a class="thought" href="entries/hal_entry.html">HAL</a> five times in a row in an increasingly plaintive voice. KV/Win 2.0 successfully recognized the five utterances, despite their obvious differences in tone and enunciation. Looking at the spectrogram, we can see that these five utterances, although they are similar in some respects, are really quite different from one another and demonstrate clearly the variability of human speech. So, except for KV/Win's restriction to discrete speech, with regard to speech recognition we've already created <a class="thought" href="entries/hal_entry.html">HAL</a>!</p>
<p>Of course, the limitation to discrete speech is no minor exception. When will our computers be capable of recognizing fully continuous speech? Recently, ARPA has funded a new round of <a class="thought" href="entries/research_entry.html">research</a> aimed at "<a class="thought" href="entries/holy_grail_entry.html">holy grail</a>" systems that combine all three capabilities--handling continuous speech with very large vocabularies and <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a>. Like the earlier ARPA <a class="thought" href="entries/sur_entry.html">SUR</a> projects, there are no restrictions on <a class="thought" href="entries/memory_entry.html">memory</a> or real-time performance. Restricting the task to understanding "business English," ARPA contractors--including Phillips, Bolt, Beranek and Newman, Dragon Systems, Inc., and others--have reported word accuracies around 97 percent or higher. <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> will take care of achieving real-time performance on affordable machines, so that we should see such systems available commercially by, perhaps, early 1998.</p>
<p>Expanding the <a class="thought" href="entries/domain_entry.html">domain</a> of recognition--not to mention understanding--to the humanlike flexibility <a class="thought" href="entries/hal_entry.html">HAL</a> displays will take a far greater mastery of the many levels of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> represented in spoken <a class="thought" href="entries/language_entry.html">language</a>. I would expect that by the year 2001--remembering that in the movie <a class="thought" href="entries/hal_entry.html">HAL</a> became intelligent much earlier--we will have systems able to recognize speech well enough to produce a written transcription of the movie from the sound track. Even then, the error rate will be far higher than <a class="thought" href="entries/hal_entry.html">HAL</a>'s (who, of course, claims he has never made a mistake).</p>
<p>In 1997 we appreciate that speech recognition does not exist in a <a class="thought" href="entries/vacuum_entry.html">vacuum</a> but has to be integrated with other levels and sources of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> Applied <a class="thought" href="entries/intelligence_entry.html">Intelligence</a>, Inc., for example, has integrated its large-vocabulary speech recognition capability with an <a class="thought" href="entries/expert_system_entry.html">expert system</a> that has extensive <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the preparation of medical reports; the <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> VoiceMED can guide doctors through the reporting process and assist them to comply with the latest regulations. If you find yourself in a hospital emergency room, there is a 10-percent chance your attending physician will dictate his or her report to one of our speech-recognition systems. We recently began adding the ability to understand <a class="thought" href="entries/natural_language_entry.html">natural language</a> commands spoken in continuous speech. If, for example, you say, "go to the second paragraph on the next page; select the second sentence; capitalize every word in this sentence; underline it ..." the <a class="thought" href="entries/system_entry.html">system</a> is likely to follow this series of commands. If you say "Open the pod bay doors," it will probably respond "Command not understood." </p><h1>How to Build a Speech Recognizer </h1><p>
<a class="thought" href="entries/software_entry.html">Software</a> today is not an isolated field, but one that encompasses and codifies every other field of endeavor. Everyone--librarians, musicians, magazine publishers, doctors, graphic artists, architects, researchers of every kind--are digitizing their <a class="thought" href="entries/knowledge_entry.html">knowledge</a> bases, methods, and expressions of their work. Those of us working on speech understanding are experiencing the same rapid change, as hundreds of scientists and engineers build increasingly elaborate data bases and structures to describe our <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of speech sounds, phonetics, <a class="thought" href="entries/linguistics_entry.html">linguistics</a>, <a class="thought" href="entries/syntax_entry.html">syntax</a>, <a class="thought" href="entries/semantics_entry.html">semantics</a>, and pragmatics--in accordance with lesson 1.</p>
<p>A speech-recognition <a class="thought" href="entries/system_entry.html">system</a> operates in phases, with each new phase using increasingly sophisticated <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the next higher level of <a class="thought" href="entries/language_entry.html">language</a>. At the front end, the <a class="thought" href="entries/system_entry.html">system</a> converts the time-varying air pressure we call sound into an electrical signal, as Bell did a hundred years ago with his crude microphones. Then, a <a class="thought" href="entries/device_entry.html">device</a> called an <a class="thought" href="entries/analog_entry.html">analog</a>-to-<a class="thought" href="entries/digital_entry.html">digital</a> converter changes the signal into a series of numbers. The numbers may be modified to normalize for loudness levels and possibly to eliminate background noise and distortion. The signal, which is now a <a class="thought" href="entries/digital_entry.html">digital</a> stream of numbers, is usually converted into multiple streams, each of which represents a different frequency band. These multiple streams are then compressed, using a variety of mathematical techniques that reduce the amount of <a class="thought" href="entries/information_entry.html">information</a> and emphasize those features of the speech signal important for recognizing speech.</p>
<p>For example, we want to know that a certain segment of sound contains a broad noiselike band of frequencies that might represent the sound of rushing air, as in the sound /<i>h</i>/ in <a class="thought" href="entries/hal_entry.html">HAL</a>; another segment contains two or three resonant frequencies in a certain ratio that might represent the vowel sound /<i>a</i>/ in <a class="thought" href="entries/hal_entry.html">HAL</a>. One way to accomplish this labeling is to store examples of such sounds and attempt to match incoming time slices against these templates. Usually, the attempt to categorize slices of sound uses a much finer <a class="thought" href="entries/classification_entry.html">classification</a> <a class="thought" href="entries/system_entry.html">system</a> than the approximately forty phonemes of English. We typically use a set of 256 or even 1,024 possible classifications in a process called <i>vector quantization</i>.</p>
<p>Once we have classified these time slices of sound, we can use one of several competing approaches to recognizing words. One of them develops statistical models for words or portions of words by analyzing massive amounts of prerecorded speech data. Markov modeling and neural nets are examples of this approach. Another approach tries to detect the underlying string of phonemes (or possibly other types of subword units) and then match them to the words spoken.</p>
<p>At <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a> Applied <a class="thought" href="entries/intelligence_entry.html">Intelligence</a> (KAI), rather than select one optimal approach, we implemented seven or eight different modules, or "experts," then programmed another <a class="thought" href="entries/software_entry.html">software</a> module, the "<i>expert manager</i>," which knows the strengths and weaknesses of the different <a class="thought" href="entries/software_entry.html">software</a> experts. In this decision-by-committee approach, the expert manager is the chief executive officer and makes the final decisions.</p>
<p>In the KAI systems, some of the expert modules are based, not on the sound of the words but on rules and the statistical behavior of word sequences. This is a variation of the <a class="thought" href="entries/hypothesis_entry.html">hypothesis</a>-and-test <a class="thought" href="entries/paradigm_entry.html">paradigm</a> in which the <a class="thought" href="entries/system_entry.html">system</a> expects to hear certain words, according to what the speaker has already said. Each of the modules in the <a class="thought" href="entries/system_entry.html">system</a> has a great deal of built-in <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. The acoustic experts contain <a class="thought" href="entries/knowledge_entry.html">knowledge</a> on the sound <a class="thought" href="entries/structure_entry.html">structure</a> of words or such subword units as phonemes. The <a class="thought" href="entries/language_entry.html">language</a> experts know how words are strung together. The expert manager can judge which experts are more reliable in particular situations.</p>
<p>The <a class="thought" href="entries/system_entry.html">system</a> as a whole begins with generic <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of speech and <a class="thought" href="entries/language_entry.html">language</a> in general, then adapts these <a class="thought" href="entries/knowledge_entry.html">knowledge</a> structures, based on what it observes in a particular speaker. In the film, Dave and Frank frequently invoke <a class="thought" href="entries/hal_entry.html">HAL</a>'s name. Even today's speech-recognition systems would quickly learn to recognize the word <a class="thought" href="entries/hal_entry.html">HAL</a> and would not mistake it for <i>hill</i> or <i>hall</i>, at least not after being corrected once or twice.</p>
<p>In continuous speech, a speech-recognition <a class="thought" href="entries/system_entry.html">system</a> needs to deal with the additional ambiguity of when words start and end. Its attempts to match the classified time slices and recognized subword units against actual word hypotheses could result in a <a class="thought" href="entries/combinatorial_explosion_entry.html">combinatorial explosion</a>. A vocabulary of, say, sixty thousand words, could produce 3.6 billion possible two-word sequences, 216 trillion three-word sequences, and so on. Obviously, as we cannot examine even a tiny fraction of these possibilities, <a class="thought" href="entries/search_entry.html">search</a> constraints based on the <a class="thought" href="entries/system_entry.html">system</a>'s <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of <a class="thought" href="entries/language_entry.html">language</a> are crucial. </p><h1><a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's Law</a> </h1><p>The other major ingredient needed to achieve the <a class="thought" href="entries/holy_grail_entry.html">holy grail</a> (i.e., a <a class="thought" href="entries/system_entry.html">system</a> that can understand fully continuous speech with high accuracy with relatively unrestricted vocabulary and <a class="thought" href="entries/domain_entry.html">domain</a> and with no previous exposure to the speaker) is a more-powerful <a class="thought" href="entries/computer_entry.html">computer</a>. We already have systems that can combine continuous speech, very large vocabularies, and <a class="thought" href="entries/speaker_independence_entry.html">speaker independence</a>--with the only limitation being restriction of the <a class="thought" href="entries/domain_entry.html">domain</a> to business English. But these systems require <a class="thought" href="entries/ram_entry.html">RAM</a> memories of over 100 megabytes and run much slower than real time on powerful workstations. Even though computational power is critical to developing speech recognition and understanding, no one in the field is worried about obtaining it in the near <a class="thought" href="entries/future_entry.html">future</a>. We know we will not have to wait long to achieve the requisite computational power because of <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>.</p>
<p>
<a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> states that computing speeds and densities double every eighteen months; it is the driving force behind a revolution so vast that the entire <a class="thought" href="entries/computer_entry.html">computer</a> revolution to date represents only a minor ripple of its ultimate implications. It was first articulated in the mid-1960s by Dr. <a class="thought" href="entries/moore_entry.html">Gordon Moore</a>. <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> actually is a corollary of a broader law I like to call <a class="thought" href="entries/kurzweil_entry.html">Kurzweil</a>'s law, which concerns the exponentially quickening pace of <a class="thought" href="entries/technology_entry.html">technology</a> back to the dawn of human <a class="thought" href="entries/history_entry.html">history</a>. A thousand years ago, not much happened in a century, technologically speaking. In the nineteenth century, quite a bit happened. Now major technological transformations occur in a few years time. <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>, a clear quantification of this exponential <a class="thought" href="entries/phenomenon_entry.html">phenomenon</a>, indicates that the pace will continue to accelerate.</p>
<p>Remarkably, this law has held true since the beginning of this century. It began with the mechanical card-based computing <a class="thought" href="entries/technology_entry.html">technology</a> used in the 1890 census, moved to the relay-based computers of the 1940s, to the <a class="thought" href="entries/vacuum_tube_entry.html">vacuum tube</a>-based computers of the 1950s, to the <a class="thought" href="entries/transistor_entry.html">transistor</a>-based machines of the 1960s, and to all the generations of integrated circuits we've seen over the past three decades. If you chart the abilities of every <a class="thought" href="entries/calculator_entry.html">calculator</a> and <a class="thought" href="entries/computer_entry.html">computer</a> developed in the past hundred years logarithmically, you get an essentially straight line. <a class="thought" href="entries/computer_entry.html">Computer</a> <a class="thought" href="entries/memory_entry.html">memory</a>, for example, is about sixteen thousand times more powerful today for the same unit cost than it was in about 1976 and is a hundred and fifty million times more powerful for the same unit cost than it was in 1948.</p>
<p>
<a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> will continue to operate unabated for many decades to come; we have not even begun to explore the third dimension in chip design. Today's chips are flat, whereas our brain is organized in three dimensions. We live in a three-dimensional world, why not use the third dimension? (Present-day chips are made up of a dozen or more layers of material that construct a single layer of transistors and other integrated components. A few chips do utilize more than one layer of components but make only limited use of the third dimension.) Improvements in <a class="thought" href="entries/semiconductor_entry.html">semiconductor</a> materials, including superconducting circuits that don't generate heat, will enable us to develop chips--that is, cubes--with thousands of layers of circuitry that, combined with far smaller <a class="thought" href="entries/component_entry.html">component</a> geometries, will improve computing power by a factor of many millions. There are more than enough new computing technologies under development to assure us of a continuation of <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> for a very long time. So, although some people argue that we are reaching the limits of <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>, I disagree. (See David Kuck's detailed analysis in chapter 3.)</p>
<p>
<a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> provides us with the infrastructure--in terms of <a class="thought" href="entries/memory_entry.html">memory</a>, <a class="thought" href="entries/computation_entry.html">computation</a>, and <a class="thought" href="entries/communication_entry.html">communication</a> <a class="thought" href="entries/technology_entry.html">technology</a>--to embody all our <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and methodologies and harness them on inexpensive platforms. It already enables us to live in a world where all our <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, all our creations, all our insights, all our ideas, and all our cultural expressions--pictures, movies, art, sound, music, books and the secret of life itself--are being digitized, captured, and understood in sequences of ones and zeroes. As we gather and codify more and more <a class="thought" href="entries/knowledge_entry.html">knowledge</a> about the <a class="thought" href="entries/hierarchy_entry.html">hierarchy</a> of spoken <a class="thought" href="entries/language_entry.html">language</a> from speech sounds to subject <a class="thought" href="entries/matter_entry.html">matter</a>, <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> will provide computing platforms able to embody that <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. At the front end, it will let us analyze a greater <a class="thought" href="entries/number_entry.html">number</a> of frequency bands, ultimately approaching the exquisite sensitivity of the human auditory sense to frequency. At the back end, it will allow us to take advantage of vast linguistic data bases.</p>
<p>Like many <a class="thought" href="entries/computer_entry.html">computer</a>-<a class="thought" href="entries/science_entry.html">science</a> problems, recognizing human speech suffers from a <a class="thought" href="entries/number_entry.html">number</a> of potential combinatorial explosions. As we increase vocabulary size in a continuous-speech <a class="thought" href="entries/system_entry.html">system</a>, for example, the <a class="thought" href="entries/number_entry.html">number</a> and length of possible word combinations increases geometrically. So making <a class="thought" href="entries/linear_entry.html">linear</a> <a class="thought" href="entries/progress_entry.html">progress</a> in performance requires us to make exponential <a class="thought" href="entries/progress_entry.html">progress</a> in our computing platforms. But that is exactly what we are doing.</p><h1>Some Predictions </h1><p>Based on <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>, and the continued efforts of over a thousand researchers in speech recognition and related areas, I expect to see <a class="thought" href="entries/commercial_entry.html">commercial</a>-grade continuous-speech dictation systems for restricted domains, such as <a class="thought" href="entries/medicine_entry.html">medicine</a> or law, to appear in 1997 or 1998. And, soon after, we will be talking to our computers in continuous speech and <a class="thought" href="entries/natural_language_entry.html">natural language</a> to control personal-<a class="thought" href="entries/computer_entry.html">computer</a> applications. By around the turn of the century, unrestricted-<a class="thought" href="entries/domain_entry.html">domain</a>, continuous-speech dictation will be the standard. An especially exciting application of this <a class="thought" href="entries/technology_entry.html">technology</a> will be listening machines for the deaf analogous to reading machines for the blind. They will convert speech into a display of text in real time, thus achieving Alexander Graham Bell's original vision a century and a quarter later.</p>
<p>Translating telephones that convert speech from one <a class="thought" href="entries/language_entry.html">language</a> to another (by first recognizing speech in the original <a class="thought" href="entries/language_entry.html">language</a>, translating the text into the target <a class="thought" href="entries/language_entry.html">language</a>, then synthesizing speech in the target <a class="thought" href="entries/language_entry.html">language</a>) will be demonstrated by the end of this century and will become common during the first decade of the twenty-first century. <a class="thought" href="entries/conversation_entry.html">Conversation</a> with computers that are increasingly unseen and embedded in our environment will become routine ways to accomplish a broad variety of tasks.</p>
<p>In a classic paper published in 1950, <a class="thought" href="entries/turing_entry.html">Alan Turing</a> foretold that by early in the next century <a class="thought" href="entries/society_entry.html">society</a> would take for granted the pervasive intervention of intelligent machines. This remarkable prediction--given the state of <a class="thought" href="entries/hardware_entry.html">hardware</a> <a class="thought" href="entries/technology_entry.html">technology</a> at that time--attests to his implicit appreciation of <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>. </p><h1>Building <a class="thought" href="entries/hal_entry.html">HAL</a>'s <a class="thought" href="entries/language_entry.html">Language</a> <a class="thought" href="entries/knowledge_base_entry.html">Knowledge Base</a> </h1><p>For reasons that should be clear from our discussion, creating a <a class="thought" href="entries/machine_entry.html">machine</a> with <a class="thought" href="entries/hal_entry.html">HAL</a>'s ability to understand spoken <a class="thought" href="entries/language_entry.html">language</a> requires a level of <a class="thought" href="entries/intelligence_entry.html">intelligence</a> and mastery of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> that spans the full range of human cognition. When we test our own ability to understand spoken words out of <a class="thought" href="entries/context_entry.html">context</a> (i.e., spoken in a random, nonsense order), we find that the accuracy of speech recognition diminishes dramatically, compared to our understanding of words spoken in a meaningful order. Once, as an <a class="thought" href="entries/experiment_entry.html">experiment</a>, I walked into a colleague's office and said "Pod 3BA." My colleague's response was "What?" When I asked him to repeat what I had said, he couldn't. <a class="thought" href="entries/hal_entry.html">HAL</a>, of course, has little difficulty understanding this phrase when Dave asks him to prepare Pod 3BA; it makes sense in the <a class="thought" href="entries/context_entry.html">context</a> of that <a class="thought" href="entries/conversation_entry.html">conversation</a>, and we human viewers of the movie easily understood it too.</p>
<p>Understanding spoken <a class="thought" href="entries/language_entry.html">language</a> uses the full range of our <a class="thought" href="entries/intelligence_entry.html">intelligence</a> and <a class="thought" href="entries/knowledge_entry.html">knowledge</a>. Many observers (including some authors of chapters in this book) predict that machines will never achieve certain human capabilities--including the deep understanding of <a class="thought" href="entries/language_entry.html">language</a> <a class="thought" href="entries/hal_entry.html">HAL</a> appears to possess. If by the word <i>never</i>, they mean <i>not in the next couple of decades</i>, then such predictions might be reasonable. If the word carries its usual meaning, such predictions are shortsighted in my view, reminiscent of predictions that "man" would never fly or that machines would never beat the human world chess champion.</p>
<p>With regard to <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>, the doubling of <a class="thought" href="entries/semiconductor_entry.html">semiconductor</a> density means that we can put twice as many processors (or, alternatively, a processor with twice the computing power) on a chip (or comparable <a class="thought" href="entries/device_entry.html">device</a>) every eighteen months. Combined with the doubling of speed from shorter signaling distances, such increases may actually quadruple the power of <a class="thought" href="entries/computation_entry.html">computation</a> every eighteen months (that is, double it every nine months). This is particularly true for algorithms that can benefit from <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a>. Most researchers anticipate the next one or two turns of Moore's screw; others look ahead to the next four or five turns. But <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> is inexorable. Taking into account both density and speed, we are presently increasing the power of <a class="thought" href="entries/computation_entry.html">computation</a> (for the same unit cost) by a factor of sixteen thousand every ten years, or 250 million every twenty years.</p>
<p>Consider, then, what it would take to build a <a class="thought" href="entries/machine_entry.html">machine</a> with the <a class="thought" href="entries/capacity_entry.html">capacity</a> of the human brain. We can approach this issue in many ways, one way is just to continue our dogged codification of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and skill on yet-faster machines. Undoubtedly this process will continue. The following scenario, however, is a bit different approach to building a <a class="thought" href="entries/machine_entry.html">machine</a> with human-level <a class="thought" href="entries/intelligence_entry.html">intelligence</a> and <a class="thought" href="entries/knowledge_entry.html">knowledge</a>--that is, building <a class="thought" href="entries/hal_entry.html">HAL</a>. Note that I've simplified the following analysis in the interest of space; it would take a much longer article to respond to all of the anticipated objections. </p><h1>Another <a class="thought" href="entries/paradigm_shift_entry.html">Paradigm Shift</a> </h1><p>The human brain uses a radically different computational <a class="thought" href="entries/paradigm_entry.html">paradigm</a> than the computers we're used to. A typical <a class="thought" href="entries/computer_entry.html">computer</a> does one thing at a time, but does it very quickly. The human brain is very slow, but every part of its net of <a class="thought" href="entries/computation_entry.html">computation</a> works simultaneously. We have about a hundred billion neurons, each of which has an average of a thousand connections to other neurons. Because all these connections can perform their computations at the same time, the brain can perform about a hundred trillion simultaneous computations. So, although human neurons are very slow--in fact about a million times slower than <a class="thought" href="entries/electronic_entry.html">electronic</a> circuits--this massive parallelism more than makes up for their slowness. Although each interneuronal connection is capable of performing only about two hundred computations each second, a hundred trillion computations being performed at the same time add up to about twenty million billion calculations per second, give or take a couple of orders of magnitude.</p>
<p>Calculations like these are a little different than conventional <a class="thought" href="entries/computer_entry.html">computer</a> instructions. At the present time, we can simulate on the order of two billion such neural-connection calculations per second on dedicated machines. That's about ten million times slower than the human brain. A factor of ten million is a big factor and is one <a class="thought" href="entries/reason_entry.html">reason</a> why present computers are dramatically more brittle and restricted than human <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. Some observers looking at this difference conclude that human <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is so much more supple and wide-ranging than <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> that the gap can never be bridged.</p>
<p>Yet a factor of ten million, particularly of the kind of massive <a class="thought" href="entries/parallel_processing_entry.html">parallel processing</a> the human brain employs, will be bridged by <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a> in about two decades. Of course, matching the raw computing speed and <a class="thought" href="entries/memory_entry.html">memory</a> <a class="thought" href="entries/capacity_entry.html">capacity</a> of the human brain--even if implemented in massively parallel architectures--will not automatically result in human level <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. The <a class="thought" href="entries/architecture_entry.html">architecture</a> and organization of these resources is even more important than the <a class="thought" href="entries/capacity_entry.html">capacity</a>. There is, however, a source of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> we can tap to accelerate greatly our efforts to design <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. That source is the human brain itself. Probing the brain's circuits will let us, essentially, copy a proven design--that is, reverse engineer one that took its original designer several billion years to develop. (And it's not even copyrighted, at least not yet.)</p>
<p>This may seem like a daunting effort, but ten years ago so did the <a class="thought" href="entries/human_genome_entry.html">Human</a> <a class="thought" href="entries/genome_entry.html">Genome Project</a>. Nonetheless, the entire human genetic code will soon have been scanned, recorded, and analyzed to accelerate our understanding of the human biogenetic <a class="thought" href="entries/system_entry.html">system</a>. A similar effort to scan and record (and perhaps to understand) the neural organization of the human brain could perhaps provide the templates of <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. As we approach the computational ability needed to simulate the human brain--we're not there today, but we will be early in the next century--I believe researchers will initiate such an effort.</p>
<p>There are already precursors of such a project. For example, a few years ago <a class="thought" href="entries/mead_entry.html">Carver Mead</a>'s company, Synaptics, created an artificial <a class="thought" href="entries/retina_entry.html">retina</a> chip that is, essentially, a <a class="thought" href="entries/silicon_entry.html">silicon</a> copy of the neural organization of the human <a class="thought" href="entries/retina_entry.html">retina</a> and its visual-processing layer. The Synaptics chip even uses digitally controlled <a class="thought" href="entries/analog_entry.html">analog</a> processing, as the human brain does.</p>
<p>How are we going to conduct such a scan? Again, although a full discussion of the issue is beyond the scope of this chapter, we can mention several approaches. A "destructive" scan could be made of a recently deceased frozen brain; or we could use high-speed, high-resolution magnetic resonance imaging (<a class="thought" href="entries/mri_entry.html">MRI</a>) or other noninvasive scanning <a class="thought" href="entries/technology_entry.html">technology</a> on the living brain. <a class="thought" href="entries/mri_entry.html">MRI</a> scanners can already image <a class="thought" href="entries/individual_entry.html">individual</a> somas (i.e., <a class="thought" href="entries/neuron_entry.html">neuron</a> cell bodies) without disturbing living tissue. The more-powerful MRIs being developed will be capable of scanning <a class="thought" href="entries/individual_entry.html">individual</a> nerve fibers only ten microns in diameter. Eventually, we will be able to automatically scan the presynaptic vesicles (i.e., the synaptic strengths) believed to be the site of human <a class="thought" href="entries/learning_entry.html">learning</a>.</p>
<p>This ability suggests two scenarios. The first is that we could scan portions of a brain to ascertain the <a class="thought" href="entries/architecture_entry.html">architecture</a> of interneuronal connections in different regions. The exact position of each nerve fiber is not as important as the overall <a class="thought" href="entries/pattern_entry.html">pattern</a>. Using this <a class="thought" href="entries/information_entry.html">information</a>, we could design simulated neural nets that will operate in a similar fashion. This process will be rather like peeling an onion as each layer of human <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is revealed. That is essentially the procedure Synaptics has followed. They copied the essential <a class="thought" href="entries/analog_entry.html">analog</a> <a class="thought" href="entries/algorithm_entry.html">algorithm</a> called <i>center surround filtering</i> also found in the first layers of mammalian neurons.</p>
<p>A more difficult, but still ultimately feasible, scenario would be to noninvasively scan someone's brain to map the locations, interconnections, and contents of the somas, axons, dendrites, presynaptic vesicles, and other neural components. The entire organization of the brain--including the contents of its <a class="thought" href="entries/memory_entry.html">memory</a>--could then be re-created on a <a class="thought" href="entries/neural_computer_entry.html">neural computer</a> of sufficiently high <a class="thought" href="entries/capacity_entry.html">capacity</a>.</p>
<p>Today we can peer inside someone's brain with <a class="thought" href="entries/mri_entry.html">MRI</a> scanners whose resolution increases with each new generation. However, a <a class="thought" href="entries/number_entry.html">number</a> of technical challenges in complete brain-mapping--including achieving suitable resolution, <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a>, lack of vibration, and safety--remain. For a variety of reasons, it will be easier to scan the brain of someone recently deceased than a living brain. Yet noninvasively scanning a living brain will ultimately become feasible as the resolution and speed of <a class="thought" href="entries/mri_entry.html">MRI</a> and other scanning technologies improve. Here too the driving force behind <a class="thought" href="entries/future_entry.html">future</a> rapid improvements is <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>, because building high-resolution three-dimensional images quickly from the raw data an <a class="thought" href="entries/mri_entry.html">MRI</a> scanner produces requires massive computational ability.</p>
<p>Perhaps you think this discussion is veering off into the realm of <a class="thought" href="entries/science_fiction_entry.html">science fiction</a>. Yet, a hundred years ago, only a handful of writers attempting to predict the technological developments of this past century foresaw any of the major forces that have shaped our era: computers, <a class="thought" href="javascript:loadBrain('Moore\'s%20Law')">Moore's law</a>, radio, television, atomic <a class="thought" href="entries/energy_entry.html">energy</a>, lasers, <a class="thought" href="entries/bioengineering_entry.html">bioengineering</a>, or most electronics--to mention a few. The century to come will undoubtedly bring many technologies we would have similar difficulty envisioning, or even comprehending today. The important point here is, however, that the projection I am making now does not contemplate any revolutionary breakthrough; it is a modest extrapolation of current trends based on technologies and capabilities that we have today. We can't yet build a brain like <a class="thought" href="entries/hal_entry.html">HAL</a>'s, but we can describe right now how we could do it. It will take longer than the time needed to build a <a class="thought" href="entries/computer_entry.html">computer</a> with the raw computing speed of the human brain, which I believe we will do by around 2020. By sometime in the first half of the next century, I predict, we will have mapped the neural circuitry of the brain.</p>
<p>Now the ability to <a class="thought" href="entries/download_entry.html">download</a> your mind to your <a class="thought" href="entries/personal_computer_entry.html">personal computer</a> will raise some interesting issues. I'll only mention a few. First, there's the philosophical issue. When people are scanned and then re--created in a <a class="thought" href="entries/neural_computer_entry.html">neural computer</a>, who will the people in the <a class="thought" href="entries/machine_entry.html">machine</a> be? The answer will depend on whom you ask. The "<a class="thought" href="entries/machine_entry.html">machine</a> people" will strenuously claim to be the original persons; they lived certain lives, went through a scanner here, and woke up in the <a class="thought" href="entries/machine_entry.html">machine</a> there. They'll say, "Hey, this <a class="thought" href="entries/technology_entry.html">technology</a> really works. You should give it a try." On the other hand, the people who were scanned will claim that the people in the <a class="thought" href="entries/machine_entry.html">machine</a> are impostors, different people who just <i>appear</i> to share their memories, histories, and personalities.</p>
<p>A related issue is whether or not a re-created mind--or any intelligent <a class="thought" href="entries/machine_entry.html">machine</a> for that <a class="thought" href="entries/matter_entry.html">matter</a>--is conscious. This question too goes beyond the scope of the chapter, but I will venture a brief comment. There is, in fact, no objective test of another <a class="thought" href="entries/entity_entry.html">entity</a>'s <a class="thought" href="entries/subjective_experience_entry.html">subjective experience</a>; it can argue convincingly that it feels joy and pain (perhaps it even "feels your pain"), but that is not proof of its <a class="thought" href="entries/subjective_experience_entry.html">subjective experience</a>. <a class="thought" href="entries/hal_entry.html">HAL</a> himself makes such a claim when he responds to the BBC interviewer's question.</p>
<p>
<i>HAL</i>: I am putting myself to the fullest possible use, which is all I think that any conscious <a class="thought" href="entries/entity_entry.html">entity</a> can ever hope to do.</p>
<p>Of course, <a class="thought" href="entries/hal_entry.html">HAL</a>'s telling us he's conscious, doesn't settle the issue, as Dan Dennett's engaging discussion of these issues demonstrates (see chapter 16).</p>
<p>Then there's the ethical issue. Will it be immoral, or even illegal, to cause pain and suffering to your <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a>? Again, I refer the reader to Dennett's chapter. Few of us worry much about these issues now for our most advanced programs today are comparable to the minds of insects. However, when they attain the <a class="thought" href="entries/complexity_entry.html">complexity</a> and subtlety of the human mind--as they will in a few decades--and when they are in fact derived from human minds or portions of human minds, this will become a pressing issue.</p>
<p>Before Copernicus, our speciecentricity was embodied in the idea that the <a class="thought" href="entries/universe_entry.html">universe</a> literally circled around us as a testament to our unique status. We no longer see our uniqueness as a <a class="thought" href="entries/matter_entry.html">matter</a> of celestial relationships but of <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. Many people see <a class="thought" href="entries/evolution_entry.html">evolution</a> as a billion-year drama leading inexorably to its grandest creation: human <a class="thought" href="entries/intelligence_entry.html">intelligence</a>. Like the Church fathers, we are threatened by the specter of <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> that competes with its creator.</p>
<p>We cannot separate the full range of human <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and <a class="thought" href="entries/intelligence_entry.html">intelligence</a> from the ability to understand human <a class="thought" href="entries/language_entry.html">language</a>, spoken or otherwise. Turing recognized this when, in his famous <a class="thought" href="entries/turing_test_entry.html">Turing test</a>, he made <a class="thought" href="entries/communication_entry.html">communication</a> through <a class="thought" href="entries/language_entry.html">language</a> the means of ascertaining whether a human-level <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is a <a class="thought" href="entries/machine_entry.html">machine</a> or a person. <a class="thought" href="entries/hal_entry.html">HAL</a> understands human spoken <a class="thought" href="entries/language_entry.html">language</a> about as well as a person; at least that's the impression we get from the movie. Achieving this level of <a class="thought" href="entries/machine_entry.html">machine</a> proficiency is not the threshold we stand on today. Still, machines are quickly gaining the ability to understand what we say, as long as we stay within certain limited but useful domains. Until <a class="thought" href="entries/hal_entry.html">HAL</a> comes along, we will be talking to our computers to dictate written documents, obtain <a class="thought" href="entries/information_entry.html">information</a> from data bases, command a diverse array of tasks, and interact with an environment that increasingly intertwines human and <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a>.</p>
<p>
<a class="thought" href="entries/copyright_entry.html">Copyright</a> (<a class="thought" href="entries/c_entry.html">C</a>) 1997. Reproduced with permission from MIT Press.</p>
<p>
<a href="http://web.archive.org/web/20100613223450/http://mitpress.mit.edu/catalog/item/default.asp?sid=28BCF92C-760F-4869-A5C6-A0405E2D6E26&amp;ttype=2&amp;tid=5792" target="_new">HAL's Legacy</a>
</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12548" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id12549"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>When will HAL understand what we are saying?<br><span class="mindxheader"><i>posted on 12/12/2002 1:06 AM by Art_Intell</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D12548%23id12549" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12549" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>When will HAL understand what we are saying? Computer speech recognition and understanding.
<br>
			By Raymond Kurzweil
<br>
<br>
Originally published 1996 in Hal's Legacy: 2001's Computer as Dream and Reality. Published on KurzweilAI.net August 6, 2001.
<br>
<br>
<br>
"Lesson 1: Knowledge is a many-layered thing"
<br>
<br>
With regard to this explanation: I agree that knowledge is a many-layered thing. However, I do not agree with the opening sentence. To wit, " Thus lesson number one for constructing a computer system that can understand human speech is to build-in knowledge at many levels: the structure of speech sounds, the way speech is produced by our vocal apparatus, the patterns of speech sounds that comprise dialects and languages, the complex ( and not fully understood ) rules of word usage, and the--greatest difficulty--general knowledge of the subject matter being spoken about."
<br>
<br>
With all due respect, I strongly disagree with this approach. I agree that knowledge is a many-layered thing. However, if we are to create a computer that is "artificially intelligent" we need to "put first things first." First of all, using a human baby as an example, we need to separate the "mechanism" from the "content" regarding its brain. A baby, any baby, from anywhere in the world relocated to America and raised in our educational system and our social environment will learn to deal with the ambiguities of our language. Thus, we do not and should not "build in knowledge at many levels, the structure of speech sounds, the patterns, the rules, etc.," but should concentrate instead on building the mechanism to remember, recall, and apply the structure, patterns and rules. For instance, if the rules are programmed then when the rules change a programmer must physically alter the program with the "new" rule. However, if we construct the mechanism correctly then when the rules change the mechanism will alter itself to accommodate the "new" rule and apply it correctly in the appropriate circumstances.
<br>
<br>
<br>
"Lesson 2: The unpredictability of human speech"
<br>
<br>
"We apply, therefore, a process called normalization, in which we make all words the same loudness so as to eliminate this noninformative source of variability." If I get to a point where I'm increasing the volume of someone's name there may be a very good reason why. For instance, if you are in a situation where you are exposed to an increasing risk of being physically hurt I may call your name at an increasing level because you are not responding correctly or appropriately to the impending danger. The reverse of this is also a possibility. If I increase the volume of your name I may be in a situation where my safety is being threatened and I need your assistance. To arbitrarily normalize speech in a situation where there is impending danger is an incorrect approach. Another interpretation is the situation where I want someone else to follow my direction and I am being ignored. I will raise my voice in an attempt to get that person to respond to my direction, i.e., when an irritating four year old is ignoring me when I tell them to stop pulling the cats tail. These are not noninformative sources of variability.
<br>
<br>
<br>
"Playing HAL"
<br>
<br>
"In 1997 we appreciate that speech recognition does not exist in a vacuum but has to be integrated with other levels and sources of knowledge." Exactly! This is an example of what I am trying to explain. We didn't know how to create the underlying intelligence so we arbitrarily program the mechanism to respond the way we want'not respond on its own. We program the patterns, we program the rules, and we program...and we program...and we program.
<br>
<br>
<br>
"How to Build a Speech Recognizer"
<br>
<br>
"Software today is not an isolated field, but one that encompasses and codifies every other field of endeavor. Everyone-- librarians, musicians, magazine publishers, doctors, graphic artists, architects, researchers of every kind--are digitizing their knowledge bases, methods, and expressions of their work. Those of us working on speech understanding are experiencing the same rapid change, as hundreds of scientists and engineers build increasingly elaborate data bases and structures to describe our knowledge of speech sounds, phonetics, linguistics, syntax, semantics, and pragmatics--in accordance with Lesson 1."
<br>
<br>
Has anyone considered what we could build if instead of programming a computer to produce what we want, we spend our creative time and effort on producing a program that has the capability of learning on its own what we want it to learn, i.e., phonetics, linguistics, syntax, semantics, and pragmatics--in accordance with Lesson 1, of course? The only thing being accomplished by everyone who is "digitizing their knowledge bases, methods, and expressions of their work" is that they are making an electronic recording of their knowledge, methods/processes, and expressions of their work. Big deal. What we should be doing is teaching this knowledge, these methods/processes, and expressions of their work to a mechanism that can remember, recall, analyze, cogitate, and respond rationally, and appropriately. Anybody can load up a database, write a program that reads the database, and program the computer to respond in a particular way based on certain programmed conditions. Has anybody thought about programming a computer to respond "outside the box?" I haven't found anyone even discussing this.
<br>
<br>
I'm not criticizing the methods used in building a speech recognizer. We don't fully understand the one we have and use everyday of our lives. So, we've done the next best thing. We created an emulation of the process with the tools and understanding available. In fact, the current level of speech recognition is very impressive. The current understanding of the process of the personal version is explained in "Wet Mind - The New Cognitive Neuroscience" by Stephen M. Kosslyn and Oliver Koenig, copyright 1992. A brief list of the auditory areas and connections: cochlea, cochlear nuclei, superior olive, lateral lemniscus, inferior colliculus, superior colliculus, medial geniculate body, temporal lobe, Heschl's gyrus, and the lateral fissure, obviously indicate a layered, modular mechanism. So, it's not surprising that Kurzweil Applied Intelligence implemented a successful, layered, modular approach.
<br>
<br>
However, I am criticizing what's being done with the product of the recognition system. We are currently involved in a process of programming a computer to respond the way we want it to respond and we are calling the end result "artificial intelligence." I don't believe this process is what we originally perceived when confronted with the concept of artificial intelligence. I, for one, really thought HAL was responding entirely on "his" own, not regurgitating an already programmed response to an already anticipated situation. According to Webster's New World Dictionary of the American Language, Second College Edition, copyright 1982 intelligence is 1. a) the ability to learn or understand from experience; ability to acquire and retain knowledge; mental ability b) the ability to respond quickly and successfully to a new situation; use of the faculty of reason in solving problems, directing conduct, etc. effectively. This is what I believe we should be working on along with all the other aspects already underway.
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id12550"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: When will HAL understand what we are saying?<br><span class="mindxheader"><i>posted on 12/12/2002 1:55 AM by Grant</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D12548%23id12550" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12550" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>&gt;"Lesson 2: The unpredictability of human speech" 
<br>
<br>
If human speech were unpredictable, we wouldn't be able to understand it.  It's an encoding system for ideas.  What we learn as children is how to use the code to encode and decode certain kinds of sounds that translate into thoughts.
<br>
<br>
Grant</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id12650"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: When will HAL understand what we are saying?<br><span class="mindxheader"><i>posted on 12/15/2002 8:46 PM by ArtIntell</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D12548%23id12650" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12650" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>The believe the unpredictability referred to in "Lesson 2: The Unpredictability of Human Speech" is with regard to spectrograms of the spoken words. "Even the same person pronouncing the same word at different times can produce quite different spectrograms. Yet, there must be something about these different sound pictures that is the same; otherwise we humans and HAL, as a human-level machine, would be unable to identify them as two examples for the same spoken word."
<br>
<br>
Grant, the coding scheme you refer to is, I believe, something that we use rather automatically, without any conscious thought or effort.
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id12732"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: When will HAL understand what we are saying?<br><span class="mindxheader"><i>posted on 12/18/2002 3:25 AM by Dimitry V</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D12548%23id12732" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12732" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Sounds like a spectrogram is the wrong tool for finding the right patterns. But I suppose that Kurzweil had a hammer and was looking for nails.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id12659"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: When will HAL understand what we are saying?<br><span class="mindxheader"><i>posted on 12/16/2002 9:33 AM by griffman</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D12548%23id12659" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12659" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Building a program that can learn to the level that is required to understand language is one the biggest problems we are trying to solve. Current methods for developing speech recognition are using methods and technologies that are available today. If we had a sufficient learning program then teaching it to speak would be as easy as teaching anything to speak. we can teach parots to speak to some extent because a parrot can learn to some extent.
<br>
<br>
I see a trend forming around the idea that when we do get a general AI system that can learn, it will be alot like an infant in many respects and teaching it will be very similar to teaching an infant...  what it is taught may be the most important decision we make as a species.
<br>
<br>
along the idea of the mechanics of speech, It may be just as beneficial to create a better voice simulator. for as long as its been around ( I know of appletalk which was around mid 80's) there hasn't been much improvement in making a computer talk like a human, well publically anyway. I believe that initially a graphical representation of the tongue movements in the mouth were used to improve the accuracy of the "human" sound of the voice. with the greater processing of  general computation as well as the significant improvements in 3d rendering, has anyone gone back and made a better working model of the human voice box? a general idea about voice mechanics is that the variations in our shape determine what our voice sounds like. this can even be taken out of the human species and applied to other creatures, i.e. a dog cannot speak like a human because it physically lacks the equipment. if the computer does not have a good representation of the voice mechanism, how can it speak effectively?
<br>
<br>
I think this is important for the computer to learn how to speak fluently. infants, of all creatures, learn their specific creature language through mimicry and observation. A child see's how their mother makes a sound and trys to make the same sound. because the voice mechanisms are relatively  the same, you get the relative same sound. at first this comes out as incoherent noises, but they are very important. the next step in the mimic system is assigning thoughts and objects to the sounds. with those two concepts in place. learning to speak fluently is left to the ability to learn at all.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id12731"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: When will HAL understand what we are saying?<br><span class="mindxheader"><i>posted on 12/18/2002 3:22 AM by Dimitry V</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D12548%23id12731" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100613223450/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D12731" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I thought that one of the RealAudio codecs uses a model of the throat and mouth, but I could be wrong. I don't think this is a big deal, though. But you could always give the AI robot an artificial lung, throat, tongue, and lip system. If you really wanted to.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100613223450im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>