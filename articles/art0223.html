<html>
<head><base href="file:///Users/beka/Projects/Brain%20Archive/brain_archive/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>The coming superintelligence: who will be in control?</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/meme/memelist.html?m=1">The Singularity</a> &gt; 
The coming superintelligence: who will be in control?
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20060925135926/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0223.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0223.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/articles/art0223.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">The coming superintelligence: who will be in control?</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
Amara D. Angelica<br></span></td>
</table>
<br>
<div class="TeaserText">At some point in the next several decades, as machines become smarter than people, they'll take over the world. Or not. What if humans get augmented with smart biochips, wearables, and other enhancements, accessing massive knowledge bases ubiquitously and becoming supersmart cyborgs who stay in control by keeping machines specialized? Or what if people and machines converge into a mass-mind superintelligence?</div>
<br>
<br><p>Originally published July 25, 2001 on KurzweilAI.net.</p>
<p>Panelists at the recent <a href="http://web.archive.org/web/20060925135926/http://www.extropy.org/ex5/extro5.htm" target="_new">EXTRO-5 conference</a> in San Jose thrashed out these and other mind-stretching scenarios in a session on "Convergent or Divergent Super-<a class="thought" href="entries/intelligence_entry.html">Intelligence</a>?".</p>
<p>Experts disagree on what "<a class="thought" href="entries/superintelligence_entry.html">superintelligence</a>" (SI) means (see <a href="#SI" target="_self">What is</a> <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a>?), but the panelists shared a concern about the possibility of a "hard takeoff," in which "brains in a box" use their <a class="thought" href="entries/intelligence_entry.html">intelligence</a> to improve themselves enough to achieve SI, rocket far beyond humans, and gain enormous power, a <a class="thought" href="entries/dystopian_entry.html">dystopian</a> view often seen scifi movies, from The Forbin Project to The <a class="thought" href="entries/matrix_entry.html">Matrix</a>.</p>
<p>"If we believe in a hard takeoff, it's going to change our views on what safeguards we need for that <a class="thought" href="entries/software_entry.html">software</a>, what <a class="thought" href="entries/ethics_entry.html">ethics</a> we have to instill, and are we going to support such <a class="thought" href="entries/research_entry.html">research</a>," advised <a class="thought" href="entries/computation_entry.html">computation</a>al neuroscientist <a href="http://web.archive.org/web/20060925135926/http://www.d.kth.se/~nv91-asa/main.html" target="_new">Anders Sandberg</a>.</p>
<p>But he discounted the likelihood of a <a class="thought" href="entries/machine_entry.html">machine</a> with "general <a class="thought" href="entries/intelligence_entry.html">intelligence</a>" (with <a class="thought" href="entries/human_entry.html">human</a>-like understanding and capable of <a class="thought" href="entries/learning_entry.html">learning</a>) being developed in the near <a class="thought" href="entries/future_entry.html">future</a>. <a class="thought" href="entries/reason_entry.html">Reason</a>s: very few <a class="thought" href="entries/research_entry.html">research</a>ers working on it ("it's more profitable to make paperclips for <a class="thought" href="entries/microsoft_entry.html">Microsoft</a>"), special-<a class="thought" href="entries/intelligence_entry.html">intelligence</a> <a class="thought" href="entries/machine_entry.html">machine</a>s (special-purpose <a class="thought" href="entries/system_entry.html">system</a>s, like <a class="thought" href="entries/ibm_entry.html">IBM</a>'s planned Blue Gene) are more effective, and it takes a long <a class="thought" href="entries/time_entry.html">time</a> to train general-<a class="thought" href="entries/intelligence_entry.html">intelligence</a> <a class="thought" href="entries/system_entry.html">system</a>s because of the "Bias-Variance dilemma" (a <a class="thought" href="entries/machine_entry.html">machine</a> can learn faster if there's a narrow <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of things to learn, but then it's limited in its <a class="thought" href="entries/intelligence_entry.html">intelligence</a>).</p>
<p>In addition, "there doesn't seem to be a strong <a class="thought" href="entries/evolution_entry.html">evolution</a>ary trend toward general <a class="thought" href="entries/intelligence_entry.html">intelligence</a>," Sandberg said. "Successful <a class="thought" href="entries/animal_entry.html">animal</a>s are not very smart." The exception: the rise of <a class="thought" href="entries/homo_sapiens_entry.html">homo sapiens</a>.<br>
<img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/articles/images/angelicaextro01.gif" vspace="10"><br>
<span class="PhotoCredit">Source: Anders Sandberg</span>
<br>
<span class="Caption">Special-<a class="thought" href="entries/intelligence_entry.html">intelligence</a> <a class="thought" href="entries/ai_entry.html">AI</a> (red) vs. general-<a class="thought" href="entries/intelligence_entry.html">intelligence</a> <a class="thought" href="entries/ai_entry.html">AI</a> (green)</span>
<br>
</p>
<p>A more likely path, he said, is a more gradual, controllable "soft takeoff" via <a class="thought" href="entries/intelligence_entry.html">intelligence</a> <a class="thought" href="entries/augmentation_entry.html">augmentation</a> (IA)--"<a class="thought" href="entries/human_entry.html">human</a> enhancement through a plurality of intelligent but very specialized <a class="thought" href="entries/system_entry.html">system</a>s," such as <a class="thought" href="entries/agent_entry.html">agent</a> <a class="thought" href="entries/program_entry.html">program</a>s, wearables, and <a class="thought" href="entries/brain_entry.html">brain</a>-enhancing <a class="thought" href="entries/device_entry.html">device</a>s. "When general <a class="thought" href="entries/intelligence_entry.html">intelligence</a> appears, it will be integrated into this <a class="thought" href="entries/system_entry.html">system</a> because it's such a useful extra capability."</p>
<p>This will also be a society-wide <a class="thought" href="entries/event_entry.html">event</a>, he explained later, "not something driven by single projects or highly augmented individuals, but more like the race onto the <a class="thought" href="entries/internet_entry.html">Internet</a>."</p><h1><a class="thought" href="entries/ai_entry.html">AI</a> vs. IA</h1><p><a class="thought" href="entries/ai_entry.html">AI</a> expert <a href="http://web.archive.org/web/20060925135926/http://optimal.org/" target="_new">Peter Voss</a> was not convinced. "There are huge barriers to working with <a class="thought" href="entries/wetware_entry.html">wetware</a>," he said. "Working with humans is much more difficult than working with <a class="thought" href="entries/machine_entry.html">machine</a>s. Who cares if you hack a <a class="thought" href="entries/machine_entry.html">machine</a>, kill it, reboot it, and copy it or whatever? Try doing that to humans; you'll have some real opposition there. <a class="thought" href="entries/computer_entry.html">Computer</a>s smart enough to help us dramatically enhance ourselves will be more than smart enough to radically enhance themselves."</p>
<p>There are many advantages of working with <i>artificial</i> <a class="thought" href="entries/system_entry.html">system</a>s rather than <a class="thought" href="entries/wetware_entry.html">wetware</a>, he added. "It's easier to experiment, we can try millions of different designs. You can't do that with <a class="thought" href="entries/human_entry.html">human</a> brains. [Biological] <a class="thought" href="entries/neuron_entry.html">neuron</a>al <a class="thought" href="entries/learning_entry.html">learning</a> speeds are much slower than <a class="thought" href="entries/computer_entry.html">computer</a>s, and that gap will widen. There's an easy upgrade path. As soon as we get the extra <a class="thought" href="entries/hardware_entry.html">hardware</a> speed and capacity we can utilize it."</p>
<p>There also are advantages in <a class="thought" href="entries/engine_entry.html">engine</a>ered vs. blind <a class="thought" href="entries/evolution_entry.html">evolution</a>, Voss pointed out. "We don't have to limit ourselves to dealing with <a class="thought" href="entries/biological_entry.html">biological</a> evolved <a class="thought" href="entries/system_entry.html">system</a>s. We can capitalize on our <a class="thought" href="entries/engine_entry.html">engine</a>ering and cognitive strengths. We can design an airplane with thrust rather than flapping wings. We can have a more logical flow-chart design with <a class="thought" href="entries/debugging_entry.html">debugging</a> tools built. And we don't have the <a class="thought" href="entries/evolution_entry.html">evolution</a>ary baggage to worry about, such as epigenesis (the theory that an individual is developed by successive differentiation of an unstructured egg) and reproduction." In addition, he said, it's slow and painful to build specialized <a class="thought" href="entries/computer_entry.html">computer</a>s and less flexible.</p>
<p>"As Ray [Kurzweil] mentioned, [an <a class="thought" href="entries/engine_entry.html">engine</a>ering design] can be duplicated and mass-produced at low cost. We don't have to go through a 20-year training process. We've got 7x24 operation in <a class="thought" href="entries/machine_entry.html">machine</a>s. How much of their <a class="thought" href="entries/time_entry.html">time</a> will humans give you a day? And as <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> increases it will become a lot cheaper than humans.</p>
<p>"Ray estimated that as early as 2010 we might have sufficient processing power in a <a class="thought" href="entries/supercomputer_entry.html">supercomputer</a> to equal a <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a>. I think a lot more processing power could be utilized if people believed that general <a class="thought" href="entries/intelligence_entry.html">intelligence</a> was a worthwhile pursuit. There's an enormous amount of processing power out there and there are huge areas of improvement in efficiency. Factors of 1000 are quite easily achieved by optimizing the <a class="thought" href="entries/hardware_entry.html">hardware</a> <a class="thought" href="entries/architecture_entry.html">architecture</a> for general <a class="thought" href="entries/intelligence_entry.html">intelligence</a> or <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> and having massively parallel, field-<a class="thought" href="entries/program_entry.html">program</a>mable arrays.</p>
<p>"Once people were convinced that was the way to go, I think factors of a million times the processing power we have right now could be achieved very quickly by more <a class="thought" href="entries/hardware_entry.html">hardware</a> being made available, more people working on the problem, and by having more specialized <a class="thought" href="entries/hardware_entry.html">hardware</a>.</p><h1>Global superbrain</h1><p>
<u>Marc Stiegler</u>[http://<a class="thought" href="entries/www_entry.html">www</a>.skyhunter.<a class="thought" href="entries/com_entry.html">com</a>/marc.html] took a very different approach. Stiegler was Acting President of Xanadu while he was VP of <a class="thought" href="entries/engine_entry.html">Engine</a>ering for Autodesk. In a seminal 1988 article, seminal 1987 article, "Hypermedia and the <a class="thought" href="entries/singularity_entry.html">Singularity</a>," in <i><a class="thought" href="entries/analog_entry.html">Analog</a></i> magazine, he predicted that global <a class="thought" href="entries/hypertext_entry.html">hypertext</a> <a class="thought" href="entries/system_entry.html">system</a>s would cause a quantum leap in augmenting <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/intelligence_entry.html">intelligence</a>--six years before the Web became a viral phenomenon.</p>
<p>His <a class="thought" href="entries/novel_entry.html">novel</a> <i>Earthweb</i>[http://<a class="thought" href="entries/www_entry.html">www</a>.the-earthweb.com] addresses the questions: How do you enable a billion people to work together as a tightly knit team? And how might our lives change when a mature version of the <a class="thought" href="entries/www_entry.html">World Wide Web</a> becomes the underpinning fabric of global society?<br>
<img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/articles/images/angelicaextro02.gif" vspace="10"><br>
<br>
<br>
</p>
<p>The first SI on this <a class="thought" href="entries/planet_entry.html">planet</a> will be "the organism that the <a class="thought" href="entries/www_entry.html">World Wide Web</a>, as it matures, will evolve into," he said. "A mature <a class="thought" href="entries/www_entry.html">World Wide Web</a> is secure, has bi-directional links, reputation-based <a class="thought" href="entries/system_entry.html">system</a>s, an idea <a class="thought" href="entries/future_entry.html">future</a>s market, 10 billion man-years of experience, and teramips of computing power; and it's a fairly entertaining <a class="thought" href="entries/system_entry.html">system</a>."</p>
<p>Perhaps most interestingly, in his vision, the Earthweb is also a truthful organism. "The more important a question is, the more forcefully it will <a class="thought" href="entries/search_entry.html">search</a> for the truth and the more brutally it will <a class="thought" href="entries/search_entry.html">search</a> for falsehoods." The Earthweb also "defends most kinds of property rights, such as <a class="thought" href="entries/digital_entry.html">digital</a> certificates, better than lawyers. It has a strong <a class="thought" href="entries/instinct_entry.html">instinct</a> for self-repair and a gentle <a class="thought" href="entries/sense_entry.html">sense</a> of self-preservation."</p>
<p>"It's maniacal about <a class="thought" href="entries/growth_entry.html">growth</a>. All of the individual cells are constantly looking for new buyers of current products and looking for opportunities to invent new products for new buyers and expand markets."</p>
<p>He compared the Earthweb to the Borg collective from <a class="thought" href="entries/star_trek_entry.html">Star Trek</a>, "which is also maniacal about <a class="thought" href="entries/growth_entry.html">growth</a>. But joining the Earthweb is voluntary, and you can leave any <a class="thought" href="entries/time_entry.html">time</a> and come back any <a class="thought" href="entries/time_entry.html">time</a>. The Earthweb is constantly inventing new <a class="thought" href="entries/reason_entry.html">reason</a>s to join and stay, so it's a much stronger organism than the Borg collective ever could be."</p>
<p>So what happens if the Earthweb runs into another SI--a <a class="thought" href="entries/brain_entry.html">brain</a> in a box? "With 10 billion man-years of experience, even the brainbox will find interaction with the Earthweb advantageous, at least for a few days. The brainbox becomes a part of the Earthweb organism and both are stronger for it. As the brainbox outgrows beyond the value of this offering, the Earthweb will create new offerings, quite possibly by deploying additional brainboxes for the first brainbox to work with."</p>
<p>As these brainboxes figure out better ways to cooperate with each other, the Earthweb "will happily build out new mechanisms for the brainboxes and they will all be absorbed into the Earthweb collective.</p>
<p>"For the doomsters of the world, I have the following warning: the Earthweb's goal is to prevent all of your dreams of <a class="thought" href="entries/planet_entry.html">planet</a>ary destruction and <a class="thought" href="entries/human_entry.html">human</a> misery from coming true. And the Earthweb is coming to a global village near you."</p>
<p>"Marc's cute description of 'SI meets Earthweb' assumes that the SI will be interested in being just a minor player in the overall <a class="thought" href="entries/network_entry.html">network</a>," Voss told us afterwards. "We enjoy playing with <a class="thought" href="entries/animal_entry.html">animal</a>s, but don't ultimately let them hold us back. This consideration would hold true both for an SI with 'a mind of its own' and for one under <a class="thought" href="entries/human_entry.html">human</a> control."</p>
<p>Does that mean an SI could take over the world? "Even if the <a class="thought" href="entries/brain_entry.html">brain</a>-in-a-box, hard-take-off super-<a class="thought" href="entries/intelligence_entry.html">intelligence</a> scenario comes about (before dramatic <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/augmentation_entry.html">augmentation</a>), it is not clear to me that such an SI would have the <i>motivation</i> to take over," says Voss. "It may simply do what it's told. This, of course, may be substantially more dangerous. Which group of humans will be controlling the SI? An independent, self-motivated--yet benign--SI would seem to represent a much better outcome.</p>
<p>"If the hard-take-off SI is not independent--if the SI does not have a mind of its own, or chooses not to exercise it--then some humans will control its awesome power. Without speculating who those people might be, I find this possibility rather unnerving. (Personally, I'm more afraid of <a class="thought" href="entries/government_entry.html">government</a>s than 'the rich'.)</p>
<p>"Assume that no substantial <a class="thought" href="entries/progress_entry.html">progress</a> on general <a class="thought" href="entries/ai_entry.html">AI</a> is achieved in the near <a class="thought" href="entries/future_entry.html">future</a>. Even in this case, it seems likely that as more processing power becomes available, and as more and more <i>specialized</i> <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is developed, we will come the point where what is needed for general <a class="thought" href="entries/intelligence_entry.html">intelligence</a> will become relatively obvious and easy. The hard takeoff will just be delayed. <a class="thought" href="entries/research_entry.html">Research</a>ers will recognize some <i>overall</i> patterns of what is important in developing different intelligent <a class="thought" href="entries/system_entry.html">system</a>s and various aspects of general <a class="thought" href="entries/intelligence_entry.html">intelligence</a> will be developed incidentally, as byproducts of specialized designs. In addition, large increases in processing power will allow less efficient designs to show some encouraging results. The sheer number of different intelligent applications designed, the increased number of people working in the field, together with vastly improved <a class="thought" href="entries/hardware_entry.html">hardware</a> will make it more likely that crucial aspects of general <a class="thought" href="entries/intelligence_entry.html">intelligence</a> will be 'stumbled upon.' At that stage, because of the substantial technological/ economic advantage, several groups are likely to vigorously pursue it."</p>
<p>In the meantime, general-<a class="thought" href="entries/intelligence_entry.html">intelligence</a> development is on hold until Voss and other enthusiasts can inspire more <a class="thought" href="entries/research_entry.html">research</a>. As he proposes on optimal.org: "Of all the people working in the field called "<a class="thought" href="entries/ai_entry.html">AI</a>" ...</p>
<ul>
<li>80% don't believe in the concept of General <a class="thought" href="entries/intelligence_entry.html">Intelligence</a> (but instead, in a large collection of specific skills and <a class="thought" href="entries/knowledge_entry.html">knowledge</a>); of those that do,</li>
<li>80% don't believe that (super) <a class="thought" href="entries/human_entry.html">human</a>-level <a class="thought" href="entries/intelligence_entry.html">intelligence</a> is possible--either ever, or for a long, long time; of those that do,</li>
<li>80% work on domain-specific <a class="thought" href="entries/ai_entry.html">AI</a> projects for commercial or academic-political <a class="thought" href="entries/reason_entry.html">reason</a>s (results are a lot quicker); of those left,</li>
<li>80% have a poor conceptual framework."</li>
</ul><h1><a name="SI" target="_new"></a>What is <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a>?</h1><p>In <a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/articles/art0168.html" target="_new">How Long Before Superintelligence</a> <a class="thought" href="entries/transhuman_entry.html">transhuman</a>ist philosopher <a class="thought" href="entries/bostrom_entry.html">Nick Bostrom</a> defines <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a> (SI) as "an intellect that is much smarter than the best <a class="thought" href="entries/human_entry.html">human</a> brains in practically every field, including scientific <a href="" target="_new">creativity</a>, general <a href="" target="_new">wisdom</a> and social skills.</p>
<p>Ray Kurzweil would expand that definition to "combine these extra-<a class="thought" href="entries/human_entry.html">human</a> capabilities with the natural advantages of <a class="thought" href="entries/machine_entry.html">machine</a>s--<a class="thought" href="entries/knowledge_entry.html">knowledge</a> sharing, speed, and <a class="thought" href="entries/memory_entry.html">memory</a> capacity."</p>
<p>Peter Voss argues that "any <a class="thought" href="entries/entity_entry.html">entity</a> that (given the same <a class="thought" href="entries/knowledge_base_entry.html">knowledge base</a>) can easily solve problems that are far beyond <a class="thought" href="entries/human_entry.html">human</a> ability should be called 'superintelligent.' For example, speedier results could be achieved by more effective rather than faster <a class="thought" href="entries/thinking_entry.html">thinking</a>. One could imagine genetic <a class="thought" href="entries/engine_entry.html">engine</a>ering producing such entities." In addition, an SI "can acquire additional--and <a class="thought" href="entries/novel_entry.html">novel</a>--skills and abilities beyond those <a class="thought" href="entries/program_entry.html">program</a>med in or taught. '<a class="thought" href="entries/creativity_entry.html">Creativity</a>' touches on that, but may not be explicit enough: the words <a class="thought" href="entries/learning_entry.html">learning</a>, innovation, and discovery come to mind."</p>
<p>Bostrom's definition "leaves open how the <a href="" target="_new">superintelligence</a> is implemented," says Voss. "It could be a <a href="" target="_new">digital</a> <a href="" target="_new">computer</a>, a dramatically augmented <a class="thought" href="entries/human_entry.html">human</a>, some other <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/engine_entry.html">engine</a>ered or evolved <a class="thought" href="entries/entity_entry.html">entity</a>, or others forms that are hard to imagine today."</p>
<p>However, "<a class="thought" href="entries/superintelligence_entry.html">superintelligence</a> is another diffuse concept," says Anders Sandberg. "It is not obvious how to measure or compare forms of <a class="thought" href="entries/intelligence_entry.html">intelligence</a> with each other, but it is clear that there can exist both quantitative and quantitative differences. As Vinge has suggested, a dog mind transferred to a vastly faster <a class="thought" href="entries/hardware_entry.html">hardware</a> would still not be able to solve mathematical problems, regardless of the amount of canine <a class="thought" href="entries/education_entry.html">education</a> given to it, since its <a class="thought" href="entries/basic_entry.html">basic</a> structure likely does not allow this form of abstract <a class="thought" href="entries/thought_entry.html">thought</a>. Minds may have different abilities to find and exploit patterns.</p>
<p>"It may be more relevant to describe <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a> in terms of ability rather than in what structural way it is 'smart.' That is in itself an interesting question (there might be incompatible and very different forms of intelligent <a class="thought" href="entries/thinking_entry.html">thinking</a>) but what is particularly relevant is that the <a class="thought" href="entries/superintelligence_entry.html">superintelligence</a> is better at achieving its real-world goals than humans, given the same initial resources and <a class="thought" href="entries/knowledge_entry.html">knowledge</a>."</p>
<p>Marc Stiegler proposed a more topical definition of the term in his EXTRO-5 talk: "An <a class="thought" href="entries/intelligence_entry.html">intelligence</a> so great it can make California politicians understand the law of <a class="thought" href="entries/energy_entry.html">energy</a> supply and demand."</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1622" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id1623"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>smart machines<br><span class="mindxheader"><i>posted on 07/30/2001 11:08 PM by gxm21@psu.edu</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id1623" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1623" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Machines will become smarter than humans but not smarter than humans who know how to use machines. The Global Brain metaphor treats the global network of humans connected to machines and to each other as an emergent global complex system. Asking who will be "in control" of it is like asking which cells are in control of the body. 
<br>
These and other issues were discussed during "GBrain-0", a recent workshop in Brussels (http://pespmc1.vub.ac.be/GB/), with a webcast by Complexity Digest (http://www.comdig.de/Conf/GB0/)
<br>
<br>
Gottfried J. Mayer, Ph.D.
<br>
Editor, Complexity Digest</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1703"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: smart machines<br><span class="mindxheader"><i>posted on 08/02/2001 5:20 PM by artisu@yahoo.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id1703" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1703" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>It takes ignorance of Kurzweil to talk about
<br>
"machines becoming smarter than humans in decades",
<br>
he has a reason - making money on this hype.
<br>
did you have a reason to waste time reading 
<br>
his trash ? 
<br>
<br>
rather read good sci-fi, or scientific work,
<br>
not Kurzweiliada ..</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1704"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: smart machines<br><span class="mindxheader"><i>posted on 08/02/2001 6:09 PM by mike99@lascruces.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id1704" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1704" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Kurzweil has an impressive record as an inventor and thinker. Perhaps that's why he is predicting these AI developments. Of course, you may believe otherwise. But must you do it in such a foul, disagreeable way?</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1727"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: smart machines<br><span class="mindxheader"><i>posted on 08/03/2001 2:16 PM by trinitytherat@cs.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id1727" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1727" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>&gt;It takes ignorance of Kurzweil to talk about 
<br>
&gt;"machines becoming smarter than humans in &gt;decades", 
<br>
&gt;he has a reason - making money on this hype. 
<br>
&gt;did you have a reason to waste time reading 
<br>
&gt;his trash ? 
<br>
&gt;rather read good sci-fi, or scientific work, 
<br>
&gt;not Kurzweiliada ..
<br>
<br>
Stumbled upon the wrong site huh? Why not try:
<br>
www.dacinghamsters.com
<br>
<br>
You might find material there better suited to your intellectual calliber;-)</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1729"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: smart machines<br><span class="mindxheader"><i>posted on 08/03/2001 2:22 PM by sequoia5150@cs.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id1729" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1729" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>"Machines will become smarter than humans but not smarter than humans who know how to use machines. The Global Brain metaphor treats the global network of humans connected to machines and to each other as an emergent global complex system. Asking who will be "in control" of it is like asking which cells are in control of the body." 
<br>
<br>
What an interesting analogy. I like it, but it still leaves out the possibility of machine to machine intelligence (a network of individually intelligent machines which excludes humans), which might surpass human-machine capability. </p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1774"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: smart machines<br><span class="mindxheader"><i>posted on 08/06/2001 7:19 AM by tomaz@techemail.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id1774" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1774" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>It is not that obvious, that will happen just this.  That will be no autonomous AI - independent of humans.
<br>
<br>
It is not that obvious - but true nevertheless. I only want to emphasize your point.
<br>
<br>
- Thomas Kristan
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id3807"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The coming superintelligence: who will be in control?<br><span class="mindxheader"><i>posted on 10/26/2001 10:34 PM by moonsinger13@yahoo.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id3807" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D3807" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Excuse my ignorance of the subject at hand, but in reference to the interaction of 'brain-in-box' SIs and the broader global SI(s), could such a meeting prove disasterous? Supposing either the brain SI or the global SI deems their counterpart a threat to its existence, what might possibly occur, and how would the consequences of such occurances affect humanity as a whole?  </p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id7794"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: The coming superintelligence: who will be in control?<br><span class="mindxheader"><i>posted on 06/28/2002 3:52 AM by sunny_day_432000@yahoo.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1622%23id7794" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20060925135926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D7794" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>There seems to be WAY too much speculatory literature, but very little hard science.  This is why transhumanism at the moment is a joke, it's at the intellectual level of a Hollywood movie.
<br>
<br>
Most transhumanists don't even have an understanding of the biology of human general intelligence.  How many of you know the current identified biological correlates of general intelligence, which is the type of intelligence IQ tests measure?  Have any of you studied the works of J. Phillipe Rushton, Linda S. Gottfredson, Richard Lynn, and Kevin MacDonald?  It seems like none of you know anything about the physiology/biology of human intelligence and behavior, yet you are bullshiting us with tall tales of a "Singularity."  Let me suggest some websites for you to actually learn something about human intelligence:
<br>
<br>
http://www.mugu.com/cgi-bin/Upstream/
<br>
http://lrainc.com/swtaboo/
<br>
http://www.mankind.org/
<br>
http://www.cycad.com/cgi-bin/pinc/index.html
<br>
http://www.charlesdarwinresearch.org/
<br>
http://www.csulb.edu/~kmacd/
<br>
http://www.rlynn.co.uk/
<br>
http://www.pioneerfund.org/
<br>
http://www.webcom.com/zurcher/thegfactor/index.html
<br>
http://www.theoccidentalquarterly.com/
<br>
http://www.wcotc.com/euvolution/euvolution/
<br>
http://www.wcotc.com/euvolution/articles.htm
<br>
http://www.eugenics.net/
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20060925135926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>