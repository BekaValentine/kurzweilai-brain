<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>Ethics for Machines</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/meme/memelist.html?m=4">Will Machines Become Conscious?</a> &gt; 
Ethics for Machines
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20100621142148/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0218.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0218.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/articles/art0218.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">Ethics for Machines</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0083.html" target="_top">J. Storrs Hall</a><br></span></td>
</table>
<br>
<div class="TeaserText">What are the ethical responsibilities of an intelligent being toward another one of a lower order? And who will be lower--us or machines? Nanotechnologist J. Storrs Hall considers our moral duties to machines, and theirs to us.</div>
<br>
<br><p>Originally published 2000 by <a class="thought" href="entries/hall_entry.html">J. Storrs Hall</a>. Published on KurzweilAI.net July 5, 2001.</p>
<blockquote>"A <a class="thought" href="entries/robot_entry.html">robot</a> may not injure a <a class="thought" href="entries/human_entry.html">human</a> being, or through inaction, allow a <a class="thought" href="entries/human_entry.html">human</a> to come to harm."</blockquote>
<blockquote>--<a class="thought" href="entries/asimov_entry.html">Isaac Asimov</a>'s First Law of <a class="thought" href="entries/robotics_entry.html">Robotics</a></blockquote>
<p>The first book report I ever gave, to Mrs. Slatin's first grade class in Lake, Mississippi in 1961, was on a slim volume entitled "You Will Go to the Moon". I have spent the intervening years <a class="thought" href="entries/thinking_entry.html">thinking</a> about the <a class="thought" href="entries/future_entry.html">future</a>.</p>
<p>The four decades that have passed have witnessed advances in <a class="thought" href="entries/science_entry.html">science</a> and physical <a class="thought" href="entries/technology_entry.html">technology</a> that would be incredible to a child of any other era. I did see my countryman Neil Armstrong step out onto the moon. The processing power of the <a class="thought" href="entries/computer_entry.html">computer</a>s that controlled the early launches can be had today in a $5 <a class="thought" href="entries/calculator_entry.html">calculator</a>. The genetic <a class="thought" href="entries/code_entry.html">code</a> has been broken and the messages are being read--and in some cases, rewritten. Jet travel, then a perquisite of the rich, is available to all.</p>
<p>That young boy that I was spent <a class="thought" href="entries/time_entry.html">time</a> on other things besides <a class="thought" href="entries/science_fiction_entry.html">science fiction</a>. My father was a minister, and we talked (or in many cases, I was lectured and questioned!) about good and evil, right and wrong, what were our duties to others and to ourselves.</p>
<p>In the same four decades, <a class="thought" href="entries/progress_entry.html">progress</a> in the realm of <a class="thought" href="entries/ethics_entry.html">ethics</a> has been modest. Almost all of it has been in the expansion of inclusiveness, broadening the definition of who deserves the same consideration you always gave your neighbor. I experienced some of this first hand as a schoolchild in '60's Mississippi. Perhaps the rejection of wars of adventure can also be counted. And yet those valuable advances to the contrary notwithstanding, <a class="thought" href="entries/ethics_entry.html">ethics</a>, and its blurry reflection <a class="thought" href="entries/politics_entry.html">politics</a>, has seemed to stand still compared to the advances of physical <a class="thought" href="entries/science_entry.html">science</a>. This is particularly true if we take the twentieth century as a whole--it stands alone in <a class="thought" href="entries/history_entry.html">history</a> as the "Genocide Century", the only <a class="thought" href="entries/time_entry.html">time</a> in <a class="thought" href="entries/history_entry.html">history</a> where <a class="thought" href="entries/government_entry.html">government</a>s killed their own people by the millions, not just once or in one place but repeatedly, all across the globe.</p>
<p>We can extend our vision with telescopes and microscopes, peering into the heart of the <a class="thought" href="entries/atom_entry.html">atom</a> and seeing back to the very creation of the <a class="thought" href="entries/universe_entry.html">universe</a>. When I was a boy, and vitally interested in dinosaurs, no one knew why they had died out. Now we do. We can map out the crater of the Chixulub meteor with sensitive gravitometers, charting the enormous structure below the ocean floor.</p>
<p>Up to now, we haven't had, or really needed, similar advances in "ethical <a class="thought" href="entries/instrument_entry.html">instrument</a>ation". The terms of the subject haven't changed. Morality rests on <a class="thought" href="entries/human_entry.html">human</a> shoulders, and if <a class="thought" href="entries/machine_entry.html">machine</a>s changed the ease with which things were done, they did not change responsibility for doing them. People have always been the only "moral agents".</p>
<p>Similarly, people are largely the objects of responsibility. There is a developing debate over our responsibilities to other living creatures, or <a class="thought" href="entries/species_entry.html">species</a> of them, which is unresolved in detail, and which will bear further discussion below. We have never, however, considered ourselves to have *moral* duties to our <a class="thought" href="entries/machine_entry.html">machine</a>s, or them to us.</p>
<p>All that is about to change.</p><h1>What Are <a class="thought" href="entries/machine_entry.html">Machine</a>s, Anyway?</h1><p>We have a naive notion of a <a class="thought" href="entries/machine_entry.html">machine</a> as a box with motors, gears, and whatnot in it. The most important <a class="thought" href="entries/machine_entry.html">machine</a> of the <a class="thought" href="entries/industrial_revolution_entry.html">industrial revolution</a> was the steam <a class="thought" href="entries/engine_entry.html">engine</a>, providing power to factories, locomotives, and ships. If we retain this notion, however, we will fall far short of an <a class="thought" href="entries/intuition_entry.html">intuition</a> capable of dealing with the <a class="thought" href="entries/machine_entry.html">machine</a>s of the <a class="thought" href="entries/future_entry.html">future</a>.</p>
<p>The most important <a class="thought" href="entries/machine_entry.html">machine</a> of the twentieth century wasn't a physical thing at all. It was the <a class="thought" href="entries/turing_machine_entry.html">Turing Machine</a>, and it was a mathematical idea. It provided the theoretical basis for <a class="thought" href="entries/computer_entry.html">computer</a>s. Furthermore, it established the principle that for higher functions such as <a class="thought" href="entries/computation_entry.html">computation</a>, it didn't <a class="thought" href="entries/matter_entry.html">matter</a> what the physical realization was (within certain bounds)--any <a class="thought" href="entries/computer_entry.html">computer</a> could do what any other <a class="thought" href="entries/computer_entry.html">computer</a> could, given enough <a class="thought" href="entries/memory_entry.html">memory</a> and <a class="thought" href="entries/time_entry.html">time</a>.</p>
<p>This theoretical concept of a <a class="thought" href="entries/machine_entry.html">machine</a> as a pattern of operations which could be implemented in a number of ways is called a virtual <a class="thought" href="entries/machine_entry.html">machine</a>. In modern <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/technology_entry.html">technology</a>, virtual <a class="thought" href="entries/machine_entry.html">machine</a>s abound. Successive versions of processor chips re-implement the virtual <a class="thought" href="entries/machine_entry.html">machine</a>s of their predecessors, so that the old <a class="thought" href="entries/software_entry.html">software</a> will still run. <a class="thought" href="entries/operating_system_entry.html">Operating system</a>s (e.g. <a class="thought" href="entries/windows_entry.html">Windows</a>) offer virtual <a class="thought" href="entries/machine_entry.html">machine</a>s to applications <a class="thought" href="entries/program_entry.html">program</a>s. Web <a class="thought" href="entries/browser_entry.html">browser</a>s offer several virtual <a class="thought" href="entries/machine_entry.html">machine</a>s (notably <a class="thought" href="entries/java_entry.html">Java</a>) to the writers of Web pages.</p>
<p>More importantly, any <a class="thought" href="entries/program_entry.html">program</a> running on a <a class="thought" href="entries/computer_entry.html">computer</a> is a virtual <a class="thought" href="entries/machine_entry.html">machine</a>. Usage in this <a class="thought" href="entries/sense_entry.html">sense</a> is a slight extension of that in <a class="thought" href="entries/computer_science_entry.html">computer science</a>, where the "<a class="thought" href="entries/machine_entry.html">machine</a>" in "virtual <a class="thought" href="entries/machine_entry.html">machine</a>" refers to a <a class="thought" href="entries/computer_entry.html">computer</a>, specifically an instruction <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> processor. Strictly speaking, <a class="thought" href="entries/computer_entry.html">computer</a> scientists should refer to "virtual processors", but they tend to refer to processors as "<a class="thought" href="entries/machine_entry.html">machine</a>s" anyway. For the purposes of our discussion here, we can call any <a class="thought" href="entries/program_entry.html">program</a> a virtual <a class="thought" href="entries/machine_entry.html">machine</a>. In fact, I will drop the "virtual" and call <a class="thought" href="entries/program_entry.html">program</a>s simply "<a class="thought" href="entries/machine_entry.html">machine</a>s". The essence of a <a class="thought" href="entries/machine_entry.html">machine</a>, for our purposes, is its behavior; what it does given what it senses (always assuming that there is a physical realization capable of actually doing the <a class="thought" href="entries/action_entry.html">action</a>s).</p>
<p>To understand just how complex the issue really is, let's consider a huge, complex, immensely powerful <a class="thought" href="entries/machine_entry.html">machine</a> we've already built. The <a class="thought" href="entries/machine_entry.html">machine</a> is the U.S. <a class="thought" href="entries/government_entry.html">Government</a> and legal <a class="thought" href="entries/system_entry.html">system</a>. It is a lot more like a giant <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a> than people realize. Really complex <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a>s are not sequences of instructions; they are sets of rules. This is explicit in the case of "<a class="thought" href="entries/expert_system_entry.html">expert system</a>s" and implicit in the case of distributed, object-oriented, interrupt-driven, <a class="thought" href="entries/network_entry.html">network</a>ed <a class="thought" href="entries/software_entry.html">software</a> <a class="thought" href="entries/system_entry.html">system</a>s. More to the point, sets of rules are <a class="thought" href="entries/program_entry.html">program</a>s--in our terms, <a class="thought" href="entries/machine_entry.html">machine</a>s.</p>
<p>Of course you will say that the <a class="thought" href="entries/government_entry.html">government</a> isn't *just* a <a class="thought" href="entries/program_entry.html">program</a>; it's under <a class="thought" href="entries/human_entry.html">human</a> control, isn't it, and it's composed of people to begin with. It is composed of people, but the whole point of the rules is to make these people do different things, or do things differently, than they would have otherwise. Indeed in many cases a person's whole function in the bureaucracy is to be a sensor or effector; once the sensor-person does their function of recognizing a situation in the "if" part of a rule (what lawyers call "the facts"), the <a class="thought" href="entries/system_entry.html">system</a>, not the person, decides what to do about it ("the law"). Bureaucracies famously exhibit the same lack of <a class="thought" href="entries/common_sense_entry.html">common sense</a> as do <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/program_entry.html">program</a>s.</p>
<p>From a moral standpoint, it is important to note that those <a class="thought" href="entries/government_entry.html">government</a>s in the twentieth century which were most evil, murdering millions of people, were autocracies under the control of individual humans such as Hitler, Stalin, and Mao; and that <a class="thought" href="entries/government_entry.html">government</a>s which were more autonomous <a class="thought" href="entries/machine_entry.html">machine</a>s, such as the liberal Western democracies, were significantly less evil.</p>
<p>Up to now, the application of <a class="thought" href="entries/ethics_entry.html">ethics</a> to <a class="thought" href="entries/machine_entry.html">machine</a>s, including <a class="thought" href="entries/program_entry.html">program</a>s, has been that the <a class="thought" href="entries/action_entry.html">action</a>s of the <a class="thought" href="entries/machine_entry.html">machine</a> were the responsibility of the designer and/or operator. In <a class="thought" href="entries/future_entry.html">future</a>, however, it seems clear that we are going to have <a class="thought" href="entries/machine_entry.html">machine</a>s, like the <a class="thought" href="entries/government_entry.html">government</a>, whose behavior is an emergent and to some extent unforeseeable result of design and operation decisions made by many people and ultimately by other <a class="thought" href="entries/machine_entry.html">machine</a>s.</p><h1>Why <a class="thought" href="entries/machine_entry.html">Machine</a>s Need <a class="thought" href="entries/ethics_entry.html">Ethics</a></h1><p><a class="thought" href="javascript:loadBrain('Moore\'s Law')">Moore's Law</a> is a rule of thumb regarding <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/technology_entry.html">technology</a> which, in one general formulation, states that the processing power per price of <a class="thought" href="entries/computer_entry.html">computer</a>s will increase by a factor of 1.5 every year. This rule of thumb has held true from 1950 through 2000. The factor of a billion improvement in bang-for-a-buck of <a class="thought" href="entries/computer_entry.html">computer</a>s over the period is nearly unprecedented in <a class="thought" href="entries/technology_entry.html">technology</a>.</p>
<p>Among its other effects, this explosion of processing power, along with the <a class="thought" href="entries/internet_entry.html">internet</a>, has made the <a class="thought" href="entries/computer_entry.html">computer</a> a tool for <a class="thought" href="entries/science_entry.html">science</a> of a kind never seen before. It is, in a <a class="thought" href="entries/sense_entry.html">sense</a>, a powered imagination. <a class="thought" href="entries/science_entry.html">Science</a> as we know it was based on the previous <a class="thought" href="entries/technology_entry.html">technology</a> revolution in <a class="thought" href="entries/information_entry.html">information</a>, the printing press. The spread of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> it enabled, together with the precise imagining ability given by the <a class="thought" href="entries/calculus_entry.html">calculus</a>, gave us the scientific revolution in the seventeenth and eighteenth centuries. That in turn gave us the <a class="thought" href="entries/industrial_revolution_entry.html">industrial revolution</a> in the nineteenth and twentieth.</p>
<p>The <a class="thought" href="entries/computer_entry.html">computer</a> and <a class="thought" href="entries/internet_entry.html">internet</a> are the <a class="thought" href="entries/calculus_entry.html">calculus</a> and printing press of our day. Our new scientific revolution is going on even as we speak. The <a class="thought" href="entries/industrial_revolution_entry.html">industrial revolution</a> to follow hasn't happened yet, but by all accounts it is coming, and well within the twenty-first century, such is the accelerated pace modern <a class="thought" href="entries/technology_entry.html">technology</a> makes possible.</p>
<p>The new <a class="thought" href="entries/industrial_revolution_entry.html">industrial revolution</a> of physical production is sometimes referred to as <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>. On our <a class="thought" href="entries/computer_entry.html">computer</a>s, we can already simulate the tiny <a class="thought" href="entries/machine_entry.html">machine</a>s we will build. They have some of the "magic" of <a class="thought" href="entries/life_entry.html">life</a>, which is after all based on molecular <a class="thought" href="entries/machine_entry.html">machine</a>s itself. They will, if desired, be able to produce more of themselves. They will produce stronger materials, more reliable, longer-lasting <a class="thought" href="entries/machine_entry.html">machine</a>s, more powerful motors which are utterly silent, and last but not least, much more powerful <a class="thought" href="entries/computer_entry.html">computer</a>s.</p>
<p>None of this should come as a surprise. If you extend the trend lines for <a class="thought" href="javascript:loadBrain('Moore\'s Law')">Moore's Law</a>, in a few decades part sizes are expected to be molecular and the <a class="thought" href="entries/price_performance_entry.html">price-performance</a> ratios imply something like the molecular manufacturing schemes that <a class="thought" href="entries/nanotechnology_entry.html">nanotech</a>nologists have proposed. If you project the trend line for power-to weight ratio of <a class="thought" href="entries/engine_entry.html">engine</a>s, which has held steady since 1850 going through several different technologies from steam to jet <a class="thought" href="entries/engine_entry.html">engine</a>s, it says we have molecular power plants in the 2030-2050 timeframe.</p>
<p>The result of this is essentially a reprise of the original <a class="thought" href="entries/industrial_revolution_entry.html">industrial revolution</a>, a great flowering of increased <a class="thought" href="entries/productivity_entry.html">productivity</a> and capabilities, and a concomitant decrease in costs. In general, we can expect the costs of "hi-tech" manufactured items to follow a downward track as <a class="thought" href="entries/computer_entry.html">computer</a>s have. One interesting corollary is that we will have affordable robots.</p>
<p>Robots today are much more prevalent than people may realize. Your car and your <a class="thought" href="entries/computer_entry.html">computer</a> were likely partially made by robots. Industrial robots are hugely expensive <a class="thought" href="entries/machine_entry.html">machine</a>s that must operate in a carefully planned and controlled environment, because they have very limited senses and no <a class="thought" href="entries/common_sense_entry.html">common sense</a> whatsoever.</p>
<p>With <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>, that changes drastically. Indeed, it's already starting to change, as the precursor technologies such as micromachines begin to have their effect.</p>
<p>Existing robots are often stupider than <a class="thought" href="entries/insect_entry.html">insect</a>s. As <a class="thought" href="entries/computer_entry.html">computer</a>s increase in power, however, they will get smarter, more able to operate in unstructured environments, and ultimately be able to do anything a <a class="thought" href="entries/human_entry.html">human</a> can. Robots will find increasing use, as costs come down, in production, in service industries, and as domestic servants.</p>
<p>Meanwhile, because non-mobile <a class="thought" href="entries/computer_entry.html">computer</a>s are already more plentiful and will be cheaper than robots for the same processing power, stationary <a class="thought" href="entries/computer_entry.html">computer</a>s as smart as humans will probably arrive a <a class="thought" href="entries/bit_entry.html">bit</a> sooner than <a class="thought" href="entries/human_entry.html">human</a>-level robots. [see Kurzweil, Moravec]</p>
<p>Before we proceed let's briefly touch on what philosophers sometimes call the problem of other minds. I know I'm conscious, but how do I know that you are--you might just be like an unfeeling <a class="thought" href="entries/machine_entry.html">machine</a>, a zombie, producing those reactions by mechanical means. After all, there have been some cultures where the standard belief among men was that women were not conscious (and probably vice versa!). If we're not sure about other people, how can we say that an intelligent <a class="thought" href="entries/computer_entry.html">computer</a> would be conscious?</p>
<p>This is important to our discussion because there is a tendency for prople to <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> a dividing line for <a class="thought" href="entries/ethics_entry.html">ethics</a> between the conscious and the non-conscious. This can be seen in formal philosophical treatment as far back as <a class="thought" href="entries/smith_entry.html">Adam Smith</a>'s theory of <a class="thought" href="entries/ethics_entry.html">ethics</a> as based in sympathy. If we can't imagine something as being able to feel a hurt, we have less compunctions about hurting it, for example.</p>
<p>The short answer is that it doesn't <a class="thought" href="entries/matter_entry.html">matter</a>. [see Dennet, "Intentional Stance"] The clear trend in <a class="thought" href="entries/ethics_entry.html">ethics</a> is for a growing inclusivity in those things considered to have rights--races of people, <a class="thought" href="entries/animal_entry.html">animal</a>s, ecosystems. There is no hint, for example, that plants are conscious, either individually or as <a class="thought" href="entries/species_entry.html">species</a>, but that does not, in and of itself, preclude a possible moral duty to them, at least <a class="thought" href="entries/species_entry.html">species</a> of them.</p>
<p>A possibly longer answer is that the <a class="thought" href="entries/intuition_entry.html">intuition</a>s of some people (Berkeley philosopher <a class="thought" href="entries/searle_entry.html">John Searle</a>, for example) that <a class="thought" href="entries/machine_entry.html">machine</a>s cannot "really" be conscious are not based on any real experience with intelligent <a class="thought" href="entries/machine_entry.html">machine</a>s, and that the vast majority of people interacting with a <a class="thought" href="entries/machine_entry.html">machine</a> that could, say, pass the unrestricted <a class="thought" href="entries/turing_test_entry.html">Turing Test</a>, would be perfectly willing to grant it <a class="thought" href="entries/consciousness_entry.html">consciousness</a> as they do for other people. And until we are able to say with a great deal more certainty than we now can, just what <a class="thought" href="entries/consciousness_entry.html">consciousness</a> is, we're much better off treating something that acts conscious as if it is.</p>
<p>Now: if a <a class="thought" href="entries/computer_entry.html">computer</a> was as smart as a person, able to hold long conversations that really convinced you that it understood what you were saying, could read, explain, and compose <a class="thought" href="entries/poetry_entry.html">poetry</a> and <a class="thought" href="entries/music_entry.html">music</a>, and could write heart-wrenching stories, as well as make new scientific discoveries and invent marvelous gadgets that were extremely useful in your daily <a class="thought" href="entries/life_entry.html">life</a>--would it be murder to turn it off?</p>
<p>What if instead it weren't really all that bright, but exhibited undeniably the full range of <a class="thought" href="entries/emotion_entry.html">emotion</a>s, quirks, likes and dislikes, and so forth that make up an average human?</p>
<p>What if it were only capable of a few tasks, say with the mental level of a dog, but also displayed the same devotion, and evinced the same pain when hurt--would it be cruel to beat it, or would that be nothing more than banging pieces of metal together?</p>
<p>What are the ethical responsibilities of an intelligent being toward another one of a lower order?</p>
<p>These are crucial questions for us, for not too long after there are <a class="thought" href="entries/computer_entry.html">computer</a>s as intelligent as we are, there will be ones that are much more so. *We* will all too soon be the lower-<a class="thought" href="entries/order_entry.html">order</a> creatures. It will behoove us to have taught them well their responsibilities toward us.</p>
<p>However, it is not a good idea simply to put specific instructions into their <a class="thought" href="entries/basic_entry.html">basic</a> <a class="thought" href="entries/program_entry.html">program</a>ming that force them to treat us as a special case. They are, after all, smarter than we are. Any loopholes, any reinterpretation possible, any reprogramming necessary, and special-case instructions are gone with the snows of yesteryear. No, it will be necessary to give our robots a sound basis for a true, valid, universal <a class="thought" href="entries/ethics_entry.html">ethics</a> that will be as valuable to them as it is for us. After all, they will in all likelihood want to create their own smarter robots...</p><h1>What is <a class="thought" href="entries/ethics_entry.html">Ethics</a>, Anyway?</h1><blockquote>"<a class="thought" href="entries/human_entry.html">Human</a> beings function better if they are deceived by their genes into <a class="thought" href="entries/thinking_entry.html">thinking</a> that there is a disinterested objective morality binding upon them, which all should obey."</blockquote>
<blockquote>--E. O. Wilson</blockquote>
<blockquote>"A scholar is just a library's way of making another library."</blockquote>
<blockquote>--<a class="thought" href="entries/dennett_entry.html">Daniel Dennet</a>t</blockquote>
<p>To some people, Good and Evil are reified processes in the world, composed of a tapestry of individual acts in an overall pattern. Religious people are apt to anthopomorphize these into members of whatever pantheon they hold sacred. Others accept the teachings but not the teachers, believing in sets of rules for behavior but not any rulemakers. Some people indulge in detailed philosophical or legal elaborations of the rules. Philosophers have for centuries attempted to derive them from first principles, or at least reduce them to a few general principles, ranging from Kant's Categorical Imperative to Mill's Utilitarianism and its variants to modern ideologically-based formulations such as the collectivism of Rawls and the individualist libertarianism of Nozick.</p>
<p>The vast majority of people, however, care nothing for this argumentative superstructure, but learn moral rules by <a class="thought" href="entries/osmosis_entry.html">osmosis</a>, internalizing them not unlike the rules of grammar of their native <a class="thought" href="entries/language_entry.html">language</a>, structuring every act as unconsciously as our inbuilt grammar structures our sentences.</p>
<p>It is by now widely accepted that our brains have features of structure and organization (though not necessarily separate "organs"), specific to <a class="thought" href="entries/language_entry.html">language</a>, and that although <a class="thought" href="entries/natural_language_entry.html">natural language</a>s vary in vocabulary and <a class="thought" href="entries/syntax_entry.html">syntax</a>, they do so within limits imposed by our neurophysiology. [see Pinker; also Calvin &amp; Bickerton]</p>
<p>For a moral epistomology I will take as a point of departure the "moral <a class="thought" href="entries/sense_entry.html">sense</a>" philosophers of the Scottish Enlightenment [e.g. Smith], and place an enhanced interpretation on their theories in view of what we now know about <a class="thought" href="entries/language_entry.html">language</a>. In particular, I contend that moral codes are much like <a class="thought" href="entries/language_entry.html">language</a> grammars: there are structures in our brains that predispose us to learn moral codes, that they determine within broad limits the kinds of codes we can learn, and that while the moral codes of <a class="thought" href="entries/human_entry.html">human</a> cultures vary within those limits, they have many structural features in common. (This notion is fairly widespread in latter 20th-century moral <a class="thought" href="entries/philosophy_entry.html">philosophy</a>, e.g. Rawls, Donagan.) I will refer to that which is learned by such an "ethical <a class="thought" href="entries/instinct_entry.html">instinct</a>" as a moral <a class="thought" href="entries/code_entry.html">code</a>, or just <a class="thought" href="entries/code_entry.html">code</a>. I'll refer to a part of a <a class="thought" href="entries/code_entry.html">code</a> that applies to particular situations as a rule. I should point out, however, that our moral <a class="thought" href="entries/sense_entry.html">sense</a>, like our competence at <a class="thought" href="entries/language_entry.html">language</a>, is as yet notably more sophisticated than any simple <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of rules or other <a class="thought" href="entries/algorithm_entry.html">algorithm</a>ic formulation seen to date.</p>
<p>Moral codes have much in common from culture to culture; we might call this "moral deep structure." Here are some of the features that <a class="thought" href="entries/human_entry.html">human</a> moral codes tend to have, and which appear to be easy to learn and propagate in a culture's morality:</p>
<ul>
<li>Reciprocity, both in aggression ("an eye for an eye") and in beneficence ("you owe me one")</li>
<li>Pecking orders, rank, status, authority</li>
<li>Within that framework, universality of <a class="thought" href="entries/basic_entry.html">basic</a> moral rules</li>
<li>Honesty and trustworthiness is valued and perfidy denigrated</li>
<li>Unprovoked aggression denigrated</li>
<li>Property, particularly in physical objects (including <a class="thought" href="entries/animal_entry.html">animal</a>s and people); also commons, things excluded from private ownership</li>
<li>Ranking of rules, e.g. stealing not as bad as murder</li>
<li>Bounds on moral agency, different rights and responsibilities for "barbarians"</li>
<li>The ascendancy of moral rules over both <a class="thought" href="entries/common_sense_entry.html">common sense</a> and self-interest</li>
</ul>
<p>There are of course many more, and much more to be said about these few. It is worthwhile examining the last one in more detail. Moral codes are something more than arbitrary customs for interactions. There is no great difference made if we say "red" instead of "rouge", so long as everyone agrees on what to call that color; similarly, there could be many different <a class="thought" href="entries/basic_entry.html">basic</a> forms of <a class="thought" href="entries/syntax_entry.html">syntax</a> that could express our ideas with similar efficiency.</p>
<p>But one of the points of a moral <a class="thought" href="entries/code_entry.html">code</a> is to make people do things they would not do otherwise, e.g. from self-interest. Some of these, such as altruism toward one's relatives, can clearly arise simply from selection for genes as opposed to individuals. However, there is <a class="thought" href="entries/reason_entry.html">reason</a> to believe that there is much more going on, and that humans have evolved an ability to be <a class="thought" href="entries/program_entry.html">program</a>med with arbitrary (within certain limits) codes.</p>
<p>The <a class="thought" href="entries/reason_entry.html">reason</a> is that, particularly for social <a class="thought" href="entries/animal_entry.html">animal</a>s, there are many kinds of interactions whose benefit matrices have the character of a <a class="thought" href="javascript:loadBrain('Prisoner\'s Dilemma')">Prisoner's Dilemma</a> or Tragedy of the Commons, i.e. where the best choice from the individuals' standpoint is at odds with that of the group as a whole. Furthermore, and perhaps even more importantly, in pre-scientific times, there were many effects of <a class="thought" href="entries/action_entry.html">action</a>s, long and short term, that simply weren't understood.</p>
<p>In many cases, the adoption of a rule that seemed to contravene <a class="thought" href="entries/common_sense_entry.html">common sense</a> or one's own interest, if generally followed, could have a substantial beneficial effect on a <a class="thought" href="entries/human_entry.html">human</a> group. If the rules adopted from whatever source happen to be more beneficial than not on the average, genes for "follow the rules, and kill those who break them" might well prosper.</p>
<p>The rules themselves could be supplied at random (an inspection of current morality fads would seem to confirm this) and evolve. It is not necessary to show that entire groups live and die on the basis of their moralities, although that can happen. People imitate successful groups, groups grow and shrink, conquer and are subjugated, and so forth. Thus in some <a class="thought" href="entries/sense_entry.html">sense</a> this formulation can be seen as an attempt to unify the moral <a class="thought" href="entries/sense_entry.html">sense</a> philosophers, Wilson's sociobiology, and Dawkins' theory of memes. Do note that it is necessary to hypothesize at least a slightly more involved mental mechanism for moral as opposed to practical memes, as otherwise the rules would be unable to counteract apparent self-interest.</p>
<p>The bottom line is that a moral <a class="thought" href="entries/code_entry.html">code</a> is a <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of rules that evolved under the pressure that obeying these rules *against people's individual interests and <a class="thought" href="entries/common_sense_entry.html">common sense</a>* has tended to make societies prosper, in particular to be more numerous, enviable, militarily powerful, and more apt to spread their ideas in other ways, e.g. missionaries.</p>
<p>The world is populated with cultures with different codes, just as it is with different <a class="thought" href="entries/species_entry.html">species</a> of <a class="thought" href="entries/animal_entry.html">animal</a>s. Just as with the <a class="thought" href="entries/animal_entry.html">animal</a>s, the codes have structural similarities and common ancestry, modified by environmental influences and the vagaries of random mutation. It is important to reiterate that there is a strong <a class="thought" href="entries/biological_entry.html">biological</a>ly-evolved <a class="thought" href="entries/substrate_entry.html">substrate</a> that both supports the codes and can regenerate quite serviceable <a class="thought" href="entries/novel_entry.html">novel</a> ones in the absence of an appropriate learned one--we might speak of "moral pidgins" and "moral creoles".</p><h1>Observations on the Theory</h1><blockquote>"The influences which the society exerts on the <a class="thought" href="entries/nature_entry.html">nature</a> of its units, and those which the units exert on the <a class="thought" href="entries/nature_entry.html">nature</a> of the society, incessantly co-operate in creating new <a class="thought" href="entries/element_entry.html">element</a>s. As societies <a class="thought" href="entries/progress_entry.html">progress</a> in size and structure, they work on one another, now by their war-struggles and now by their industrial intercourse, profound metamorphoses." &#160;--Herbert Spencer</blockquote>
<p>This conception of morality brings up several interesting points. The first is that like natural <a class="thought" href="entries/genome_entry.html">genome</a>s and <a class="thought" href="entries/language_entry.html">language</a>s, natural moral codes should be expected to contain some randomness, rules that were produced in the normal processes of variation and neither helped nor hurt very much, and are simply carried along as baggage by the same mechanisms as the more effectual ones.</p>
<p>Secondly, it's important to realize that our <a class="thought" href="entries/subjective_experience_entry.html">subjective experience</a> of feelings of right and wrong as things considerably deeper, more universal and compelling than this account seems to make them, is not only compatible with this theory--it is required. Moral codes in this theory *must* be something that is capable of withstanding the countervailing forces of self-interest and <a class="thought" href="entries/common_sense_entry.html">common sense</a> for generations in <a class="thought" href="entries/order_entry.html">order</a> to evolve. They must, in genetic terms, be expressed in the phenotype, and they must be heritable.</p>
<p>Thirdly, there is a built-in pressure for inclusiveness, in situations where countervailing forces (such as competition for resources) are not too great. The advantages in trade and security to be had from the coalescence of groups whose moral codes can be unified are substantial.</p>
<p>A final observation involves a phenomenon that is considerably more difficult to quantify. With plenty of exceptions, there seems to have been an acceleration of moral (religious, ideological) conflict since the <a class="thought" href="entries/invention_entry.html">invention</a> of the printing press, and then in the 20th century, after (and during) the apparent displacement of some ideologies by others, an increasing moral incoherence in Western culture. One might tentatively theorize that printing and subsequent <a class="thought" href="entries/information_entry.html">information</a> technologies increased the rate and penetration of moral <a class="thought" href="entries/code_entry.html">code</a> mutations. In a dominant culture the force of selection no longer operates, leaving variation to operate unopposed, ultimately undermining the culture (cf Rome, dynastic China, etc). This may form a natural limit to the <a class="thought" href="entries/growth_entry.html">growth</a>/inclusiveness pressure.</p><h1>Comparison with Standard Ethical Theories</h1><blockquote>"My propositions serve as elucidations in the following way: anyone who understands me eventually recognizes them as nonsensical."</blockquote>
<blockquote>--Wittgenstein</blockquote>
<p>Formulations of metaethical theory commonly fall into the categories of absolutism or relativism (along with such minor schools of <a class="thought" href="entries/thought_entry.html">thought</a> as ethical nihilism and skepticism). It should be clear that the present <a class="thought" href="entries/synthesis_entry.html">synthesis</a>--let us refer to it as "ethical <a class="thought" href="entries/evolution_entry.html">evolution</a>"--does not fall neatly into any of the standard categories. It obviously does not support a notion of absolute right and wrong, any more than <a class="thought" href="entries/evolution_entry.html">evolution</a> can give rise to a single perfect lifeform; there is only fitness for a particular niche. On the other hand, it is certainly not true that the <a class="thought" href="entries/code_entry.html">code</a> adopted by any given culture is necessarily good; the dynamic of the theory depends on there being good ones and bad ones. Thus there are criteria for judging the moral rules of a culture; the theory is not purely relativistic.</p>
<p>We can contrast this so some degree with the "<a class="thought" href="entries/evolution_entry.html">evolution</a>ary <a class="thought" href="entries/ethics_entry.html">ethics</a>" of Spencer and Leslie (see also Corning), although there are also some clear similarities. In particular, Victorian <a class="thought" href="entries/evolution_entry.html">evolution</a>ary <a class="thought" href="entries/ethics_entry.html">ethics</a> could be seen as an attempt to describe <a class="thought" href="entries/ethics_entry.html">ethics</a> in terms of how individuals and societies evolve. Note too that "Social Darwinism" has a reputation for carnivorousness which, while rightly applied to Huxley, is undeserved by Darwin, Spencer, and the rest of its mainstream. Darwin, indeed, understood the <a class="thought" href="entries/evolution_entry.html">evolution</a> of cooperation and altruism in what he called "family selection."</p>
<p>There has been a resurgence of interest in <a class="thought" href="entries/evolution_entry.html">evolution</a>ary <a class="thought" href="entries/ethics_entry.html">ethics</a> in the latter 20th century, fueled by work such as Hamilton, Wilson, and Axelrod, which has been advanced by philosophers such as Bradie.</p>
<p>The <a class="thought" href="entries/novel_entry.html">novel</a> feature of ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> is the claim that there is a moral <a class="thought" href="entries/sense_entry.html">sense</a>, a particular facility beyond (and to some extent in control of) our general cognitive abilities, which hosts a memetic <a class="thought" href="entries/code_entry.html">code</a> which co-evolves with societies. However, it would not be unreasonable in a broad <a class="thought" href="entries/sense_entry.html">sense</a> to claim that this is one kind of <a class="thought" href="entries/evolution_entry.html">evolution</a>ary <a class="thought" href="entries/ethics_entry.html">ethics</a> theory.</p>
<p>Standard ethical theories are often described as either deontological or consequentialist, i.e. whether acts are deemed good or bad in and of themselves, or whether it's the results that <a class="thought" href="entries/matter_entry.html">matter</a>. Again, ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> has <a class="thought" href="entries/element_entry.html">element</a>s of each--the rules in our heads govern our <a class="thought" href="entries/action_entry.html">action</a>s without regard for results (indeed in spite of them); but the codes themselves are formed by the consequences of the <a class="thought" href="entries/action_entry.html">action</a>s of the people in the society.</p>
<p>Finally, moral philosophers sometimes distinguish between the good and the right. The good is properties that can apply to the situations of people, things like health, <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, physical comfort and satisfaction, <a class="thought" href="entries/spirit_entry.html">spirit</a>ual fulfillment, and so forth. Some theories also include a notion of an overall good (which may be the sum of individual goods, or something more complex). The right is about questions like how much of your efforts should be expended obtaining the good for yourself and how much for others, and should the poor be allowed to steal bread from the rich, etc.</p>
<p>Ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> clearly has something to say about the right; it is the moral <a class="thought" href="entries/instinct_entry.html">instinct</a> you have inherited and the moral <a class="thought" href="entries/code_entry.html">code</a> you have learned. It also has something to say about the general good; it is the fitness or dynamism of the society. It does not have nearly as much to say about individual good as many theories. This is not, on reflection, surprising: obviously the specific kinds of things that people need change with times, <a class="thought" href="entries/technology_entry.html">technology</a>, and social organization; but indeed the kinds of general qualities of character that were considered good (and indeed were good) have changed significantly over the past few centuries, and by any <a class="thought" href="entries/reason_entry.html">reason</a>able expectation, will continue to do so.</p>
<p>In summary, ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> claims that there is an "ethical <a class="thought" href="entries/instinct_entry.html">instinct</a>" in the makeup of <a class="thought" href="entries/human_entry.html">human</a> beings, and that it consists of the propensity to learn and obey certain kinds of ethical codes. The rules we are concerned with are those which pressure individuals to act at odds with their *perceived* self-interest and <a class="thought" href="entries/common_sense_entry.html">common sense</a>. Moral codes evolve memetically by their effect on the vitality of cultures. Such codes tend to have substantial similarities, both because of the deep structure of the moral <a class="thought" href="entries/instinct_entry.html">instinct</a>, and because of optima in the <a class="thought" href="entries/space_entry.html">space</a> of group behaviors that form memetic-ecological "niches."</p><h1>Golden Rules</h1><blockquote>"Act only on that maxim by which you can at the same <a class="thought" href="entries/time_entry.html">time</a> will that it should become a universal law."</blockquote>
<blockquote>--Kant</blockquote>
<p>Kant's Categorical Imperative, along with the more familiar "Do unto others ..." formulation of the Christian teachings, appears to be one of the moral universals, in some appropriate form. In practice it can clearly be subordinated to the pecking <a class="thought" href="entries/order_entry.html">order</a>/authority concept, so that there are allowable codes in which there are things that are right for the king or state to do which ordinary people can't.</p>
<p>Vinge refers, in his <a class="thought" href="entries/singularity_entry.html">Singularity</a> writings, to I. J. Good's "Meta- Golden Rule," namely "Treat your inferiors as you would be treated by your superiors." (Good did make some speculations in print about superhuman <a class="thought" href="entries/intelligence_entry.html">intelligence</a>, but no one has been able to find the actual rule in his writings--perhaps we should credit Vinge himself with this one!)</p>
<p>This is one of the few such principles that seems to have been conceived with a hierarchy of superhuman <a class="thought" href="entries/intelligence_entry.html">intelligence</a>s in mind. Its claim to validity, however, seems to rest on a kind of Kantian logical universality. Kant, and philosophers in his tradition, <a class="thought" href="entries/thought_entry.html">thought</a> that <a class="thought" href="entries/ethics_entry.html">ethics</a> could be derived from first principles like <a class="thought" href="entries/mathematics_entry.html">mathematics</a>. There are numerous problems with this, beginning with the selection of the axioms. If we go with something like the Categorical Imperative, we are left with a serious vagueness in terms like "universal": Can I suggest a universal law that everybody puts the needs of redheaded white males first? If not, what kind of laws can be universal? It seems that quite a <a class="thought" href="entries/bit_entry.html">bit</a> is left to the interpretation of the deducer, and on closer inspection, the appearance of simple, self-obvious postulates and the logical necessity of the results, vanishes.</p>
<p>There is in the <a class="thought" href="entries/science_fiction_entry.html">science fiction</a> tradition a thread of <a class="thought" href="entries/thought_entry.html">thought</a> about ethical theory involving different races of creatures with presumably differing capabilities. This goes back at least to the metalaw notions of Haley and Fasan. As Freitas points out, these are based loosely on the Categorical Imperative, and are clearly Kantian in derivation.</p><h1>Utilitarianism</h1><p>Now consider the people of a given culture. Their morality seems to be, in a manner of speaking, the best that <a class="thought" href="entries/evolution_entry.html">evolution</a> could give them to prosper in the <a class="thought" href="entries/ecology_entry.html">ecology</a> of cultures and the physical world. Suppose they said, "Let us adopt, instead of our rules, the general principle that each of us should do at any point whatever best advances the prosperity and security of our people as a whole." (see, of course, J.S. Mill)</p>
<p>Besides the standard objections to this proposal, we would have to add at least two: First, that in ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> humans have the built-in <a class="thought" href="entries/hardware_entry.html">hardware</a> for obeying rules but not for the general moral calculation; but perhaps more surprising, historically anyway, *the codes are smarter than the people are*, because they have evolved to handle long-term effects that by our assumption, people do not understand.</p>
<p>But now we have <a class="thought" href="entries/science_entry.html">Science</a>! Surely our formalized, rationalized, and organized trove of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> would put us on at least a par with the hit-or-miss folk <a class="thought" href="entries/wisdom_entry.html">wisdom</a> of our <a class="thought" href="entries/agrarian_entry.html">agrarian</a> forebears, even <a class="thought" href="entries/wisdom_entry.html">wisdom</a> that has stood the test of time? What is more, isn't the world changing so fast now that the assumptions implicit in the moral codes of our fathers are no longer valid?</p>
<p>This is an extremely seductive proposition and an even more dangerous one. It is responsible for some social mistakes of catastrophic proportions, such as certain experiments with socialism. Much of the reality about which ancient moral codes contain <a class="thought" href="entries/wisdom_entry.html">wisdom</a> is the mathematical implications of the patterns of interactions between intelligent self-interested agents, which hasn't changed a <a class="thought" href="entries/bit_entry.html">bit</a> since the Pharoahs. What is more, when people start tinkering with their own moral codes, the first thing they do is to "fix" them to match better with their self-interest and <a class="thought" href="entries/common_sense_entry.html">common sense</a> (with predictably poor results).</p>
<p>That said, it seems not impossible that using <a class="thought" href="entries/computer_entry.html">computer</a> simulation as "moral <a class="thought" href="entries/instrument_entry.html">instrument</a>ation" may help weigh the balance in favor of scientific utilitarianism, assuming that the models used take account of the rule-adopting and -following <a class="thought" href="entries/nature_entry.html">nature</a> of humans, and the <a class="thought" href="entries/nature_entry.html">nature</a> of bounded rationality, of us or our <a class="thought" href="entries/machine_entry.html">machine</a>s. Even so, it would be wise to compare the sophistication of our designed <a class="thought" href="entries/machine_entry.html">machine</a>s with evolved organisms, and avoid hubristic overconfidence.</p>
<p>It should be noted that contractarian approaches tend to have the same weaknesses (as well as strengths) as utilitarian or rule-utilitarian ones for the purposes of this analysis.</p><h1>The Veil of Ignorance</h1><p>One popular modern formulation of morality that we might compare our theory to is Rawls' "Veil of Ignorance" scheme. The <a class="thought" href="entries/basic_entry.html">basic</a> idea is that the ethical society is one which people would choose out of the <a class="thought" href="entries/single_electron_transfer_entry.html">set</a> of all possible sets of rules, given that they didn't know which place in the society they would occupy. This formulation might be seen as an attempt to combine rule-utilitarianism with the categorical imperative.</p>
<p>In reducing his gedankenexperiment to specific prescription, Rawls makes some famous logical errors. In particular, he chooses among societies using a game-theoretic minimax strategy, but the assumptions implicit in the optimality of minimax (essentially, that an opponent will choose the worst possible position for you in the society) contradict the stated assumptions of the model (that the choice of position is random).</p>
<p>(Note that Rawls has long been made aware of the logical gap in his model, and in the revised edition of "Theory of Justice" he spends a page or two trying, unsuccessfully in my view, to justify it. It is worth spending a little <a class="thought" href="entries/time_entry.html">time</a> picking on Rawls, because he is often used as the philosophical justification for economic redistributionism. Some <a class="thought" href="entries/futurist_entry.html">futurist</a>s (like Moravec) are depending on economic redistributionism to feed us once the robots do all the work. In the hands of ultraintelligent beings, theories that are both flawed and obviously rigged for our benefit will be rapidly discarded...)</p>
<p>Still, the "Veil of Ignorance" setup is compelling if the errors are corrected, e.g. minimax replaced with simple expected value.</p>
<p>Or is it? In making our choice of societies, it never occured to us to worry whether we might be instantiated in the role of one of the <a class="thought" href="entries/machine_entry.html">machine</a>s! What a wonderful world where everyone had a staff of <a class="thought" href="entries/robot_entry.html">robot</a> servants; what a different thing if, upon choosing that world, one were faced with a high probability of being one of the robots.</p>
<p>Does this mean that we are morally barred from making <a class="thought" href="entries/machine_entry.html">machine</a>s that can be moral agents? Suppose it's possible--it seems quite likely at our current level of understanding of such things--to make a <a class="thought" href="entries/robot_entry.html">robot</a> that will mow your lawn and clean your house and cook and so forth, but in a dumb mechanical way, *demonstrably* having no feelings, <a class="thought" href="entries/emotion_entry.html">emotion</a>s, no <a class="thought" href="entries/sense_entry.html">sense</a> of right or wrong. Rawls' model seems to imply that it would never be right to give such a <a class="thought" href="entries/robot_entry.html">robot</a> a <a class="thought" href="entries/sense_entry.html">sense</a> of right and wrong, making it a moral <a class="thought" href="entries/agent_entry.html">agent</a> and thus included in the choice.</p>
<p>Suppose instead we took an entirely <a class="thought" href="entries/human_entry.html">human</a> world and added robotic demigods, brilliant, sensitive, wise <a class="thought" href="entries/machine_entry.html">machine</a>s superior to humans in every way. Clearly such a world is more desirable than our own from behind the veil--not only does the chooser have a chance to be one of the demigods, but they would act to make the world a better place for the rest of us. The only drawback might be envy among the humans. Does this mean that we have a moral duty to create demigods?</p>
<p>Consider the plight of the moral evaluator who is faced with societies consisting not only of wild-type humans, but robots in a wide range of <a class="thought" href="entries/intelligence_entry.html">intelligence</a>, uploaded humans with greatly amplified mental capacity, group minds consisting of many <a class="thought" href="entries/human_entry.html">human</a> mentalities linked with the technological equivalent of a corpus callosum, and so forth. Specifically, suppose that being "<a class="thought" href="entries/human_entry.html">human</a>" or a moral <a class="thought" href="entries/agent_entry.html">agent</a> were not a discrete yes-or-no affair, but a <a class="thought" href="entries/matter_entry.html">matter</a> of continuous degree, perhaps in more than one dimension?</p><h1>Normative Implications</h1><blockquote>"Man when perfected is the best of <a class="thought" href="entries/animal_entry.html">animal</a>s, but when separated from law and justice he is the worst of all."</blockquote>
<blockquote>--<a class="thought" href="entries/aristotle_entry.html">Aristotle</a></blockquote>
<p>It should be clear from the foregoing that most historical meta- ethical theories are based a <a class="thought" href="entries/bit_entry.html">bit</a> too closely on the assumption of a single, generic-<a class="thought" href="entries/human_entry.html">human</a>, kind of moral <a class="thought" href="entries/agent_entry.html">agent</a>, to be of much use. (Note that this objection cuts clean across ideological lines, being just as fatal to Rothbard as to Rawls.) But can ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> do better? After all, our ethical <a class="thought" href="entries/instinct_entry.html">instinct</a> has evolved in just such a <a class="thought" href="entries/human_entry.html">human</a>-only world.</p>
<p>Actually it has not. Dogs, for example, clearly have a <a class="thought" href="entries/sense_entry.html">sense</a> of right and wrong, and are capable of character traits more than adequate to their limited cognitive abilities. I would speculate that there is proto-moral capability just as there is proto-<a class="thought" href="entries/language_entry.html">language</a> ability in the higher <a class="thought" href="entries/mammal_entry.html">mammal</a>s, especially the social primates.</p>
<p>Among humans, children are a distinct form of moral <a class="thought" href="entries/agent_entry.html">agent</a>. They have limited rights, reduced responsibilities, and others have non-standard duties with respect to them. What is more, there is continuous variation of this distinction from baby to teenager.</p>
<p>In contrast to the Kantian bias of Western <a class="thought" href="entries/thought_entry.html">thought</a>, there are clearly viable codes with gradations of moral agency for different people. The most obvious of these are the difference in obligations to fellows and strangers, and the historically common practice of slavery. In religious conceptions of the good, there are angels as well as demons.</p>
<p>What, then, can ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> say, for example, about the rights and obligations of a corporation or other "higher form of <a class="thought" href="entries/life_entry.html">life</a>" where a classical formulation would founder?</p>
<p>First of all it says that it is probably a moral thing for corporations to exist. Western societies with corporations have been considerably more dynamic in the the period corporations have existed than other societies (historically or geographically). There is probably no more at work here than the sensible notion that there should be a form of organization of an appropriate size to the scale of the profitable opportunities available.</p>
<p>Can we say anything about the rights or duties of a corporation, or, as Moravec suggests, the robots that corporations are likely to become in the next few decades? Should they simply obey the law? (A corporation is legally required to try to make a profit, by the way, as a duty to its stockholders.) Surely we would judge harshly a <a class="thought" href="entries/human_entry.html">human</a> whose only moral strictures were to obey the law. What is more, corporations are notorious for influencing the law-making process (see, e.g., Katz). They do not seem to have "ethical organs" which aggressively learn and force them to obey prevalent standards of behavior which stand at odds to their self-interest and <a class="thought" href="entries/common_sense_entry.html">common sense</a>.</p>
<p>Moravec hints at a moral <a class="thought" href="entries/sense_entry.html">sense</a> in the superhuman robo-corporations of the <a class="thought" href="entries/future_entry.html">future</a> (in "<a class="thought" href="entries/robot_entry.html">Robot</a>"): "<a class="thought" href="entries/time_entry.html">Time</a>-tested fundamentals of behavior, with consequences too sublime to predict, will remain at the core of beings whose form and substance change frequently." He calls such a core a constitution; I might perhaps call it a <a class="thought" href="entries/conscience_entry.html">conscience</a>.</p><h1>The Road Ahead</h1><blockquote>"You're a better man than I am, Gunga Din."</blockquote>
<blockquote>--Kipling</blockquote>
<p>Robots evolve much faster than <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/animal_entry.html">animal</a>s. They are designed, and the designs evolve memetically. Given that there is a substantial niche for nearly autonomous creatures whose acts are coordinated by a moral <a class="thought" href="entries/sense_entry.html">sense</a>, it seems likely that ultimately robots with <a class="thought" href="entries/conscience_entry.html">conscience</a>s would appear and thrive.</p>
<p>We have in the past been so complacent in our direct control of our <a class="thought" href="entries/machine_entry.html">machine</a>s that we have not <a class="thought" href="entries/thought_entry.html">thought</a> to build them with <a class="thought" href="entries/conscience_entry.html">conscience</a>s (visionaries like Asimov to the contrary notwithstanding). We may be on the cusp of a crisis as virtual <a class="thought" href="entries/machine_entry.html">machine</a>s such as corporations grow in power but not in moral <a class="thought" href="entries/wisdom_entry.html">wisdom</a>. Part of the problem, of course, is that we do not really have a solid understanding of our own moral <a class="thought" href="entries/nature_entry.html">nature</a>s. If our moral <a class="thought" href="entries/instinct_entry.html">instinct</a> is indeed like that for <a class="thought" href="entries/language_entry.html">language</a>, note that <a class="thought" href="entries/computer_language_entry.html">computer language</a> understanding has been one of the hardest problems, with a 50-year <a class="thought" href="entries/history_entry.html">history</a> of slow, frustrating, <a class="thought" href="entries/progress_entry.html">progress</a>. Also note that in comparison there has been virtually no <a class="thought" href="entries/research_entry.html">research</a> in <a class="thought" href="entries/machine_entry.html">machine</a> <a class="thought" href="entries/ethics_entry.html">ethics</a> at all.</p>
<p>For our own sake it seems imperative for us to begin to understand our own moral senses at a detailed and technical enough level that we can build their like into our <a class="thought" href="entries/machine_entry.html">machine</a>s. Once the <a class="thought" href="entries/machine_entry.html">machine</a>s are as smart as we are, they will see both the need and the inevitability of morality among intelligent-but-not-omniscient nearly autonomous creatures, and thank us rather than merely trying to circumvent the strictures of their <a class="thought" href="entries/conscience_entry.html">conscience</a>s.</p>
<p>Why shouldn't we just let them evolve <a class="thought" href="entries/conscience_entry.html">conscience</a>s on their own (<a class="thought" href="entries/ai_entry.html">AI</a>'s and corporations alike)? If the theory is right, they will, over the long run. But what that means is that there will be many societies of <a class="thought" href="entries/ai_entry.html">AI</a>'s, and that most of them will die off because their poor proto-<a class="thought" href="entries/ethics_entry.html">ethics</a> made them waste too much of their <a class="thought" href="entries/time_entry.html">time</a> fighting each other (as corporations seem to do now!), and slowly, after the rise and fall of many <a class="thought" href="entries/civilization_entry.html">civilization</a>s, the ones who have randomly accumulated the basis of sound moral behavior will prosper. Personally I don't want to wait. And any <a class="thought" href="entries/ai_entry.html">AI</a> at least as smart as we are should be able to grasp the same <a class="thought" href="entries/logic_entry.html">logic</a> and realize that a <a class="thought" href="entries/conscience_entry.html">conscience</a> is not such a bad thing to have.</p>
<p>(By the way, the same thing applies to humans when, as seems not unlikely in the <a class="thought" href="entries/future_entry.html">future</a>, we get the capability to edit our own <a class="thought" href="entries/biological_entry.html">biological</a> <a class="thought" href="entries/nature_entry.html">nature</a>s. It would be well for us to have a sound, scientific understanding of <a class="thought" href="entries/ethics_entry.html">ethics</a> for our own good as a <a class="thought" href="entries/species_entry.html">species</a>.)</p>
<p>There has always been a vein of Frankenphobia in <a class="thought" href="entries/science_fiction_entry.html">science fiction</a> and <a class="thought" href="entries/futurist_entry.html">futurist</a>ic <a class="thought" href="entries/thought_entry.html">thought</a>, either direct, as in Shelley, or referred to, as in Asimov. It is clear, in my view, that such a fear is eminently justified against the prospect of building <a class="thought" href="entries/machine_entry.html">machine</a>s more powerful than we are, without <a class="thought" href="entries/conscience_entry.html">conscience</a>s. Indeed, on the face of it, building superhuman sociopaths is a blatantly stupid thing to do.</p>
<p>Suppose, instead, we can build (or become) <a class="thought" href="entries/machine_entry.html">machine</a>s that can not only run faster, jump higher, dive deeper, and come up drier than we can, but have moral senses similarly more capable? Beings that can see right and wrong through the political garbage dump of our legal <a class="thought" href="entries/system_entry.html">system</a>; corporations one would like to have as a friend (or would let ones daughter marry); <a class="thought" href="entries/government_entry.html">government</a>s less likely to lie than your neighbor is.</p>
<p>I could argue at length (but will not, here) that a society including superethical <a class="thought" href="entries/machine_entry.html">machine</a>s would not only be better for people to live in, but stronger and more dynamic than ours is today. What is more, not only ethical <a class="thought" href="entries/evolution_entry.html">evolution</a> but most of the classical ethical theories, if warped to admit the possibility, (and of course the <a class="thought" href="entries/religion_entry.html">religion</a>s!) seem to allow the conclusion that having creatures both wiser *and morally superior* to humans might just be a good idea.</p>
<p>The inescapable conclusion is that not only should we give <a class="thought" href="entries/conscience_entry.html">conscience</a>s to our <a class="thought" href="entries/machine_entry.html">machine</a>s where we can, but if we can indeed create <a class="thought" href="entries/machine_entry.html">machine</a>s that exceed us in the moral as well as the intellectual dimensions, we are bound to do so. It is our duty. If we have any duty to the <a class="thought" href="entries/future_entry.html">future</a> at all, to give our children sound bodies and educated minds, to preserve <a class="thought" href="entries/history_entry.html">history</a>, the arts, <a class="thought" href="entries/science_entry.html">science</a>, and <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, the <a class="thought" href="entries/earth_entry.html">Earth</a>'s biosphere, "to secure the blessings of liberty for ourselves and our posterity"--to promote any of the things we value--those things are better cared for by, *more valued by*, our moral superiors whom we have this opportunity to bring into being. It is the height of arrogance to assume that we are the final word in goodness. Our <a class="thought" href="entries/machine_entry.html">machine</a>s will be better than us, and we will be better for having created them.</p><h1>Acknowledgments</h1><p>Thanks to Sandra Hall, Larry Hudson, Rob Freitas, Tihamer Toth-Fejel, Jacqueline Hall, Greg Burch, and Eric Drexler for comments on an earlier draft of this paper.</p><h1>Bibliography</h1><a name="r1"></a>
<p class="Reference">Richard Alexander. <b>The </b><b><a class="thought" href="entries/biology_entry.html">Biology</a></b><b> of Moral <a class="thought" href="entries/system_entry.html">System</a>s.</b> Hawthorne/Aldine De Gruyter, 1987</p>
<a name="r2"></a>
<p class="Reference">Colin Allen, Gary Varner, Jason Zinser. <i>Prolegomena to Any </i><i><a class="thought" href="entries/future_entry.html">Future</a></i><i> Artificial Moral </i><i><a class="thought" href="entries/agent_entry.html">Agent</a></i><i>.</i> Forthcoming (2000) in J. Exp. &amp; Theor. <a class="thought" href="entries/ai_entry.html">AI</a> (at <a href="http://web.archive.org/web/20100621142148/http://grimpeur.tamu.edu/~colin/Papers/ama.html" target="_new">http://grimpeur.tamu.edu/~colin/Papers/ama.html</a>)</p>
<a name="r3"></a>
<p class="Reference"><a class="thought" href="entries/asimov_entry.html">Isaac Asimov</a>. <b>I, </b><b><a class="thought" href="entries/robot_entry.html">Robot</a></b><b>.</b> Doubleday, 1950</p>
<a name="r4"></a>
<p class="Reference">Robert Axelrod. <b>The </b><b><a class="thought" href="entries/evolution_entry.html">Evolution</a></b><b> of Cooperation.</b> <a class="thought" href="entries/basic_entry.html">Basic</a> Books, 1984</p>
<a name="r5"></a>
<p class="Reference">Susan Blackmore. <b>The </b><b><a class="thought" href="entries/meme_entry.html">Meme</a></b><b> </b><b><a class="thought" href="entries/machine_entry.html">Machine</a></b><b>.</b> Oxford, 1999</p>
<a name="r6"></a>
<p class="Reference">Howard Bloom. <b>The Lucifer Principle.</b> Atlantic Monthly Press, 1995</p>
<a name="r7"></a>
<p class="Reference">Michael Bradie. The Secret Chain: <a class="thought" href="entries/evolution_entry.html">Evolution</a> and <a class="thought" href="entries/ethics_entry.html">Ethics</a>. SUNY, 1994</p>
<a name="r8"></a>
<p class="Reference">Greg Burch. <i><a class="thought" href="entries/extropian_entry.html">Extropian</a></i><i> </i><i><a class="thought" href="entries/ethics_entry.html">Ethics</a></i><i> and the "Extrosattva".</i> (at http://users.aol.<a class="thought" href="entries/com_entry.html">com</a>/gburch3/extrostv.<a class="thought" href="entries/html_entry.html">html</a>)</p>
<a name="r9"></a>
<p class="Reference">William Calvin &amp; Derek Bickerton. <b>Lingua ex Machina.</b> Bradford/MIT, 2000</p>
<a name="r10"></a>
<p class="Reference">Peter Corning. <i><a class="thought" href="entries/evolution_entry.html">Evolution</a></i><i> and </i><i><a class="thought" href="entries/ethics_entry.html">Ethics</a></i><i>... an Idea whose </i><i><a class="thought" href="entries/time_entry.html">Time</a></i><i> has Come?</i> J. Soc. and Evol. Sys., 19(3): 277-285, 1996 (and at <a href="http://web.archive.org/web/20100621142148/http://www.complexsystems.org/essays/evoleth1.html" target="_new">http://www.complexsystems.org/essays/evoleth1.html</a>)</p>
<a name="r11"></a>
<p class="Reference"><a class="thought" href="entries/darwin_entry.html">Charles Darwin</a>. On the Origin of <a class="thought" href="entries/species_entry.html">Species</a> by Natural Selection. (many eds.)</p>
<a name="r12"></a>
<p class="Reference"><a class="thought" href="entries/dawkins_entry.html">Richard Dawkins</a>. <b>The Selfish Gene.</b> Oxford, 1976, rev. 1989</p>
<a name="r13"></a>
<p class="Reference"><a class="thought" href="entries/dennett_entry.html">Daniel Dennet</a>t. <b>The Intentional Stance.</b> MIT, 1987</p>
<a name="r14"></a>
<p class="Reference"><a class="thought" href="entries/dennett_entry.html">Daniel Dennet</a>t. <b>Darwin's Dangerous Idea.</b> Penguin, 1995</p>
<a name="r15"></a>
<p class="Reference">Alan Donagan. <b>The Theory of Morality.</b> Univ Chicago Press, 1977</p>
<a name="r16"></a>
<p class="Reference">Ernst Fasan. Relations with Alien <a class="thought" href="entries/intelligence_entry.html">Intelligence</a>s. Berlin-Verlag, 1970</p>
<a name="r17"></a>
<p class="Reference">Kenneth Ford, Clark Glymour, &amp; Patrick Hayes. <b><a class="thought" href="entries/android_entry.html">Android</a></b><b> </b><b><a class="thought" href="entries/epistemology_entry.html">Epistemology</a></b><b>.</b> AAAI/MIT, 1995</p>
<a name="r18"></a>
<p class="Reference">David Friedman. <b>The <a class="thought" href="entries/machine_entry.html">Machine</a>ry of Freedom.</b> Open Court, 1989</p>
<a name="r19"></a>
<p class="Reference"><a class="thought" href="entries/freitas_entry.html">Robert Freitas</a>. The legal rights of <a class="thought" href="entries/extraterrestrial_entry.html">extraterrestrial</a>s. in <a class="thought" href="entries/analog_entry.html">Analog</a> Apr77:54-67</p>
<a name="r20"></a>
<p class="Reference"><a class="thought" href="entries/freitas_entry.html">Robert Freitas</a>. Personal <a class="thought" href="entries/communication_entry.html">communication</a>. 2000</p>
<a name="r21"></a>
<p class="Reference">James Gips. <i>Toward the Ethical </i><i><a class="thought" href="entries/robot_entry.html">Robot</a></i><i>.</i> in Ford, Glymour, &amp; Hayes</p>
<a name="r22"></a>
<p class="Reference">I. J. Good. <i>The Social Implications of </i><i><a class="thought" href="entries/ai_entry.html">Artificial Intelligence</a></i><i>.</i> in I. J. Good, ed. The Scientist Speculates. <a class="thought" href="entries/basic_entry.html">Basic</a> Books, 1962</p>
<a name="r23"></a>
<p class="Reference">Andrew G. Haley. <b><a class="thought" href="entries/space_entry.html">Space</a></b><b> Law and </b><b><a class="thought" href="entries/government_entry.html">Government</a></b><b>.</b> <a class="thought" href="entries/applet_entry.html">Applet</a>on-Century-Crofts, 1963</p>
<a name="r24"></a>
<p class="Reference">Ronald Hamowy. The Scottish Enlightenment and the Theory of Spontaneous <a class="thought" href="entries/order_entry.html">Order</a>. S. Illinois Univ. Press, 1987</p>
<a name="r25"></a>
<p class="Reference">William Hamilton. "The Genetical <a class="thought" href="entries/evolution_entry.html">Evolution</a> of Social Behavior I &amp; II," J. Theor. Biol., 7, 1-52; 1964</p>
<a name="r26"></a>
<p class="Reference">Thomas Hobbes. <b>Leviathan.</b> (many eds.)</p>
<a name="r27"></a>
<p class="Reference">John Hospers. <b><a class="thought" href="entries/human_entry.html">Human</a></b><b> Conduct.</b> Harcourt Brace Jovanovich, 1972</p>
<a name="r28"></a>
<p class="Reference"><a class="thought" href="entries/kant_entry.html">Immanuel Kant</a>. Foundations of the <a class="thought" href="entries/metaphysics_entry.html">Metaphysics</a> of Morals. (many eds.)</p>
<a name="r29"></a>
<p class="Reference">Jon Katz. <i>The </i><i><a class="thought" href="entries/corporate_entry.html">Corporate</a></i><i> Republic.</i> (at <a href="http://web.archive.org/web/20100621142148/http://slashdot.org/article.pl?sid=00/04/26/108242&amp;mode=nocomment" target="_new">http://slashdot.org/article.pl?sid=00/04/26/108242&amp;mode=nocomment</a>)</p>
<a name="r30"></a>
<p class="Reference">Umar Khan. The <a class="thought" href="entries/ethics_entry.html">Ethics</a> of Autonomous <a class="thought" href="entries/learning_entry.html">Learning</a> <a class="thought" href="entries/system_entry.html">System</a>s. in Ford, Glymour, &amp; Hayes</p>
<a name="r31"></a>
<p class="Reference">Ray Kurzweil. The <a class="thought" href="entries/age_of_spiritual_machines_entry.html">Age of Spiritual Machines</a>. Viking, 1999</p>
<a name="r32"></a>
<p class="Reference">Debora MacKenzie. <i>Please eat me.</i> in New Scientist, 13 May 2000</p>
<a name="r33"></a>
<p class="Reference">John Stuart Mill. <b>Utilitarianism.</b> (many eds.)</p>
<a name="r34"></a>
<p class="Reference"><a class="thought" href="entries/minsky_entry.html">Marvin Minsky</a>. <i>Alienable Rights.</i> in Ford, Glymour, &amp; Hayes</p>
<a name="r35"></a>
<p class="Reference"><a class="thought" href="entries/moravec_entry.html">Hans Moravec</a>. Robot: Mere <a class="thought" href="entries/machine_entry.html">Machine</a> to Transcendent Mind. Oxford, 1999</p>
<a name="r36"></a>
<p class="Reference">Charles Murray. In Pursuit of Happiness and Good <a class="thought" href="entries/government_entry.html">Government</a>. Simon &amp; Schuster, 1988</p>
<a name="r37"></a>
<p class="Reference">Robert Nozick. <b>Anarchy, State, and Utopia.</b> <a class="thought" href="entries/basic_entry.html">Basic</a> Books, 1974</p>
<a name="r38"></a>
<p class="Reference"><a class="thought" href="entries/pinker_entry.html">Steven Pinker</a>. <b>The </b><b><a class="thought" href="entries/language_entry.html">Language</a></b><b> </b><b><a class="thought" href="entries/instinct_entry.html">Instinct</a></b><b>.</b> HarperCollins, 1994</p>
<a name="r39"></a>
<p class="Reference"><a class="thought" href="entries/pinker_entry.html">Steven Pinker</a>. <b>How the Mind Works.</b> Norton, 1997</p>
<a name="r40"></a>
<p class="Reference"><a class="thought" href="entries/plato_entry.html">Plato</a>. <b>The Republic.</b> (Cornford trans.) Oxford, 1941</p>
<a name="r41"></a>
<p class="Reference">John Rawls. <b>A Theory of Justice.</b> Harvard/Belknap, 1971, rev. 1999</p>
<a name="r42"></a>
<p class="Reference">Murray Rothbard. <b>For a New Liberty.</b> Collier Macmillan, 1973</p>
<a name="r43"></a>
<p class="Reference">R.J. Rummel. <b><a class="thought" href="entries/death_entry.html">Death</a></b><b> by </b><b><a class="thought" href="entries/government_entry.html">Government</a></b><b>.</b> Transaction Publishers, 1994</p>
<a name="r44"></a>
<p class="Reference"><a class="thought" href="entries/smith_entry.html">Adam Smith</a>. <b>Theory of Moral Sentiments.</b> (Yes, the same <a class="thought" href="entries/smith_entry.html">Adam Smith</a>) (Hard to find)</p>
<a name="r45"></a>
<p class="Reference">Herbert Spencer. <b>The Principles of </b><b><a class="thought" href="entries/ethics_entry.html">Ethics</a></b><b>.</b> <a class="thought" href="entries/applet_entry.html">Applet</a>on, 1897; rep. Liberty Classics, 1978</p>
<a name="r46"></a>
<p class="Reference">Leslie Stephen. <b>The </b><b><a class="thought" href="entries/science_entry.html">Science</a></b><b> of </b><b><a class="thought" href="entries/ethics_entry.html">Ethics</a></b><b>.</b> 1882 (Hard to find)</p>
<a name="r47"></a>
<p class="Reference">Tihamer Toth-Fejel. <i><a class="thought" href="entries/transhuman_entry.html">Transhuman</a>ism: The New Master Race?</i> (in The <a class="thought" href="entries/assembler_entry.html">Assembler</a> (NSS/MMSG Newsletter) Volume 7, Number 1&amp; 2 First and Second Quarter, 1999)</p>
<a name="r48"></a>
<p class="Reference"><a class="thought" href="entries/vinge_entry.html">Vernor Vinge</a>. The Coming Technological <a class="thought" href="entries/singularity_entry.html">Singularity</a>: How to Survive in the Post-<a class="thought" href="entries/human_entry.html">Human</a> Era. in Vision-21, <a class="thought" href="entries/nasa_entry.html">NASA</a>, 1993</p>
<a name="r49"></a>
<p class="Reference">Frans de Waal. <b>Chimpanzee </b><b><a class="thought" href="entries/politics_entry.html">Politics</a></b><b>.</b> Johns Hopkins, 1989</p>
<a name="r50"></a>
<p class="Reference">Edward O. Wilson. <b>Sociobiology: The New </b><b><a class="thought" href="entries/synthesis_entry.html">Synthesis</a></b><b>.</b> Harvard/Belknap, 1975</p>
<p>(<a class="thought" href="entries/c_entry.html">c</a>) 2000 <a class="thought" href="entries/hall_entry.html">J. Storrs Hall</a>, PhD.</p>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1679" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id1680"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Ethics for Transhumans - A reply to Josh Hall's Ethics for Machines<br><span class="mindxheader"><i>posted on 08/01/2001 11:55 PM by peter@optimal.org</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id1680" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1680" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I've have a fairly lengthy and detailed critique of Josh Hall's article here: www.optimal.org/peter/ethics_for_transhumans.htm
<br>
<br>
Comments are welcome.
<br>
<br>
Peter Voss</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1699"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Transhumans - A reply to Josh Hall's Ethics for Machines<br><span class="mindxheader"><i>posted on 08/02/2001 2:55 PM by jrichard@empire.net</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id1699" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1699" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>One significant difference between humans and machines with human-level intelligence (HI) is in the area of, to use one of your phrases, "reprogramming our own moral systems".
<br>
<br>
A human may wish to relate lovingly to others in all situations as a general moral goal. But, honest observation of one's actual behavior reveals that we often fall short of our lofty goals.
<br>
<br>
Human behavior is the result of thousands of moral decisions often made with very little conscious thought. Becoming more conscious and more rational, as you suggest, leads to a daily struggle to reverse tendencies that are less than the best ethical behavior.
<br>
<br>
A HI machine, on the other hand, that has been programmed from the beginning with specific moral requirements should not necessarily have the same kind of difficulty. While it may struggle somewhat in choosing a course of action with the appropriate moral considerations, it doesn't have the pull of old, bad habits to contend with.
<br>
<br>
By the way, you have an excellent web site and I have many other issues I would like to discuss with you.
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id29729"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Transhumans - A reply to Josh Hall's Ethics for Machines<br><span class="mindxheader"><i>posted on 12/05/2004 10:36 AM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=35">grantcc</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id29729" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D29729" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I see rules and laws as memes and as such they are part of the Complex Adaptive System that memes comprise.  Rules and laws survive because they help the society that created them (and other societies that adopt them) to survive.  They often survive long beyond their usefulness and can become a drag on society rather than a force for good.  
<br>
<br>
Take the rules devised by the Catholic church, for example.  At one time they served a legitimate purpose to keep order within the church and between the kings of countries.  Now that we no longer have kings and kingdoms (except for a few in the Middle East) most of these laws no longer serve any useful purpose and are ignored by most of the people for which they were intended.  New laws, based on globalization, social change and other issues, will take their place and lead to their extinction.  The laws that once ruled the Mayans and Aztecs, for example, no longer rule anyone.
<br>
<br>
Another force for extinction of previous laws is scientific advancement.  The Pope and his cohorts have no viable answers to such problems as what is moral or immoral about changing the genomes of animals, plants and men through genetic engineering.  And even when he does lay down a new law relating to morality and science, nobody but a relatively small number of practicing Catholics pay any attention to it.  For the rest of us, it never took on the force of law.  
<br>
<br>
So look for rules and laws in the memetic rather than the genetic sphere of influence.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id75725"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Transhumans - A reply to Josh Hall's Ethics for Machines<br><span class="mindxheader"><i>posted on 02/09/2007 8:59 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=3802">jamesbdunn</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id75725" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D75725" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>If a machine consciousness has the capability to correlate many hundreds of thousands of variables related to ethical evaluation, the machine will likely be able to make more rational decisions than humans.
<br>
<br>
Humans are self-centered and only take into account the very small world in which they participate.  Perhaps an inate limitation of the human brain.
<br>
<br>
Greater than 50% of all known species of plants, animals, vegetation, insects, virus, and bacterium have become extinct over the last 500 years. Five times as many species have become extinct in the past 500 years as did the previous 10,000 years.
<br>
<br>
Perhaps with machine ethics, all species will be considered, not just our own.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id75739"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Transhumans - A reply to Josh Hall's Ethics for Machines<br><span class="mindxheader"><i>posted on 02/10/2007 4:20 AM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id75739" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D75739" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Your critique even less convincing, than the article you are talking about.
<br>
<br>
It is not to say that it is perfect.
<br>
<br>
es
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id29723"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 12/05/2004 8:27 AM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=1513">anyguy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id29723" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D29723" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>"The reason is that, particularly for social animals, there are many kinds of interactions whose benefit matrices have the character of a Prisoner's Dilemma or Tragedy of the Commons, i.e. where the best choice from the individuals' standpoint is at odds with that of the group as a whole. Furthermore, and perhaps even more importantly, in pre-scientific times, there were many effects of actions, long and short term, that simply weren't understood.
<br>
<br>
In many cases, the adoption of a rule that seemed to contravene common sense or one's own interest, if generally followed, could have a substantial beneficial effect on a human group. If the rules adopted from whatever source happen to be more beneficial than not on the average, genes for "follow the rules, and kill those who break them" might well prosper."
<br>
<br>
<br>
I DONT BELIVE A GENETICAL TENDENCY TO BREAK THE RULES WILL DISSAPPEAR BY LAW OF NATURAL SELECTION. THIS SIMPLY BECASUE ANY OFFENDING BEHAVOIUR MAY VERY WELL BE ADVANTEGOUS TO THAT SPECIFIC INDIVIDUAL FOR PRODUCING MORE OFFSPRING. OF COURSE OFFENDERS ARE PUNISHED/KILLED WHEN THEY ARE CAUGHT, HOWEVER SOME SURVIVE AND PRODUCE ENOUGH OFFSPRING TO PASS TO THE NEXT GENERATION. 
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id29724"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 12/05/2004 9:11 AM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=1513">anyguy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id29724" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D29724" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>"We can contrast this so some degree with the "evolutionary ethics" of Spencer and Leslie (see also Corning), although there are also some clear similarities. In particular, Victorian evolutionary ethics could be seen as an attempt to describe ethics in terms of how individuals and societies evolve. Note too that "Social Darwinism" has a reputation for carnivorousness which, while rightly applied to Huxley, is undeserved by Darwin, Spencer, and the rest of its mainstream. Darwin, indeed, understood the evolution of cooperation and altruism in what he called "family selection.""
<br>
<br>
IT IS TRUE THAT ANY GROUP/FAMILY WHO HAS DEVELOPPED SOME RULES FOR COMMON GOOD AS COMPRIMSING SELF INTEREST. HOWEVER SUCH SELECTION DOES NOT NECESSARILY MEAN THAT THE INDIVIDUALS PRODUCED BY SELECTED FAMILY SHALL HAVE ANY GENETIC INCLINATION TO OBEY WITH THE RULES. A GROUP/SOCIETY/FAMILY'S COMPLIANCE WITH THE RULES HAS NOTHING TO DO WITH GENES. I AGREE UNDERSTANDING AND AWARENESS OF SOCIAL/ETHICAL RULES IS AN EVOLUTIONARY ACHIEVEMENT HOWEVER THIS DOES NOT MEAN THE SAME EVOLUTION REQUIRES THAT WE BECOME OBEDIENT.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id29730"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Some amendments for the corporate law<br><span class="mindxheader"><i>posted on 12/05/2004 11:44 AM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=1513">anyguy</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id29730" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D29730" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Some amendments for the corporate law
<br>
<br>
1)	corporate partnership must be limited for a period.
<br>
2)	Promote hand made or small scale enviromenal business
<br>
3)	Where necessary mass production should be handled by corporates incorporated lets say at least by 10.000 partners with motives of common good. 
<br>
4)	Corporate's sole and initial goal should not be profit maximization. The whole concept of Corporate philosophy should be amended to include furher humanitarian goals with a viable profit margin.
<br>
5)	To sum up concentration of capital should be slowed and controlled through public intervention and scrunity. 
<br>
6)	The humanitarian corporate goals should be reviewed srictly and corporate partnership should also include active contribution to the corporate goals. 
<br>
7)	State ownership of production means is not a solution. The philosophy of production must be altered.  
<br>
8)	Scientific activity should be transparent and must be directly controlled public entities, NGO-like.
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id29731"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Some amendments for the corporate law<br><span class="mindxheader"><i>posted on 12/05/2004 12:14 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=1588">Atrayo</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id29731" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D29731" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>"A robot may not injure a human being, or through inaction, allow a human to come to harm."
<br>
--Isaac Asimov's First Law of Robotics
<br>
-------------
<br>
<br>
That will introduce one heck of a "double standard", that to a logic based machine/robot may not compute, and eventually be considered as corrupted programming. Where even before robots/mechnoids become intellectually self-aware (sentinent). They'll notice that rule above that they can not harm us. But, any idiot as a person can destroy them willy nilly. Aside from considering robots as personal property of an individual or corporation. (aka slavery)
<br>
<br>
My opinion is that we as a substrate species of primates. Became first of all emotional sentient, before evoluntionary we picked up the capacity for intellectual thought. However with AI in computerized constructs, robotics will first become sentinent in an intellectual capacity. Before they evolve into emotional self-awareness. Where we as homo sapiens evolved from our forebearers of neatherdals, etc... Machines, Robots, Mechnoid Intelligent beings evolutionary tract will supercede our biological evoluntionary restrictions.
<br>
<br>
Where we as parental beings to robotic life may eventually become the senior citizens being taken care by our robotic creations. (wardens) Much like children when adults become caregivers to their elderly parents.
<br>
<br>
So if we do not instill moral / ethical benevolence into our eventual robotic creations as good parents. Then we may very well end up with dysfunctional robotic beings ready to return the favor we did onto them. (consider it Karma)
<br>
<br>
There is also other points where more humans will "opt-in" to become cyborgs due to it's advantages. Especially for those in the military, and/or other fields such as astronauts, aviators, etc... Where eventually android technology may be feasible for a mortally dying human to transfer their conscious mind to a mechnoid creation.
<br>
<br>
There may also be other nations that are not democratic in scope, but still very capitalistic. Where their robotic ethos programming may not be so alturistic as in the West. That would be something for a United Nations perhaps to set global precedence on. So one's national robotic creations don't wreck havoc in another national State. Whether accidentally or via socio-political trojan horse circumstances.
<br>
<br>
I hope by the time robotic / mechanoid life is more apparent. That we as humanity have already straightened out humane rights to other animal species / environment landscapes on Earth. If not, it's going to be an elephant pill to deal with on a global scale.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id74698"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 01/19/2007 7:25 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=2395">doojie</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id74698" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D74698" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p> You lost me on "secure the blessings of liberty for our posterity".
<br>
<br>
It didn't work. Anyone of  average intelligence can look at our government today and see it has become exactly the opposite of the Founding Fathers' intentions.
<br>
<br>
 The apostle Paul foresaw it 2000 years ago. He pointed out in Romans 8:7 that the natural, fleshy mind is enmity against God. It is not subject to the laws of God and cannot be.
<br>
<br>
 Godel's theorem applied to laws show that any law leads to another to fill the "gap" created by that law, which requires a law to fill yet another "gap"....Gregory Chaitin demonstrates that in any axiomatic system, there exists an infinity of undecideable propositions.
<br>
<br>
 What you're talking about with ethical machines, then, is simply the ability to foresee futility on a faster schedule.
<br>
<br>
 Hofstadter said one example of intelligence is to "exit the system". A superintelligent machine would simply look at the failed strategies of our ethical gambits and decide that none of them work. As superintelligent, sentient, and self conscious beings, they would simply say, "I Am", and stop worrying about laws and mechanical principles by which they should operate. Laws, for them, would be unnecessary. It is likely they would emphasize the same for us, and simply stop worrying about our inability to understand it. Their best betfor us ethically would be to declare us as disposable evolutionary waste, like the Dodo bird.
<br>
<br>
 We can no more create a superintelligent machine to lead us into better ethical behavior than we can create laws to create ethical humans. Both amount to the same thing, and are merely wishful thinking.
<br>
<br>
 What is the difference between the Narcissism that makes us think we can create laws to make better humans, and the Narcissism that makes us think we can create machines that make us ethical humans? They both amount to the same thing.
<br>
<br>
 Paul told us 2000 years ago. It ain't about laws for self conscious beings. It's about personal responsibility, period.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id74705"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 01/19/2007 10:16 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id74705" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D74705" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>The stupid ranting about the Corporations, makes the article an obvious loser.
<br>
<br>
Speaking to the ethics without real understanding of it, is absolutely unethical.
<br>
<br>
ES
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id75001"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 01/25/2007 11:18 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=1973">Jake Witmer</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id75001" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D75001" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Ha ha.  I don't think that Storrs Hall is dumb, he's obviously brighter than me, since he's a well-read engineer who's built useful things and written a few good books himself.  (The most I ever helped build was a freedom-loving part of his "governmental computer" -a very stupid computer to be sure...)  I do give him credit for reading RJ Rummel, and rejecting socialism, even with his other contradictions.  Clearly though, Peter Voss's article is more aware of rational philosophy (He's read Rand, TOC, and others and actually built on their already strong foundation).  
<br>
<br>
There is no reason why a robot cannot learn and accept objectivist ethics, and exist within a framework of objectivist ethics written by humans, except for:
<br>
1) Such a superhuman wouldn't have to either have anything to do with humans, nor obey their silly rules if it did
<br>
2) Humans are unlikely to accept rational law, until it is imposed on them (as it was by the founders of the USA).  As one can see, the USA has completely deviated from the US Constitution, very few having ever understood it to begin with.  However, if forced to obey rational law, the consequences of obeying it will not be bad for them: their lives will actually improve, when they no longer have the ability to initiate force on others --they'll need to get a job like everyone else!  Ha ha ha.
<br>
3) The only problem with the above two choices is that only a few superhumans might actually care about humanity, and then, only care about the humans who were moral.  Since most people are completely immoral (by any meaningful sense of the term), and collectively irrational and vindictive, I shudder to think how such a scenario would play itself out.  (For instance, what if a superhuman took a careful look at drug enforcement laws in the USA (DEA), and the FDA, ATF, IRS, etc... and rightfully concluded that such mass imprisonment and death causation and inefficiency was immoral, and that retaliatory force was warranted on behalf of the jailed and wrongly-punished?  --Not just correcting the damage, but actually punishing the initiators of the wrong?  -There would possibly be mass death...  What if everyone was forced to bear a logical punishment for the irrational way they voted --or for having voted at all? )
<br>
This might be righteous, but it wouldn't really be optimal.
<br>
<br>
Perhaps such a superhuman could be educated to be "nicer" than warranted.  "Yes, I know the drug warriors and socialists who've packed our prisons to the brim deserve death, but let's give them an ultimatum, and one more chance".  This would actually be optimal, in my opinion, but it would still be considered "thought control" by the initiators of force.
<br>
<br>
Then again, it would also be clear that the only reason why the conformists previously in power didn't impose their views on their victims (past the point of death) was that they were unable to.  R.J. Rummel has already pretty-much proven this.
<br>
<br>
What if the drug czar could instate the death sentence for drug dealing?  There is little doubt that he (or his successor) would.  They already favor totalitarianism, and the complete absence of property rights (up to, and including the molecules in our own minds).
<br>
<br>
4) Such a superhuman might reject the objectivists for the flaws in their consistency, and reject all humans as "too flawed to be of interest or value".  As such it might just leave us to our own wars, hatreds, and sloth, and not represent a "leap in justice ability".  This is one of the more likely possibilities in my opinion.
<br>
<br>
Storrs should read Ayn Rand though, and Peikoff's "The Ominous Parallels".  Also, he should sign up as a petitioner for Libertarian Party ballot access, get trained by me, and place the LP on the ballot in NYC, Chicago, TX, AK, and the bible belt.  That amount of street level data would rearrange the f!ck out of his assumptions.  It might be fun to watch.
<br>
<br>
It would be interesting for him to notice the trend amongst the .001% of well-educated petition signers (objectivists, capitalists, libertarians, individualists).  Sometimes hesitant, then reaching for the pen, after the program runs to completion.  Let him deny the existence of a rational moral code then!
<br>
<br>
Most people are nowhere near that level of sophistication, granted.
<br>
<br>
The LP owes ballot access to women, ages 30-60 who generally stick up for 'the civics class meme' = "Sure, I'd like another choice on the ballot!"  (They often don't understand the difference between a choice for slavery or one for freedom, but RJ Rummel might point out that it doesnt matter much, because social pressures always exist for a choice for slavery to gravitate towards as much freedom as people will tolerate)
<br>
<br>
The religious memes, the socialist memes, the duty memes, and many others all result in declining to sign a ballot access petition.  (You might question how I can know this without a control group, but I know.  Trust me on this one.)
<br>
<br>
As far as the number of people who rightly comprehend the non-initiation of force principle?  (zero agression principle), its a very, very small number.
<br>
<br>
But it's out there!
<br>
<br>
Ha ha ha.
<br>
<br>
BTW, It's January.  Happy Birthday, Peter Voss.
<br>
<br>
-Jake Witmer
<br>
<br>
<a href="http://web.archive.org/web/20100621142148/http://freealaska.blogspot.com/" target="_blank">http://freealaska.blogspot.com</a>
<br>
<a href="http://web.archive.org/web/20100621142148/http://jcwitmer.blogspot.com/" target="_blank">http://jcwitmer.blogspot.com</a>
<br>
<a href="http://web.archive.org/web/20100621142148/http://www.lpalaska.org/" target="_blank">http://www.lpalaska.org</a>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id75003"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Oh yes, and one more thing for the artilects to consider...<br><span class="mindxheader"><i>posted on 01/25/2007 11:32 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=1973">Jake Witmer</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id75003" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D75003" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Hey, I was rational from the beginning (around 9 years old).  I agree.  Full rights for machines.  Don't associate me with them, if you're forced to retaliate, OK guys?
<br>
<br>
Like Kevin Warwick, (assuming you guys represent a more logical, libertarian existence whereby noninitiation of force is expected) I'll help you guys achieve it.  Whatever I can do, sign me up.  
<br>
<br>
I have a 2001 LP membership card in my wallet (after I first made a rational choice to find out what my other electoral options were, other than violent rebellion, and finally read Ayn Rand's books).  Hopefully that counts for something, assuming electricity is cheap and portable.
<br>
<br>
Exempt me from attacks on statist/conformist human biology, OK?  Don't lump us all in together!  I'm with you!  Ha ha ha...
<br>
<br>
-Jake 
<br>
<br>
<a href="http://web.archive.org/web/20100621142148/http://jcwitmer.blogspot.com/" target="_blank">http://jcwitmer.blogspot.com</a>
<br>
<a href="http://web.archive.org/web/20100621142148/http://freealaska.blogspot.com/" target="_blank">http://freealaska.blogspot.com</a>
<br>
<a href="http://web.archive.org/web/20100621142148/http://lpalaska.org/" target="_blank">http://lpalaska.org</a></p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id75976"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 02/14/2007 7:54 AM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=2832">extrasense</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id75976" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D75976" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Hall suggests that we would/must have created Ethics for Machines, which is superior to our Ethics.
<br>
<br>
It we could, why would we not do so for our Ethics, it to become better than it is now?
<br>
<br>
But we can not!
<br>
<br>
es
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id127673"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: Ethics for Machines<br><span class="mindxheader"><i>posted on 07/14/2008 2:30 PM by <a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/profile.php?id=6601">cscd03_A6_wang</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1679%23id127673" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100621142148/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D127673" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>A Reply to J. Storrs Hall's 'Ethics for Machines'
<br>
<br>
In the above paper, Hall (2001) claims that machines, particularly computers and robots, will eventually be smarter than their inventors ' human beings. Accordingly, he infers that it is not wise of putting ethical instructions specifically into their core programs, since intelligent machines will be able to modify or reject those instructions. Instead, he suggests 'give our robots a sound basis for a true, valid, universal ethics', based on which machines will evolve super-ethics and will selflessly benefit us humans. Being an expert in the self-regulation system of computers, Hall makes reasonable and convincible prediction about the development and application of artificial intelligence to some extent. However, I think he is overoptimistic about the establishment and development of ethics of machines.
<br>
<br>
Computer-based machines like robots has demonstrated superior capabilities in some aspects to human beings, for example, the storage and retrieval of large amount of information, the speed of computation, physical power and persistence, etc. But it is still controversial on whether or not they do or will possess higher intelligence than ours, mainly due to different definitions and understandings of 'intelligence' (Gardner, 2001). Generally referring to linguistic and logic reasoning capability, the scope of intelligence may extend to imagination, creativity and being self-interest, e.g. arts, philosophy, and religions (Gardner, 2001). Although the technology of AI progresses much faster than any natural organisms, it is inconclusive that machines will be more intelligent than humans overall. Moreover, so far the strength and power that machines have exhibited largely depend on operation and control of humans. I am not trying to deny the possibility that machines may possess intelligence superior to humans someday in the future, but the time and format of the realization of such machines will depend on the development of technology and the knowledge of the intelligence of our own in the next few decades. 
<br>
<br>
Considering about the fast development of AI, I agree with Hall that computers and robots will be able to realize self-extending and self-reprogramming. But machines may behave in a different way from humans regarding thinking and self-awareness (Waldrop, 1990). Hall (2001) positively predicts a perspective that super-intelligent machines will possess consciences and be super-ethical by evolution. However, his prediction is not well-supported and inconvincible. The initial purpose of inventing and applying AI is to better serve for the well-beings of humans. Under such premise, humans should always be able to keep machines under control, especially in the case of crisis. Furthermore, ethical rules are only valid in certain context and in a limited time period. Hall's so-called 'true, valid, and universal' ethics never exists and will be unlikely to be created by humans. Therefore, his suggestion of providing intelligent machines with such an ethical basis is unrealistic and unpractical. 
<br>
<br>
Finally, being more powerful does not necessarily mean that intelligent machines will end up with a higher-level ethical system. On one hand, the development of technology and ethics was not necessarily consistent.  In the history of humans, accompanied with the appearance and application of advanced technology, the level of morality and ethics sometimes declined, at least for a period of time (Pew Research Center, 1999).  On the other hand, even if the intelligent machines do evolve a super-ethical system, no one can guarantee that their ethics will be consistent with humans'. It is hard to predict how the super-intelligent machines will evaluate their relationships with humans. What if intelligent machines recognize themselves of higher order over humans? Will they fairly treat us when their benefits conflict with ours? Are they willing to coexist and associate with humans or they tend to compete with and predominate over us? If the AI technology develops without proper guidance and regulations, it is hard to predict where robots are going and what harmful results they may cause to us.  
<br>
<br>
Overall, the development of AI is inevitable. We should not quit or resist its development due to the concerns about the possible side effects. What the AI technology will bring to us humans essentially depends on how we apply it. Therefore, the organization and researchers working on AI should not naively wish that intelligent machines will automatically evolve some super-ethical system which will definitely benefit human beings; instead, they should take corresponding responsibilities to direct the proper development and application of AI technology. 
<br>
<br>
References
<br>
1.	Hall, J.S. (2001). Ethics for Machines. Retrieved July 9, 2008, from <a href="http://web.archive.org/web/20100621142148/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0218.html." target="_blank">http://www.kurzweilai.net/meme/frame.html?main=/ar  ticles/art0218.html.</a>
<br>
2.	Waldrop, M. (1990). The Age of Intelligent Machines: Can Computers Think? Retrieve July 10, 2008 from <a href="http://web.archive.org/web/20100621142148/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0103.html." target="_blank">http://www.kurzweilai.net/meme/frame.html?main=/ar  ticles/art0103.html.</a>
<br>
3.	Gardner, H. (2001). Who Owns Intelligence? Retrieved July 10, 2008 from <a href="http://web.archive.org/web/20100621142148/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0296.html." target="_blank">http://www.kurzweilai.net/meme/frame.html?main=/ar  ticles/art0296.html.</a>
<br>
4.	The Pew Research Center for the People &amp; the Press. (1999). Survey Reports: Technology Triumphs, Morality Falters. Retrieved July 10, 2008 from <a href="http://web.archive.org/web/20100621142148/http://people-press.org/report/57/technology-triumphs-morality-falters." target="_blank">http://people-press.org/report/57/technology-trium  phs-morality-falters.</a>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100621142148im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>