<html>
<head><base href="https://kurzweilai-brain.gothdyke.mom/"><link href="articlemaster.css" rel="stylesheet" title="style1" type="text/css">
<style>
.sidebar {border-left-width: 2px; border-right-width: 0px; border-top-width: 0px; border-bottom-width: 0px; border-color: #000000; border-style: solid; padding-left: 12px;}
</style>
<title>In Response to</title>
</head>
<body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0"><div id="centering-column"><div id="header">
  <div id="logo">
    <img src="logo.gif" />
  </div>
  <div id="title">
    <h1>Brain Archive</h1><br />
    <a href="">Entry Index</a>
  </div>
  <div class="clearer"></div>
</div>
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" height="100%" width="780">
<tr height="100%">
<td align="left" valign="top">
<table align="center" bgcolor="#EEEEEE" border="0" cellpadding="0" cellspacing="0" width="780">
<tr>
<td><img alt="" border="0" height="5" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="20"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="90"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="375"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="200"></td>
<td><img alt="" border="0" height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="30"></td>
</tr>
<tr>
<td> &#160; </td>
<td colspan="5"> <span class="breadcrumb"><a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/" target="_top">Origin</a> &gt;
 <a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/meme/memelist.html?m=2">Dangerous Futures</a> &gt; 
In Response to
<br>
Permanent link to this article: <a href="http://web.archive.org/web/20100614000926/http://www.kurzweilai.net/meme/frame.html?main=/articles/art0226.html" target="_top">http://www.kurzweilai.net/meme/frame.html?main=/articles/art0226.html</a></span>
<br>
<a class="printable" href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/articles/art0226.html?printable=1" target="_new">Printable Version</a></td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="50" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="1"></td></tr>
<tr>
<td> &#160; </td>
<td> &#160; </td>
<td valign="top"><span class="Title">In Response to</span>
<br>
<span class="Subtitle"></span>
<table border="0" cellpadding="0" cellspacing="0">
<td valign="top"><span class="Authors">by &#160;</span></td>
<td><span class="Authors">
<a class="Authors" href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/bios/frame.html?main=/bios/bio0005.html" target="_top">Ray Kurzweil</a><br></span></td>
</table>
<br>
<div class="TeaserText">Although George Gilder and Richard Vigilante share Ray Kurzweil's grave concerns about Bill Joy's apparently neo-Luddite calls for relinguishing broad areas of technology, Kurzweil is critical of Gilder and Vigilante's skepticism regarding the feasibility of the dangers.</div>
<br>
<br><p>Portions of this response were published in <i>The American Spectator</i>, March 2001. Published on KurzweilAI.net July 25, 2001.</p>
<p><a class="thought" href="entries/gilder_entry.html">George Gilder</a>'s "Stop Everything...It's Techno-Horror!" can be read <a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/articles/art0225.html" target="_self">here</a>.</p>
<p>Fundamentally, <a class="thought" href="entries/gilder_entry.html">George Gilder</a> and Richard Vigilante and I share a deeply critical reaction to <a class="thought" href="entries/joy_entry.html">Bill Joy</a>'s prescription of <a class="thought" href="entries/relinquishment_entry.html">relinquishment</a> of "our pursuit of certain types of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>." Just as George Soros attracted attention by criticizing the capitalist <a class="thought" href="entries/system_entry.html">system</a> of which he was a primary beneficiary, the credibility of Joy's treatise on the dangers of <a class="thought" href="entries/future_entry.html">future</a> <a class="thought" href="entries/technology_entry.html">technology</a> has been enhanced by his reputation as a primary architect of contemporary <a class="thought" href="entries/technology_entry.html">technology</a>. Being a technologist, Joy claims not to be anti-<a class="thought" href="entries/technology_entry.html">technology</a>, saying that we should keep the beneficial technologies, and relinquish only those dangerous ones, like <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>. The problem with Joy's view is that the dangerous technologies are exactly the same as the beneficial ones. The same <a class="thought" href="entries/biotechnology_entry.html">biotechnology</a> tools and <a class="thought" href="entries/knowledge_entry.html">knowledge</a> that will save millions of <a class="thought" href="entries/future_entry.html">future</a> lives from <a class="thought" href="entries/cancer_entry.html">cancer</a> and other diseases could potentially provide a terrorist with the means for creating a bioengineered <a class="thought" href="entries/pathogen_entry.html">pathogen</a>. The same <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> that will eventually help clean up the environment and provide material products at almost no cost are the same technologies that could be misused to introduce new <a class="thought" href="entries/nonbiological_entry.html">nonbiological</a> <a class="thought" href="entries/pathogen_entry.html">pathogen</a>s.</p>
<p>I call this the deeply intertwined promise and peril of <a class="thought" href="entries/technology_entry.html">technology</a>, and it's not a new story. <a class="thought" href="entries/technology_entry.html">Technology</a> empowers both our creative and destructive <a class="thought" href="entries/nature_entry.html">nature</a>s. Stalin's tanks and Hitler's trains used <a class="thought" href="entries/technology_entry.html">technology</a>. Yet few people today would really want to go back to the short (<a class="thought" href="entries/human_entry.html">human</a> live span less than half of today's), brutish, disease-filled, poverty-stricken, labor-intensive, disaster-prone lives that ninety-nine percent of the <a class="thought" href="entries/human_entry.html">human</a> race struggled through a few centuries ago.</p>
<p>We can't have the benefits without at least the potential dangers. The only way to avoid the dangerous technologies would be to relinquish essentially all of <a class="thought" href="entries/technology_entry.html">technology</a>. And the only way to accomplish that would be a totalitarian <a class="thought" href="entries/system_entry.html">system</a> (e.g., Brave New World) in which the state has exclusive use of <a class="thought" href="entries/technology_entry.html">technology</a> to prevent everyone else from advancing <a class="thought" href="entries/technology_entry.html">technology</a>. Joy's recommendation does not go that far obviously, but his call for relinquishing broad areas of the pursuit of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> is based on an unrealistic assumption that we can parse safe and risky areas of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>.</p>
<p>Gilder and Vigilante write, "in the <a class="thought" href="entries/event_entry.html">event</a> of. . . an unplanned bio-catastrophe, we would be far better off with a powerful and multifarious biotech industry with long and diverse experience in handling such perils, constraining them, and inventing remedies than if we had "relinquished" these technologies to a small elite of <a class="thought" href="entries/government_entry.html">government</a> scientists, their work closely classified and shrouded in secrecy."</p>
<p>I agree quite hardily with this eloquent perspective. Consider as a contemporary test case, how we have dealt with one recent technological challenge. There exists today a new form of fully <a class="thought" href="entries/nonbiological_entry.html">nonbiological</a> self-replicating <a class="thought" href="entries/entity_entry.html">entity</a> that didn't exist just a few decades ago: the <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/virus_entry.html">virus</a>. When this form of destructive intruder first appeared, strong concerns were voiced that as they became more sophisticated, <a class="thought" href="entries/software_entry.html">software</a> <a class="thought" href="entries/pathogen_entry.html">pathogen</a>s had the potential to destroy the <a class="thought" href="entries/computer_entry.html">computer</a> <a class="thought" href="entries/network_entry.html">network</a> medium they live in. Yet the "<a class="thought" href="entries/immune_system_entry.html">immune system</a>" that has evolved in response to this challenge has been largely effective. Although destructive self-replicating <a class="thought" href="entries/software_entry.html">software</a> entities do cause damage from <a class="thought" href="entries/time_entry.html">time</a> to <a class="thought" href="entries/time_entry.html">time</a>, the injury is but a tiny fraction of the benefit we receive from the <a class="thought" href="entries/computer_entry.html">computer</a>s and <a class="thought" href="entries/communication_entry.html">communication</a> links that harbor them.</p>
<p>One might counter that <a class="thought" href="entries/computer_entry.html">computer</a> viruses do not have the lethal potential of <a class="thought" href="entries/biological_entry.html">biological</a> viruses or of destructive <a class="thought" href="entries/future_entry.html">future</a> <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>. Although true, this only strengthens my observation. The fact that <a class="thought" href="entries/computer_entry.html">computer</a> viruses are not usually deadly to humans (although they can be if they intrude on <a class="thought" href="entries/mission_critical_entry.html">mission critical system</a>s such as airplanes and intensive care units) only means that more people are willing to create and release them. It also means that our response to the danger is relatively relaxed. Conversely, when it comes to <a class="thought" href="entries/future_entry.html">future</a> self replicating entities that may be potentially lethal on a large scale, our response on all levels will be vastly more intense.</p>
<p>Joy's treatise is effective because he paints a picture of <a class="thought" href="entries/future_entry.html">future</a> dangers as if they were released on today's unprepared world. The reality is that the sophistication and power of our defensive technologies and <a class="thought" href="entries/knowledge_entry.html">knowledge</a> will grow along with the dangers. When we have gray goo, we will also have blue goo ("police" <a class="thought" href="entries/nanobot_entry.html">nanobot</a>s that combat the "bad" <a class="thought" href="entries/nanobot_entry.html">nanobot</a>s). The story of the twenty-first century has not yet been written, so we cannot say with assurance that we will successfully avoid all misuse. But the surest way to prevent the development of the defensive technologies would be to relinquish the pursuit of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> in broad areas, which would only drive these efforts underground where they would be dominated by the least reliable practitioners (e.g., the terrorists).</p>
<p>There is still a great deal of suffering in the world. Are we going to tell the millions of <a class="thought" href="entries/cancer_entry.html">cancer</a> patients that we're canceling all <a class="thought" href="entries/cancer_entry.html">cancer</a> <a class="thought" href="entries/research_entry.html">research</a> despite very promising emerging treatments because the same <a class="thought" href="entries/technology_entry.html">technology</a> might be abused by a terrorist? Consider the following tongue-in-cheek announcement, which I read during a radio debate with Joy: "<a class="thought" href="entries/sun_microsystems_entry.html">Sun Microsystems</a> announced today that it was relinquishing all <a class="thought" href="entries/research_entry.html">research</a> and development that might improve the <a class="thought" href="entries/intelligence_entry.html">intelligence</a> of its <a class="thought" href="entries/software_entry.html">software</a>, the <a class="thought" href="entries/computation_entry.html">computation</a>al power of its <a class="thought" href="entries/computer_entry.html">computer</a>s, or the effectiveness of its <a class="thought" href="entries/network_entry.html">network</a>s due to concerns that the inevitable result of <a class="thought" href="entries/progress_entry.html">progress</a> in these fields may lead to profound and irreversible dangers to the environment and even to the <a class="thought" href="entries/human_entry.html">human</a> race itself. 'Better to be safe than sorry,' Sun's Chief Scientist <a class="thought" href="entries/joy_entry.html">Bill Joy</a> was quoted as saying. Trading of Sun shares was automatically halted in accordance with Nasdaq trading rules after dropping by 90 percent in the first hour of trading." Joy did not find my mock announcement amusing, but my point is a serious one: advancement in a broad array of technologies is an economic imperative.</p>
<p>Although I agree with Gilder and Vigilante's opposition to the essentially totalitarian <a class="thought" href="entries/nature_entry.html">nature</a> of the call for <a class="thought" href="entries/relinquishment_entry.html">relinquishment</a> of broad areas of the pursuit of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and <a class="thought" href="entries/technology_entry.html">technology</a>, their American Spectator article directs a significant portion of its argument against the technical feasibility of the dangers. This is not the best strategy in my view to counter Joy's thesis. We don't have to look further than today to see that <a class="thought" href="entries/technology_entry.html">technology</a> is a double-edged sword.</p>
<p>They write, for example, that "But there are, to date, no <a class="thought" href="entries/nanobot_entry.html">nanobot</a>s," and go on to cast doubt on their feasibility. Of course, it is the <a class="thought" href="entries/nature_entry.html">nature</a> of <a class="thought" href="entries/future_entry.html">future</a> <a class="thought" href="entries/technology_entry.html">technology</a> that it doesn't exist today. But Gilder, as the author of two outstanding books (The <a class="thought" href="entries/microcosm_entry.html">Microcosm</a> and The <a class="thought" href="entries/telecosm_entry.html">Telecosm</a>) that document the <a class="thought" href="entries/exponential_growth_entry.html">exponential growth</a> of diverse technologies, recognizes that these trends are not likely to stop any <a class="thought" href="entries/time_entry.html">time</a> soon. Combined with the equally compelling trend of <a class="thought" href="entries/miniaturization_entry.html">miniaturization</a> (we're currently shrinking both <a class="thought" href="entries/electronic_entry.html">electronic</a> and mechanical <a class="thought" href="entries/technology_entry.html">technology</a> by a factor of 5.6 per linear dimension per decade), it is <a class="thought" href="entries/reason_entry.html">reason</a>able to conclude that technologies such as <a class="thought" href="entries/nanobot_entry.html">nanobot</a>s are inevitable within a few decades. There are many positive <a class="thought" href="entries/reason_entry.html">reason</a>s that <a class="thought" href="entries/nanobot_entry.html">nanobot</a>s will be developed including dramatic implications for health, the environment, and the economy.</p>
<p>Gilder and Vigilante refer to the "Joy-Drexler-Lovins, <a class="thought" href="entries/gnr_entry.html">GNR</a>, trinity of Techno-Horror." I would suggest not including Eric Drexler in this line-up. As the principal original theorist of the feasibility of <a class="thought" href="entries/technology_entry.html">technology</a> on a nanometer scale, and the founder, along with his wife, <a class="thought" href="entries/peterson_entry.html">Christine Peterson</a>, of the <a class="thought" href="entries/foresight_institute_entry.html">Foresight Institute</a>, a leading <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> think tank, Drexler is hardly anti-<a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>. I would not call Drexler's vision "bipolar" and "manic-depressive" because his original treatise describes the potential dangers of self-replicating entities built on a nanometer scale (which, incidentally does not mean that the entities are one nanometer in size, but rather that key features are measured in nanometers). We clearly don't consider the nuclear power industry to be anti-nuclear power, but we would nonetheless expect them to recognize the potential dangers of a reactor melt-down, and to take stringent steps to avoid such a disaster.</p>
<p>The <a class="thought" href="entries/foresight_institute_entry.html">Foresight Institute</a> has been developing ethical guidelines and <a class="thought" href="entries/technology_entry.html">technology</a> strategies to avoid potential dangers of <a class="thought" href="entries/future_entry.html">future</a> <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>, but that doesn't make them anti-<a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a>. An example of an ethical guideline is the avoidance of physical entities that can self-replicate in a natural environment. An example of a <a class="thought" href="entries/technology_entry.html">technology</a> strategy is what <a class="thought" href="entries/nanotechnology_entry.html">nanotech</a>nologist Ralph Merkle calls the "Broadcast <a class="thought" href="entries/architecture_entry.html">Architecture</a>." Merkle's idea is that replicating entities would have to obtain self-replicating codes from a centralized secure <a class="thought" href="entries/server_entry.html">server</a>, which would guard against undesirable replication. The Broadcast <a class="thought" href="entries/architecture_entry.html">Architecture</a> is impossible in the <a class="thought" href="entries/biological_entry.html">biological</a> world, which represents at least one way in which <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> can be made safer than <a class="thought" href="entries/biotechnology_entry.html">biotechnology</a>.</p>
<p>Much of Gilder and Vigilante's criticism of the feasibility of <a class="thought" href="entries/future_entry.html">future</a> technologies centers on <a class="thought" href="entries/genetic_algorithm_entry.html">genetic algorithm</a>s and other self-organizing <a class="thought" href="entries/program_entry.html">program</a>s as if the plan was to simply (and mindlessly) recreate the powers of the natural world by rerunning <a class="thought" href="entries/evolution_entry.html">evolution</a>. We find a variety of self-organizing <a class="thought" href="entries/paradigm_entry.html">paradigm</a>s in the few dozen regions of the <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> that we currently have an understanding of (with several hundred regions left to be reverse <a class="thought" href="entries/engine_entry.html">engine</a>ered). <a class="thought" href="entries/self_organizing_entry.html">Self-organization</a> is a powerful concept, but it is hardly automatic. We use a variety of self-organizing methods in my own field of <a class="thought" href="entries/pattern_recognition_entry.html">pattern recognition</a>, and they are critical to achieving a variety of intelligent behaviors. But the accelerating <a class="thought" href="entries/progress_entry.html">progress</a>ion of <a class="thought" href="entries/technology_entry.html">technology</a> is not fueled by an automatic process of simulating <a class="thought" href="entries/evolution_entry.html">evolution</a>. Rather, it the result of many interacting trends: vastly more powerful <a class="thought" href="entries/computation_entry.html">computation</a> and <a class="thought" href="entries/communication_entry.html">communication</a> technologies, about which Gilder has written so extensively, the exponentially shrinking size of <a class="thought" href="entries/technology_entry.html">technology</a>, our exponentially growing <a class="thought" href="entries/knowledge_entry.html">knowledge</a> of the <a class="thought" href="entries/human_entry.html">human</a> biogenetic <a class="thought" href="entries/system_entry.html">system</a>, and the <a class="thought" href="entries/human_entry.html">human</a> <a class="thought" href="entries/brain_entry.html">brain</a> and nervous <a class="thought" href="entries/system_entry.html">system</a>, and many other <a class="thought" href="entries/salient_entry.html">salient</a> accelerating and intersecting developments.</p>
<p>Gilder and Vigilante cite Joy's "respectful" quotations of <a class="thought" href="entries/unabomber_entry.html">Unabomber</a> <a class="thought" href="entries/kaczynski_entry.html">Ted Kaczynski</a>. These Kaczynski quotes come from my book, and I cited them to analyze specifically where Kaczynski's <a class="thought" href="entries/thinking_entry.html">thinking</a> goes wrong. For example, I quoted the following statement from his <a class="thought" href="entries/unabomber_entry.html">Unabomber</a> manifesto: "You can't get rid of the 'bad' parts of <a class="thought" href="entries/technology_entry.html">technology</a> and retain only the 'good' parts. Take modern <a class="thought" href="entries/medicine_entry.html">medicine</a>, for example. <a class="thought" href="entries/progress_entry.html">Progress</a> in medical <a class="thought" href="entries/science_entry.html">science</a> depends on <a class="thought" href="entries/progress_entry.html">progress</a> in <a class="thought" href="entries/chemistry_entry.html">chemistry</a>, <a class="thought" href="entries/physics_entry.html">physics</a>, <a class="thought" href="entries/biology_entry.html">biology</a>, <a class="thought" href="entries/computer_science_entry.html">computer science</a> and other fields. Advanced medical treatments require expensive, high-tech equipment that can be made available only by a technologically <a class="thought" href="entries/progress_entry.html">progress</a>ive, economically rich society. Clearly you can't have much <a class="thought" href="entries/progress_entry.html">progress</a> in <a class="thought" href="entries/medicine_entry.html">medicine</a> without the whole technological <a class="thought" href="entries/system_entry.html">system</a> and everything that goes with it."</p>
<p>As far as it goes, this statement of Kaczynski is essentially correct. Where Kaczynski and I part company (and I am sure Gilder and Vigilante as well) is his conclusion that the "bad" parts greatly outweigh the good parts. Given this, it is only logical to get rid of all further <a class="thought" href="entries/technology_entry.html">technology</a> development. Joy's position is that we relinquish only the "bad" parts, but on this point I believe that Kaczynski's articulation of the infeasibility of such parsing is correct. We have a fundamental choice to make. Kaczynski stands for violent suppression of the pursuit of <a class="thought" href="entries/knowledge_entry.html">knowledge</a>, and the values of freedom that go along with it. Joy would relinquish only broad areas of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> and leave this task presumably to some sort of <a class="thought" href="entries/government_entry.html">government</a> enforcement. But <a class="thought" href="entries/nanotechnology_entry.html">nanotechnology</a> is not a simple unified field, it is rather the inevitable end result of the ongoing <a class="thought" href="entries/exponential_trend_entry.html">exponential trend</a> of <a class="thought" href="entries/miniaturization_entry.html">miniaturization</a> in all areas of <a class="thought" href="entries/technology_entry.html">technology</a>, which continues to move forward on hundreds of fronts.</p>
<p>Gilder has written with great enthusiasm and insight in his books and newsletters of the <a class="thought" href="entries/exponential_growth_entry.html">exponential growth</a> of many technologies, including Gilder's Law on the explosion of <a class="thought" href="entries/bandwidth_entry.html">bandwidth</a>. In my own writings, I have shown how the <a class="thought" href="entries/exponential_growth_entry.html">exponential growth</a> of the power of <a class="thought" href="entries/technology_entry.html">technology</a> is pervasive and affects a great multiplicity of areas. The impact of these interacting and accelerating revolutions is significant in the short-term (i.e., over years), but revolutionary in the long term (i.e., over decades). I believe that the most cogent strategy to oppose the allure of the suppression of the pursuit of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> is not to deny the potential dangers of <a class="thought" href="entries/future_entry.html">future</a> <a class="thought" href="entries/technology_entry.html">technology</a> nor the theoretical feasibility of disastrous scenarios, but rather to build the case that the continued relatively open pursuit of <a class="thought" href="entries/knowledge_entry.html">knowledge</a> is the most reliable (albeit not foolproof) way to reap the promise while avoiding the peril of profound twenty-first century technologies.</p>
<p>I believe that Gilder and Vigilante and I are in essential agreement on this issue. They write the following which persuasively articulates the point:</p>
<blockquote>"Part of the 'mysterious' realm that Einstein called 'the cradle of all true <a class="thought" href="entries/art_entry.html">art</a> and true <a class="thought" href="entries/science_entry.html">science</a>,' chance is beyond the ken of inductive <a class="thought" href="entries/reason_entry.html">reason</a>. When Albert Hirschman writes that '<a class="thought" href="entries/creativity_entry.html">creativity</a> always comes as a surprise to us,' he is acknowledging this essential property of <a class="thought" href="entries/invention_entry.html">invention</a>. Any effort to reduce the world to the dimensions of our own present understanding will exclude novelty and <a class="thought" href="entries/progress_entry.html">progress</a>. The domain of chance is our <a class="thought" href="entries/access_entry.html">access</a> to futurity and to providence. 'Trusting to chance' seems terrifying, but it is the only way to be open to possibility."</blockquote>
</td><td>&#160;</td><td valign="top"><a href="#discussion">Join the discussion about this article on Mind&#183;X!</a><p></p></td><td> &#160; </td>
</tr>
<tr><td colspan="6"><img alt="" border="0" height="35" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/blank.gif" width="35"></td></tr>
<tr>
<td>&#160;</td>
<td colspan="4">
<a name="discussion"></a><p><span class="mindxheader">&#160;&#160;&#160;[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1541" target="_top">Post New Comment</a>]<br>&#160;&#160;&#160;</span>Mind&#183;X Discussion About This Article:</p><a name="id1542"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>What if...<br><span class="mindxheader"><i>posted on 07/27/2001 12:42 PM by marc@yebble.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id1542" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1542" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I understand that there will be larger benefits from nanotechnology than damage. But what if in the beginning ,where there is no nano police and nothing to stop it , something fails. Think of the Melissa Virus. And stopping thousands or millions of nanobots somewhere in the world is much more complex than just deactivate certain system features.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1581"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: What if...<br><span class="mindxheader"><i>posted on 07/29/2001 3:23 PM by frogigr@mediaone.net</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id1581" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1581" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Yes, nanotechnology, much like Biotechnology and AI, represent threats to the evolution of the human race.  At the heart of each of these technologies is the creation of matter controlled by science to such an extent that the overgrowth of such will render the open doors of human hearts insignificant.  This is precisely why the battle for the hearts of man must be fought now.  Only his entry into the heart allows for the voluntary, algorithmic arrangement of human effort as is required for the optimal evolution of mankind.  The race is on between two exponentialities both fast approacching the knee joint depicted in Kurweil's Singularity premise.  
<br>
<br>
In one camp exist those who embrace the affairs of the heart.  In the other....those who would rather beat it for fear of leaving anything to chance.  Ironic, I suppose,that those who rue chance, should they win the race, will be awarded by the removal of any remaining.
<br>
<br>
<br>
Ready
<br>
<br>
<br>
Set 
<br>
<br>
<br>
Go!!!
<br>
<br>
<br>
"shak'n like a leaf"
<br>
<br>
<br>
frog</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2416"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="40"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="639"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: What if...<br><span class="mindxheader"><i>posted on 09/05/2001 11:40 AM by john.b.davey@btinternet.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2416" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2416" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Don't worry. Digital computer technology (including neural technologies) will be sinister only in the sense like most computer programs they won't work properly. That is more of a concern than 'superintelligence'.</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2418"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="60"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="619"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: What if...<br><span class="mindxheader"><i>posted on 09/05/2001 1:30 PM by tomaz@techemail.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2418" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2418" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I feel no urge to go to the 'www.traditionalthinkers.com/forum' to tell them, how wrong they are.
<br>
<br>
But those guys have a need to that. Coming here assuring us for example, that will be no real AI. Ever. That it is not possible, that Ray Kurzweil or Hans Moravec is not educated enough, that Jesus doesn't approve transhumanism ...
<br>
<br>
Why?  :-)
<br>
<br>
- Thomas
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2420"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="80"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="599"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: What if...<br><span class="mindxheader"><i>posted on 09/05/2001 1:51 PM by john.b.davey@btinternet.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2420" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2420" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I actually came to this site to see if I could get some stuff about AI ( of the weaker kind ). 
<br>
<br>
'Frankenstein' was written nearly 200 years ago incidentally, so I would dispute that your interest in artificial life forms is anything other than in the grandest traditionalism of the 'Pioneers of Progress'. Exaggerating the claims of technology is grand old practise going back 200 years !
<br>
<br>
And to be frank I find it fascinating that people still cling to the notions that computer programs can have mental states despite the fact that this is evidently nonsense. </p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2423"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="100"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="579"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: What if...<br><span class="mindxheader"><i>posted on 09/05/2001 3:05 PM by tomaz@techemail.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2423" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2423" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>John!
<br>
<br>
To be frank, I disagree in something, with the majority (???) of people here. 
<br>
<br>
I don't care, if most people don't buy the Singularity memplex.
<br>
<br>
As Jean-Henri Fabre said:
<br>
<br>
Seek those who find your road agreeable, your personality and mind stimulating, your philosophy acceptable, and your experiences helpful. Let those who do not, seek their own kind. 
<br>
<br>
<br>
<br>
- Thomas 
<br>
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id1564"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: In Response to "Stop Everything...It's Techno-Horror"<br><span class="mindxheader"><i>posted on 07/28/2001 1:13 PM by master of suspicion</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id1564" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D1564" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Kaczynski has proven one thing:
<br>
A man brilliant in one field can be an idiot in another.  A warning to technologists.
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2424"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: In Response to "Stop Everything...It's Techno-Horror"<br><span class="mindxheader"><i>posted on 09/05/2001 3:13 PM by tomaz@techemail.com</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2424" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2424" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>Thanks for the warning. How sweet!
<br>
<br>
But Unabomber was anti technologist - as I recall. Who opposed since and technology. 
<br>
<br>
- Thomas</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2307"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: In Response to "Stop Everything...It's Techno-Horror"<br><span class="mindxheader"><i>posted on 08/28/2001 4:47 PM by jpjolly</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2307" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2307" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I am a great admirer of Raymond Kurzweil's visionary work, but I fail to see the eloquence in the Gilder piece. It does nothing but make fun of Joy. The NRA nonsense is particularly troubling - that sure won't raise the level of this discussion.
<br>
<br>
Either these technologies are revolutionary and are going to have big effects, both positive and negative, or they are no big deal. We already know that advances in nanotechnology, biotechnology, and AI represent milestones in human history. So when scientists stand up and say that only the positive effects will be felt, it's hard to believe they are thinking scientifically. In fact, it makes you wonder if they are anything more than geek lobbyists. Some scientists should probably be asking themselves if they haven't gotten so carried away with their own Mensa membership that they don't think they are susceptible to manipulation. 
<br>
<br>
If you take strong medicine, you have to accept the side effects, too - sure, you can take something homeopathic without side effects, but it won't make you well, either. The question facing us with the new technologies is whether we need the medicine badly enough to accept the side effects, too. The argument that 'every new technological advance has had radical effects on society' is too cheap - humanity has never faced the prospect of taking its fate into its own hands to such a degree. 
<br>
<br>
The main players in scientific progress today are scientists, business people, and military people, all obsessed with 'getting there first'. Public awareness and understanding of just what is becoming technically possible is low. So who is going to explain the downsides in a sensible discussion, if not scientists? "Why the Future Doesn't Need Us" is like Bill Joy's career, full of intuition and inspiration. Those traits are necessary for making big discoveries. Maybe you have to have them to understand the problems they bring, too.
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id2405"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="20"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="659"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: In Response to "Stop Everything...It's Techno-Horror"<br><span class="mindxheader"><i>posted on 09/04/2001 3:52 PM by bwkaplan@eos.ncsu.edu</i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id2405" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D2405" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>"Why the Future Doesn't Need Us" is like Bill Joy's career, full of intuition and inspiration."
<br>
<br>
First the statement: Bill Joy is a hypocrite. He is sanguine enough to abstain in the perils of some future technologies only after he has made his millions. Give me a break. 
<br>
<br>
To his credit: It is a discussion that the world needs to be starting today. We, as a community, need to know the implications of what exactly is being 'sold' to us as the future. Someone with less credentials probably wouldn't have caused such a stir.
<br>
<br>
To his argument: I can not fathom someone with so much experience in a competitive field is naive enough to believe that there are easily divisible 'good' technologies and 'bad' technologies. I don't know ANYTHING about technology (yet), but I know that much. I agree with R.K. in that the rewards and perils of technology are intimately fused.
<br>
<br>
<br>
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id21534"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: In Response to "Stop Everything...It's Techno-Horror"<br><span class="mindxheader"><i>posted on 11/09/2003 6:26 PM by <a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/profile.php?id=815">jmikeal8</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id21534" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D21534" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>I am in agreement with Ray Kurzweil when he said that the 'sophistication and power of our defensive technologies and knowledge will grow along with the dangers.'  However, is there and will there be an effective enough defense against artificial intelligence?  What sort of preventative measures and assurances are being made to ensure that artificial intelligence, if created, will not cause any harm?  Considering the computer viruses that have devastated the world, including the Code Red virus, and the recent MSBlaster virus as well as the SoBig virus, a considerable amount of damage was done all over the world before the patches and new virus definitions were created to resolve the problems.  Keeping this in mind, since artificial intelligence will be a great many times smarter than humans, how will we be able to stop it from launching nuclear missiles on the world or causing nuclear meltdowns in all nuclear power plants?  Short of having the artificial intelligence kept in an isolated area with no connection to the Internet, will we be swift enough in staunching its destructive effects?
<br>
<br>
Not only do we have to worry about any flaws within AI that would cause it to wreak havoc on the world, but we also have need to worry about some hacker exploiting bugs or flaws through a virus as well.  We have to consider the fact that if AI is discovered, the power it would inherit may be too much.
<br>
<br>
- UTSC Student
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<a name="id24204"></a>
<table border="0" cellpadding="5" cellspacing="0" width="100%">
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="0"></td>
<td colspan="4"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="679"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-left.gif"></td>
<td bgcolor="#CCCCCC"><p>Re: In Response to "Stop Everything...It's Techno-Horror"<br><span class="mindxheader"><i>posted on 03/06/2004 4:34 PM by <a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/profile.php?id=1111">pvansh</a></i></span></p></td>
<td align="right" bgcolor="#CCCCCC" valign="top"><span class="mindxheader">[<a href="#discussion">Top</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=show_thread.php?rootID%3D1541%23id24204" target="_top">Mind&#183;X</a>]<br>[<a href="https://web.archive.org/web/20100614000926/http://www.kurzweilai.net/mindx/frame.html?main=post.php?reply%3D24204" target="_top">Reply to this post</a>]</span></td>
<td align="right" bgcolor="#CCCCCC" style="padding: 0px;" valign="top"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-right.gif"></td>
</tr>
<tr>
<td></td>
<td bgcolor="#DDDDDD" colspan="4"><p>This article reflects the debate that has continued for quite a long time between most individuals in the technology industry. Clearly everyone has a belief or point of view about where technology should go and on how we should approach the potential discoveries the future may bring. 
<br>
<br>
I find Joy's point of view very intriguing. Along the lines of what another poster wrote, it appears as though Joy is being somewhat of a hypocrite. As an individual who himself has benefited significantly from emerging new technologies, he appears to be turning his back on others who wish to reap the individual and social benefits that he was able to enjoy in the past. It seems as though he believes that his intentions were founded and legitimate but those of others are dangerous and questionable.
<br>
<br>
Regardless, this is clearly not the main issue of the article. The issue clearly remains whether to support emerging technologies or to oppose them because of their potential risks in the future. I can definitely see where Joy and those who think like him are coming from, but I also believe their ideas are not plausible. One cannot simply stop or prevent innovation. Individuals, since the dawn of mankind, have always tried to improve living conditions and make our lives easier. We cannot assume that mankind will cease to innovate when being told to do so. It is simply not possible and is the key reason why we cannot simply stop development in specific areas or cease to develop totally.
<br>
<br>
As well, we must consider the failure to develop. If we fail to develop in specific areas we leave ourselves vulnerable to malicious attacks from those who may develop the technologies to harm others. We must realize that if we are left behind in a technological field we risk the possibility of being attacked and having little or no defense because our lack of knowledge in the field.  So, as Kurzweil indicates, we must continue to develop, not only to advance our societies but to protect ourselves from the evils that others can perpetrate against us if we don't.
<br>
<br>
Kurzweil's ideas appear to be the direction in which we are likely to continue, but the concern I raise is the rate at which these new technologies are developed. My greatest disparity with Kurzweil's view is the rate at which these new technologies should develop. The potential for developing too quickly without sufficient security or expertise in a field will prevent us from successfully being capable of intercepting and diffusing problems that could arise. The fear is that boundless development will continue without regard for safeguards or potential security flaws, and these could be exploited causing immeasurable consequences.
<br>
</p></td>
</tr>
<tr>
<td><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-left.gif"></td>
<td bgcolor="#DDDDDD" colspan="2"><img height="1" src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/images/blank.gif" width="1"></td>
<td align="right" bgcolor="#DDDDDD" style="padding: 0px;" valign="bottom"><img src="https://web.archive.org/web/20100614000926im_/http://www.kurzweilai.net/mindx/images/round-bottom-right.gif"></td>
</tr>
</table>
<p></p></td>
<td>&#160;</td>
</tr>
</table>
</td></tr></table>
</body>
</html>